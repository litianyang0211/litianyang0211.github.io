<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Machine Learning Paradigm</title>
    <url>/Computer-science/Machine-learning/Machine_Learning_Paradigm.html</url>
    <content><![CDATA[<h1 id="supervised-and-unsupervised-machine-learning">1. Supervised and Unsupervised Machine Learning</h1>
<h2 id="supervised-learning">1.1. Supervised Learning</h2>
<ul>
<li><p>We have access to a <strong>labeled dataset</strong> of input-output pairs: <span class="math inline">\(\mathcal{D}=\{x_n, y_n\}_{n=1}^N\)</span>.</p></li>
<li><p>The aim is to learn a <strong>predictive model</strong> <span class="math inline">\(f\)</span> that takes an input <span class="math inline">\(x \in \mathcal{X}\)</span> and predicts its corresponding output <span class="math inline">\(y \in \mathcal{Y}\)</span>.</p></li>
<li><p>Examples: classification and regression.</p></li>
</ul>
<h2 id="unsupervised-learning">1.2. Unsupervised Learning</h2>
<ul>
<li><p>We have no clear output variable that we are attempting to predict: <span class="math inline">\(\mathcal{D}=\{x_n\}_{n=1}^N\)</span>, which is referred to as <strong>unlabeled data</strong>.</p></li>
<li><p>The aim is to extract some salient features for the dataset, such as underlying structure, patterns, or characteristics.</p></li>
<li><p>Examples: clustering, feature extraction, density estimation, representation learning, data visualization, data compression, and deep generative model.</p></li>
</ul>
<h1 id="discriminative-and-generative-machine-learning">2. Discriminative and Generative Machine Learning</h1>
<ul>
<li><p><strong>Discriminative method</strong> directly makes predictions.</p></li>
<li><p><strong>Generative method</strong> tries to explain how the data is generated.</p></li>
</ul>
<h2 id="discriminative-machine-learning">2.1. Discriminative Machine Learning</h2>
<ul>
<li><p>In supervised setting, discriminative method directly learns a mapping <span class="math inline">\(f_\theta\)</span> from input <span class="math inline">\(x\)</span> to output <span class="math inline">\(y\)</span> (or characterization of <span class="math inline">\(y\)</span>, e.g., class probability).</p></li>
<li><p><strong>Training</strong> uses <span class="math inline">\(\mathcal{D}=\{x_n, y_n\}_{n=1}^N\)</span> to estimate optimal values of the parameters <span class="math inline">\(\theta^*\)</span> by minimizing an expected loss or risk.</p></li>
<li><p><strong>Prediction</strong> at a new input <span class="math inline">\(x\)</span> involves simply applying <span class="math inline">\(f_{\widehat{\theta}}(x)\)</span>, where <span class="math inline">\(\widehat{\theta}\)</span> is estimate of <span class="math inline">\(\theta^*\)</span>.</p></li>
<li><p>The set of allowable <span class="math inline">\(\theta\)</span> does not need to be fixed: in <strong>non-parametric</strong> approach, the dimensionality of <span class="math inline">\(\theta\)</span> increases with dataset size.</p></li>
<li><p>Though less common, there are discriminative unsupervised method.</p></li>
<li><p>Examples: neural network, support vector machine, random forest, linear or logistic regression, and <span class="math inline">\(k\)</span>-nearest neighbors.</p></li>
</ul>
<h2 id="generative-machine-learning">2.2. Generative Machine Learning</h2>
<ul>
<li><p>Generative method constructs a <strong>probabilistic model</strong> to explain how the data is generated. For example, with labeled data <span class="math inline">\(\mathcal{D}=\{x_n, y_n\}_{n=1}^N\)</span>, we might construct a joint model <span class="math inline">\(p(X, Y)\)</span> over input and output.</p></li>
<li><p>Generative method can still be used for prediction since generative method implies a predictive model via the conditional distribution <span class="math inline">\(p(Y=y \mid X=x)\)</span>.</p></li>
<li><p><strong>Model parameters</strong> <span class="math inline">\(\theta\)</span> can be generative. For example, with unsupervised data <span class="math inline">\(\mathcal{D}=\{x_n\}_{n=1}^N\)</span>, we might construct a generative model <span class="math inline">\(p(\theta, X)\)</span>, which is the foundation for Bayesian machine learning.</p></li>
<li><p>Examples: mixture model, probabilistic context-free grammar, deep generative model, Boltzmann machine, and Bayesian approach.</p></li>
</ul>
]]></content>
      <categories>
        <category>Computer science</category>
        <category>Machine learning</category>
      </categories>
      <tags>
        <tag>Machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Causal Inference</title>
    <url>/Statistics/Graphical-model/Causal_Inference.html</url>
    <content><![CDATA[<h1 id="causal-inference">1. Causal Inference</h1>
<p><strong>Definition 1.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG, and <span class="math inline">\(p\)</span> be a distribution Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>. <span class="math inline">\((\mathcal{G}, p)\)</span> is <strong>causal</strong> for <span class="math inline">\(x_V\)</span>, if <span class="math display">\[p(x_{V-A} \mid \text{do}(x_A))=\prod_{i \in V-A} p(x_i \mid x_{\text{pa}(i)})\]</span> for all <span class="math inline">\(A \subseteq V\)</span> and <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>.</p>
<p><strong>Example 1.1</strong>    Suppose <span class="math inline">\(p\)</span> is causal w.r.t. <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math inline">\(p(y, z \mid \text{do}(x))=p(z)p(y \mid x, z)\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            Z((Z)) --> X((X)) & Y((Y))
X --> Y
          </pre>
<div class="note warning">
            <p><span class="math inline">\(p(y, z \mid \text{do}(x))=p(z)p(y \mid x, z) \neq p(y, z \mid x)\)</span>, where <span class="math inline">\(p(y, z \mid x)=p(z \mid x)p(y \mid x, z)\)</span>.</p><p>However, we still have <span class="math display">\[p(y \mid \text{do}(x))=\sum_z p(y, z \mid \text{do}(x))=\sum_z p(z)p(y \mid x, z)\]</span> or <span class="math display">\[p(z \mid \text{do}(x))=\sum_y p(y, z \mid \text{do}(x))=p(z)\sum_y p(y \mid x, z)=p(z).\]</span></p>
          </div>
<h1 id="causal-dag">2. Causal DAG</h1>
<p><span id="thm2.1"><strong>Lemma 2.1 (Adjustment Formula)</strong></span>    Let <span class="math inline">\(\mathcal{G}\)</span> be a causal DAG, then <span class="math display">\[p(y \mid \text{do}(z))=\sum_{x_{\text{pa}(z)}} p(x_{\text{pa}(z)}) p(y \mid z, x_{\text{pa}(z)}).\]</span></p>
<p><strong><em>Proof.</em></strong> Let <span class="math inline">\(X_V=(Y, Z, X_{\text{pa}(z)}, X_W)\)</span> where <span class="math inline">\(X_W\)</span> contains any other variables. Then by definition <span class="math display">\[\begin{aligned}
p(y, x_{\text{pa}(z)}, x_W \mid \text{do}(z))&amp;=\prod_{i \in W} p(x_i \mid x_{\text{pa}(i)})p(y \mid x_{\text{pa}(y)}) p(x_{\text{pa}(z)} \mid x_{\text{pa}(\text{pa}(z))})
\\&amp;=\frac{p(y, x_{\text{pa}(z)}, x_W, z)}{p(z \mid x_{\text{pa}(z)})}
\\&amp;=\frac{p(z \mid x_{\text{pa}(z)})p(x_{\text{pa}(z)})p(y, x_W \mid z, x_{\text{pa}(z)})}{p(z \mid x_{\text{pa}(z)})}
\\&amp;=p(x_{\text{pa}(z)})p(y, x_W \mid z, x_{\text{pa}(z)}).
\end{aligned}\]</span></p>
Hence, <span class="math display">\[\begin{aligned}
p(y \mid \text{do}(z))&amp;=\sum_{x_{\text{pa}(z)}, x_W} p(y, x_{\text{pa}(z)}, x_W \mid \text{do}(z))
\\&amp;=\sum_{x_{\text{pa}(z)}} p(x_{\text{pa}(z)})\sum_{x_W}p(y, x_W \mid z, x_{\text{pa}(z)})
\\&amp;=\sum_{x_{\text{pa}(z)}} p(x_{\text{pa}(z)})p(y \mid z, x_{\text{pa}(z)}).
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 2.1</strong>    </p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            Z((Z)) --> W((W))
T((T)) --> X((X)) --> Y((Y))
Z --> X
W --> Y
          </pre>
<p>According to <a href="#thm2.1">adjustment formula</a>, <span class="math inline">\(\displaystyle p(y \mid \text{do}(x))=\sum_{t, z}p(t, z)p(y \mid x, t, z)\)</span>. Since <span class="math inline">\(T \perp\!\!\!\!\perp Y \mid X, Z\)</span>, then <span class="math display">\[p(y \mid \text{do}(x))=\sum_{t, z}p(t, z)p(y \mid x, z)=\sum_z p(z)p(y \mid x, z).\]</span></p>
<p>Besides, we have <span class="math display">\[\begin{aligned}
p(y \mid \text{do}(x))&amp;=\sum_z p(z)p(y \mid x, z)
\\&amp;=\sum_{z, w} p(z)p(y, w \mid x, z)
\\&amp;=\sum_{z, w} p(z)p(w \mid x, z)p(y \mid w, x, z)
\\&amp;=\sum_{z, w} p(z)p(w \mid z)p(y \mid w, x)
\\&amp;=\sum_{w} p(w)p(y \mid w, x)
\end{aligned}\]</span></p>
<div class="note info">
            <p>The example illustrates that there are often multiple equivalent ways of obtaining the same causal quantity.</p>
          </div>
<h1 id="d-separation">3. d-Separation</h1>
<p><strong>Definition 3.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG, and <span class="math inline">\(\pi\)</span> be a path in <span class="math inline">\(\mathcal{G}\)</span>. We say that an internal vertex <span class="math inline">\(v\)</span> is a <strong>collider</strong> if both adjacent edges point to <span class="math inline">\(v\)</span>: <span class="math inline">\(\to v \leftarrow\)</span>. Otherwise, <span class="math inline">\(v\)</span> is a <strong>non-collider</strong>.</p>
<p><strong>Definition 3.2</strong>    Let <span class="math inline">\(\pi\)</span> be a path from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span>. We say that <span class="math inline">\(\pi\)</span> is <strong>open</strong> conditional on <span class="math inline">\(C \subseteq V-\{a, b\}\)</span> if:</p>
<ul>
<li><p>all colliders must be in <span class="math inline">\(\text{an}(C)\)</span>;</p></li>
<li><p>no non-colliders can be in <span class="math inline">\(C\)</span>.</p></li>
</ul>
<p>Otherwise, <span class="math inline">\(\pi\)</span> is <strong>blocked</strong> or <strong>closed</strong> given <span class="math inline">\(C\)</span>.</p>
<p><strong>Example 3.1</strong>    </p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            Z((Z)) --> W((W))
T((T)) --> X((X)) --> Y((Y))
Z --> X
W --> Y
          </pre>
<p>We have <span class="math display">\[\begin{aligned}
\pi_1&amp;: T \to X \to Y \\
\pi_2&amp;: T \to X \leftarrow Z \to W \to Y
\end{aligned}\]</span></p>
<p>Given <span class="math inline">\(\varnothing\)</span>, <span class="math inline">\(\pi_1\)</span> is open since <span class="math inline">\(X\)</span> is a non-collider not in <span class="math inline">\(\varnothing\)</span>; <span class="math inline">\(\pi_2\)</span> is blocked since <span class="math inline">\(X\)</span> is a collider not in <span class="math inline">\(\text{an}_\mathcal{G}(\varnothing)\)</span>.</p>
<p>Given <span class="math inline">\(\{X\}\)</span>, <span class="math inline">\(\pi_1\)</span> is blocked since <span class="math inline">\(X\)</span> is a non-collider in <span class="math inline">\(\{X\}\)</span>; <span class="math inline">\(\pi_2\)</span> is open since <span class="math inline">\(X\)</span> is a collider in <span class="math inline">\(\text{an}_\mathcal{G}(\{X\})\)</span>.</p>
<p>Given <span class="math inline">\(\{X, Z\}\)</span>, <span class="math inline">\(\pi_1\)</span> and <span class="math inline">\(\pi_2\)</span> are blocked.</p>
<p><strong>Definition 3.3</strong>    Let <span class="math inline">\(A, B, C\)</span> be disjoint subsets of <span class="math inline">\(V\)</span> in a DAG <span class="math inline">\(\mathcal{G}\)</span>, where <span class="math inline">\(C\)</span> may be empty. We say <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>d-separated</strong> given <span class="math inline">\(C\)</span>, if all paths from any <span class="math inline">\(a \in A\)</span> to any <span class="math inline">\(b \in B\)</span> are blocked by <span class="math inline">\(C\)</span>, denoted <span class="math inline">\(A \perp_d B \mid C\)</span>.</p>
<p><strong>Theorem 3.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG with disjoint subsets <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>. Then <span class="math inline">\(A \perp_s B \mid C\ [(\mathcal{G}_{\text{an}(A \cup B \cup C)})^m]\)</span> iff <span class="math inline">\(A\)</span> is d-separated from <span class="math inline">\(B\)</span> by <span class="math inline">\(C\)</span> in <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong>Example 3.2</strong>    </p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            a((a)) --> b((b)) & c((c)) --> d((d)) --> e((e))
          </pre>
<p>Since <span class="math inline">\(b \not\perp_s c \mid a, e\ [(\mathcal{G}_{\text{an}(\{a, b, c, e\})})^m]\)</span>, then <span class="math inline">\(b \not\perp_d c \mid a, e\)</span>.</p>
<h1 id="adjustment-set">4. Adjustment Set</h1>
<p><strong>Definition 4.1</strong>    <span class="math inline">\(C\)</span> is an <strong>adjustment set</strong> for <span class="math inline">\((t, y)\)</span> if <span class="math display">\[p(y \mid \text{do}(t))=\sum_{x_C} p(x_C)p(y \mid t, x_C).\]</span></p>
<div class="note info">
            <p>We know that <span class="math inline">\(C=\text{pa}(t)\)</span> is an adjustment set.</p>
          </div>
<p><strong>Definition 4.2</strong>    Given a causal effect <span class="math inline">\(T \to Y\)</span>, we define the <strong>causal nodes</strong> <span class="math inline">\(\text{cn}(T \to Y)\)</span> to be nodes on a causal path from <span class="math inline">\(T\)</span> to <span class="math inline">\(Y\)</span>, except for <span class="math inline">\(T\)</span>, i.e., <span class="math display">\[\text{cn}(T \to Y)=\text{an}(Y) \cap \text{de}(T)-\{T\}.\]</span> We define the <strong>forbidden nodes</strong> for <span class="math inline">\(T \to Y\)</span> as <span class="math display">\[\text{forb}(T \to Y)=\text{de}(\text{cn}(T \to Y)) \cup \{T\}.\]</span></p>
<p><strong>Example 4.1</strong>    </p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            Z((Z)) --> W((W))
Z --> X((X))
W --> Y((Y))
T((T)) --> X --> Y --> S((S))
T --> Y
X --> R((R))
          </pre>
<p>By definition <span class="math inline">\(\text{cn}(T \to Y)=\{X, Y\}\)</span> and <span class="math inline">\(\text{forb}(T \to Y)=\{R, S, T, X, Y\}\)</span>.</p>
<p><strong>Definition 4.3</strong>    Given a causal effect <span class="math inline">\(T \to Y\)</span>, we say that <span class="math inline">\(C \subseteq V-\{T, Y\}\)</span> is a <strong>valid adjustment set</strong> if:</p>
<ul>
<li><p><span class="math inline">\(C \cap \text{forb}(T \to Y)=\varnothing\)</span>;</p></li>
<li><p><span class="math inline">\(C\)</span> must block all non-causal path from <span class="math inline">\(T\)</span> to <span class="math inline">\(Y\)</span>.</p></li>
</ul>
<div class="note info">
            <p>We can divide a valid adjustment set <span class="math inline">\(C\)</span> into two components: <span class="math display">\[B=C \cap \text{nd}(T) \quad \text{and} \quad D=C-B=C \cap \text{de}(T).\]</span> We call <span class="math inline">\(B\)</span> the <strong>backdoor adjustment set</strong>.</p>
          </div>
<p><strong>Theorem 4.1</strong>    If <span class="math inline">\(C\)</span> is a valid adjustment set for <span class="math inline">\((T, Y)\)</span>, then so is <span class="math inline">\(B=C \cap \text{nd}(T)\)</span>.</p>
<p><strong><em>Proof.</em></strong> If <span class="math inline">\(C \cap \text{forb}(T \to Y)=\varnothing\)</span>, then <span class="math inline">\(B \cap \text{forb}(T \to Y)=\varnothing\)</span> since <span class="math inline">\(B \subseteq C\)</span>.</p>
<p>Consider path from <span class="math inline">\(T\)</span> to <span class="math inline">\(Y\)</span>. Any path beginning with an edge <span class="math inline">\(T \to \cdots\)</span> is either causal, or will meet a collider.</p>
<ul>
<li><p>Causal path is open given <span class="math inline">\(B\)</span> since all non-colliders are outside <span class="math inline">\(B \subseteq C\)</span>.</p></li>
<li><p>If path meeting a collider is blocked by <span class="math inline">\(C\)</span>, then the collider is not in <span class="math inline">\(\text{an}(C)\)</span>, and thus the collider is not in <span class="math inline">\(\text{an}(B) \subseteq \text{an}(C)\)</span>, i.e., the path meeting a collider is blocked by <span class="math inline">\(B\)</span>.</p></li>
</ul>
<p>Any path beginning with an edge <span class="math inline">\(T \leftarrow \cdots\)</span> is blocked by <span class="math inline">\(C\)</span>.</p>
<ul>
<li><p>If the path is blocked at a collider, then the collider is not in <span class="math inline">\(\text{an}(B)\)</span>.</p></li>
<li><p>If the path is blocked at a non-collider that is not in <span class="math inline">\(\text{de}(T)\)</span>, then the non-collider is in <span class="math inline">\(B\)</span>.</p></li>
<li><p>If the path is blocked at a non-collider that is in <span class="math inline">\(\text{de}(T)\)</span>, then the non-collider is in <span class="math inline">\(D\)</span>. Hence, the path contains a collider in <span class="math inline">\(D\)</span>, where the path must be blocked by <span class="math inline">\(B\)</span>.</p></li>
</ul>
Hence <span class="math inline">\(B\)</span> is a valid adjustment set for <span class="math inline">\((T, Y)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>Any set <span class="math inline">\(B \cup A\)</span> for <span class="math inline">\(A \subseteq D\)</span> with <span class="math inline">\(\text{an}(A) \cap D=A \cap D=A\)</span> is a valid adjustment set for <span class="math inline">\((T, Y)\)</span>.</p>
          </div>
<p><strong>Theorem 4.2</strong>    For any <span class="math inline">\(d \in C-B\)</span>, either <span class="math inline">\(d \perp_d y \mid (C-\text{de}(d)) \cup \{t\}\)</span> or <span class="math inline">\(d \perp_d t \mid C-\text{de}(d)\)</span>, where <span class="math inline">\(C\)</span> is a valid adjustment set for <span class="math inline">\((t, y)\)</span>,</p>
<p><strong><em>Proof.</em></strong> Assume for a contradiction that both statements fail. Consider <span class="math inline">\(A=\text{an}(\{d, y, t\} \cup (C-\text{de}(d)))\)</span>. In <span class="math inline">\((\mathcal{G}_A)^m\)</span>, there are open undirected paths <span class="math inline">\(\pi_y\)</span> (from <span class="math inline">\(d\)</span> to <span class="math inline">\(y\)</span> given <span class="math inline">\((C-\text{de}(d)) \cup \{t\}\)</span>) and <span class="math inline">\(\pi_t\)</span> from (<span class="math inline">\(d\)</span> to <span class="math inline">\(t\)</span> given <span class="math inline">\(C-\text{de}(d)\)</span>).</p>
<p>We can concatenate these two paths to obtain an open path <span class="math inline">\(\pi\)</span> from <span class="math inline">\(t\)</span> to <span class="math inline">\(y\)</span> given <span class="math inline">\((C-\text{de}(d)) \cup \{d\}\)</span>.</p>
<center>
<strong><em>TBC...</em></strong>
</center>
<p><span id="thm4.3"><strong>Lemma 4.3</strong></span>    If <span class="math inline">\(C\)</span> is a valid adjustment set and <span class="math inline">\(B=C \cap \text{nd}(t)\)</span>, then <span class="math display">\[\sum_{x_C} p(x_C)p(y \mid t, x_C)=\sum_{x_B} p(x_B)p(y \mid t, x_B).\]</span></p>
<p><strong><em>Proof.</em></strong> Take <span class="math inline">\(d \in D\)</span> that has no descendants in <span class="math inline">\(C-\{d\}\)</span>. We know that either <span class="math inline">\(d \perp_d y \mid (C-\{d\}) \cup \{t\}\)</span>, in which case we have <span class="math display">\[\begin{aligned}
\sum_{x_C} p(x_C)p(y \mid t, x_C)&amp;=\sum_{x_{C-\{d\}}, x_d} p(x_{C-\{d\}}, x_d)p(y \mid t, x_{C-\{d\}}, x_d)
\\&amp;=\sum_{x_{C-\{d\}}, x_d} p(x_{C-\{d\}}, x_d)p(y \mid t, x_{C-\{d\}})
\\&amp;=\sum_{x_{C-\{d\}}} p(x_{C-\{d\}})p(y \mid t, x_{C-\{d\}}),
\end{aligned}\]</span></p>
<p>or <span class="math inline">\(d \perp_d t \mid C-\{d\}\)</span>, in which case we have <span class="math display">\[\begin{aligned}
\sum_{x_C} p(x_C)p(y \mid t, x_C)&amp;=\sum_{x_C} p(x_{C-\{d\}})p(x_d \mid x_{C-\{d\}})p(y \mid t, x_C)
\\&amp;=\sum_{x_{C-\{d\}}} p(x_{C-\{d\}}) \sum_{x_d} p(x_d \mid x_{C-\{d\}}, t)p(y \mid t, x_C)
\\&amp;=\sum_{x_{C-\{d\}}} p(x_{C-\{d\}}) \sum_{x_d} p(x_d, y \mid x_{C-\{d\}}, t)
\\&amp;=\sum_{x_{C-\{d\}}} p(x_{C-\{d\}})p(y \mid t, x_{C-\{d\}}).
\end{aligned}\]</span></p>
By inductive argument, the result holds.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.3</strong>    Let <span class="math inline">\(C\)</span> be a valid adjustment set for <span class="math inline">\(T \to Y\)</span>, then <span class="math display">\[p(y \mid \text{do}(t))=\sum_{x_C} p(x_C)p(y \mid t, x_C).\]</span></p>
<p><strong><em>Proof.</em></strong>    By <a href="#thm4.3">lemma 4.2</a>, it is sufficient to prove with assumption that <span class="math inline">\(C\)</span> contains no descendants of <span class="math inline">\(t\)</span>. Since <span class="math inline">\(C \cap \text{de}(t) = \varnothing\)</span>, then <span class="math inline">\(t \perp_d C \mid \text{pa}(t)\)</span> by local Markov property.</p>
<p>Suppose there is an open path <span class="math inline">\(\pi\)</span> from some <span class="math inline">\(s \in \text{pa}(t)\)</span> to <span class="math inline">\(y\)</span> given <span class="math inline">\(C \cup \{t\}\)</span>. If <span class="math inline">\(\pi\)</span> passes through <span class="math inline">\(t\)</span>, it is a collider on the path, and we can shorten it to give an open path from <span class="math inline">\(t\)</span> to <span class="math inline">\(y\)</span> that begins <span class="math inline">\(t \leftarrow\)</span>. If <span class="math inline">\(\pi\)</span> is open given <span class="math inline">\(C\)</span>, then we add <span class="math inline">\(s \to t\)</span> and path is open given a valid adjustment set, which is a contradiction. If <span class="math inline">\(\pi\)</span> is note open given <span class="math inline">\(C\)</span>, then there exits a collider <span class="math inline">\(r \in \text{an}(t)-\text{an}(C)\)</span>.</p>
<center>
<strong><em>TBC...</em></strong>
</center>
Then <span class="math display">\[\begin{aligned}
p(y \mid \text{do}(t))&amp;=\sum_{x_{\text{pa}(t)}} p(y \mid t, x_{\text{pa}(t)})p(x_{\text{pa}(t)})
\\&amp;=\sum_{x_{\text{pa}(t)}} p(x_{\text{pa}(t)}) \sum_{x_C} p(y, x_C \mid t, x_{\text{pa}(t)})
\\&amp;=\sum_{x_{\text{pa}(t)}} p(x_{\text{pa}(t)}) \sum_{x_C} p(y \mid t, x_{\text{pa}(t)}, x_C)p(x_C \mid t, x_{\text{pa}(t)})
\\&amp;=\sum_{x_{\text{pa}(t)}} p(x_{\text{pa}(t)}) \sum_{x_C} p(y \mid t, x_C)p(x_C \mid x_{\text{pa}(t)})
\\&amp;=\sum_{x_C} p(y \mid t, x_C)\sum_{x_{\text{pa}(t)}} p(x_{\text{pa}(t)})p(x_C \mid x_{\text{pa}(t)})
\\&amp;=\sum_{x_C} p(y \mid t, x_C)\sum_{x_{\text{pa}(t)}} p(x_{\text{pa}(t)}, x_C)
\\&amp;=\sum_{x_C} p(y \mid t, x_C)p(x_C).
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="gaussian-causal-model">5. Gaussian Causal Model</h1>
<p>The adjustment formula is a way of estimating causal effect using observational distributions: <span class="math display">\[\mathbb{E}[Y \mid \text{do}(z)]=\sum_{x_C}p(x_C)\mathbb{E}[Y \mid z, x_C].\]</span></p>
<p>For multivariate Gaussian, <span class="math inline">\(\displaystyle \mathbb{E}[Y \mid z, x_C]=\beta_0+\beta_zz+\sum_{c \in C} \alpha_cx_c\)</span>, and thus <span class="math display">\[\begin{aligned}
\mathbb{E}[Y \mid \text{do}(z)]&amp;=\int_{\mathcal{X}_C} p(x_C)\left(\beta_0+\beta_zz+\sum_{c \in C} \alpha_cx_c\right)\text{d}x_C
\\&amp;=\beta_0+\beta_zz+\sum_{c \in C} \alpha_c\mathbb{E}[X_c].
\end{aligned}\]</span></p>
<p>The causal effect for <span class="math inline">\(Z\)</span> on <span class="math inline">\(Y\)</span> is <span class="math inline">\(\beta_z\)</span>.</p>
<h1 id="structural-equation-model">6. Structural Equation Model</h1>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Causal inference</tag>
      </tags>
  </entry>
  <entry>
    <title>Conditional Independence</title>
    <url>/Statistics/Graphical-model/Conditional_Independence.html</url>
    <content><![CDATA[<h1 id="definition">1. Definition</h1>
<p><strong>Definition 1.1</strong>    Let <span class="math inline">\(X, Y\)</span> be r.v.s. defined on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>. We denote the joint density <span class="math inline">\(p(x, y)\)</span>, and call <span class="math display">\[p(y)=\int_\mathcal{X} p(x, y)\text{d}x\]</span> the <strong>marginal density</strong> of <span class="math inline">\(Y\)</span>. The <strong>conditional density</strong> of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is defined as any function <span class="math inline">\(p(x \mid y)\)</span> s.t. <span class="math display">\[p(x, y)=p(y)p(x \mid y).\]</span></p>
<div class="note warning">
            <p>If <span class="math inline">\(p(y)&gt;0\)</span> then the solution is unique and given by the familiar expression <span class="math display">\[p(x \mid y)=\frac{p(x, y)}{p(y)}.\]</span></p>
          </div>
<p><strong>Definition 1.2</strong>    Let <span class="math inline">\(X, Y\)</span> be r.v.s. defined on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>. <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>marginally independent</strong> if <span class="math display">\[p(x \mid y)=p(x), \quad \forall x \in \mathcal{X}, y \in \mathcal{Y}, p(y)&gt;0,\]</span> which is equivalent to <span class="math display">\[p(x, y)=p(x)p(y).\]</span></p>
<p><strong>Definition 1.3</strong>    Let <span class="math inline">\(X, Y, Z\)</span> be r.v.s. defined on a Cartesian product space and the joint density be <span class="math inline">\(p(x, y, z)\)</span>. We say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>conditionally independent</strong> given <span class="math inline">\(Z\)</span> if <span class="math display">\[p(x \mid y, z)=p(x \mid z), \quad \forall x \in \mathcal{X}, y \in \mathcal{Y}, z \in \mathcal{Z}\ \text{s.t.}\ p(y, z)&gt;0,\]</span> denoted <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span>.</p>
<div class="note info">
            <p>If <span class="math inline">\(Z\)</span> is degenerate, i.e., there is some <span class="math inline">\(z\)</span> s.t. <span class="math inline">\(P(Z=z)=1\)</span>, then the definition above is the same as saying that <span class="math inline">\(X \perp\!\!\!\!\perp Y\)</span>.</p>
          </div>
<p><strong>Example 1.1</strong>    Let <span class="math inline">\(X_1, \ldots, X_k\)</span> be a Markov chain. Then <span class="math inline">\(X_k\)</span> is independent of <span class="math inline">\(X_1, \ldots, X_{k-2}\)</span> conditional upon <span class="math inline">\(X_{k-1}\)</span>: <span class="math display">\[P(X_k=x_k \mid X_{k-1}=x_{k-1}, \ldots, X_1=x_1)=P(X_k=x_k \mid X_{k-1}=x_{k-1})\]</span> for all <span class="math inline">\(x_k, x_{k-1}, \ldots, x_1\)</span>, i.e., <span class="math inline">\(X_k \perp\!\!\!\!\perp X_1, \ldots, X_{k-2} \mid X_{k-1}\)</span>, a.k.a. <strong>Markov property</strong>, or <strong>memoryless property</strong>.</p>
<p><span id="eg1.2"><strong>Example 1.2</strong></span>    Suppose <span class="math inline">\(\mathbf{X}_V=(X_1, \ldots, X_p)^T\)</span> is a multivariate Gaussian distribution, i.e., <span class="math display">\[f(\mathbf{x}_V; \mu, \Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{p/2}}\exp\left[-\frac{1}{2}(\mathbf{x}_V-\mu)^T\Sigma^{-1}(\mathbf{x}_V-\mu)\right].\]</span> Then <span class="math display">\[X_p \mid X_1=x_1, \ldots, X_{p-1}=x_{p-1} \sim \mathcal{N}(\mu_p+\Sigma_{p, -p}\Sigma_{-p, -p}^{-1}(\mathbf{x}_{-p}-\mu_{-p}), \sigma_{pp \cdot 1 \cdots p-1}),\]</span> where <span class="math inline">\(\sigma_{aa \cdot B}=\sigma_{aa}-\Sigma_{aB}\Sigma_{BB}^{-1}\Sigma_{Ba}\)</span>, is called the <strong>Schur complement</strong> of entry <span class="math inline">\((a, a)\)</span> in <span class="math inline">\(\Sigma\)</span>.</p>
<p><strong>Theorem 1.1</strong>    Let <span class="math inline">\(X, Y, Z\)</span> be r.v.s. defined on a Cartesian product space. The following are equivalent:</p>
<p>    (1) <span class="math inline">\(p(x \mid y, z)=p(x \mid z)\)</span> for all <span class="math inline">\(x, y, z\)</span> s.t. <span class="math inline">\(p(y, z)&gt;0\)</span>;</p>
<p>    (2) <span class="math inline">\(p(x, y \mid z)=p(x \mid z)p(y \mid z)\)</span> for all <span class="math inline">\(x, y, z\)</span> s.t. <span class="math inline">\(p(z)&gt;0\)</span>;</p>
<p>    (3) <span class="math inline">\(p(x, y, z)=p(y, z)p(x \mid z)\)</span> for all <span class="math inline">\(x, y, z\)</span> s.t. <span class="math inline">\(p(z)&gt;0\)</span>;</p>
<p>    (4) <span class="math inline">\(p(z)p(x, y, z)=p(x, z)p(y, z)\)</span> for all <span class="math inline">\(x, y, z\)</span>;</p>
<p>    (5) <span class="math inline">\(p(x, y, z)=f(x, z)g(y, z)\)</span> for some functions <span class="math inline">\(f, g\)</span> and all <span class="math inline">\(x, y, z\)</span>.</p>
<p><strong><em>Proof.</em></strong> We need to show (1) <span class="math inline">\(\Rightarrow\)</span> (2) <span class="math inline">\(\Rightarrow\)</span> (3) <span class="math inline">\(\Rightarrow\)</span> (1), (3) <span class="math inline">\(\Leftrightarrow\)</span> (4), and (3) <span class="math inline">\(\Leftrightarrow\)</span> (5).</p>
<p>(i) Since <span class="math inline">\(p(y, z)&gt;0\)</span>, then <span class="math inline">\(p(z)&gt;0\)</span>. Since <span class="math inline">\(p(x \mid y, z)p(y \mid z)=p(x, y \mid z)\)</span>, then <span class="math display">\[p(x, y \mid z)=p(x \mid z)p(y \mid z),\]</span> i.e., (1) <span class="math inline">\(\Rightarrow\)</span> (2).</p>
<p>(ii) We have <span class="math display">\[p(x, y, z)=p(x, y \mid z)p(z)=p(x \mid z)p(y \mid z)p(z)=p(y, z)p(x \mid z)\]</span> for all <span class="math inline">\(x, y, z\)</span> s.t. <span class="math inline">\(p(z)&gt;0\)</span>, i.e., (2) <span class="math inline">\(\Rightarrow\)</span> (3).</p>
<p>(iii) It is obvious to show that (3) <span class="math inline">\(\Rightarrow\)</span> (1), (3) <span class="math inline">\(\Leftrightarrow\)</span> (4), and (3) <span class="math inline">\(\Rightarrow\)</span> (5).</p>
<p>(iv) Suppose <span class="math inline">\(p(x, y, z)=f(x, z)g(y, z)\)</span> for some functions <span class="math inline">\(f, g\)</span> and all <span class="math inline">\(x, y, z\)</span>. We have <span class="math display">\[p(y, z)=\int_\mathcal{X} p(x, y, z)\text{d}x=g(y, z)\int_\mathcal{X} f(x, z)\text{d}x:=g(y, z)\widetilde{f}(z).\]</span> Then <span class="math display">\[p(x, y, z)=\frac{f(x, z)p(y, z)}{\widetilde{f}(z)}.\]</span></p>
Since <span class="math display">\[p(x \mid y, z)=\frac{p(x, y, z)}{p(y, z)}=\frac{f(x, z)}{\widetilde{f}(z)}=p(x \mid z),\]</span> then we have (5) <span class="math inline">\(\Rightarrow\)</span> (1) <span class="math inline">\(\Rightarrow\)</span> (3).
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="graphoid-axioms">2. Graphoid Axioms</h1>
<p><strong>Theorem 2.1 (Graphoid Axioms)</strong>    Conditional independence satisfies the following properties:</p>
<p>    (1) Symmetry: <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z \Rightarrow Y \perp\!\!\!\!\perp X \mid Z\)</span>;</p>
<p>         (<span class="math inline">\(X \perp\!\!\!\!\perp Y \Rightarrow Y \perp\!\!\!\!\perp Z\)</span>)</p>
<p>    (2) Decomposition: <span class="math inline">\(X \perp\!\!\!\!\perp Y, W \mid Z \Rightarrow X \perp\!\!\!\!\perp Y \mid Z\)</span>;</p>
<p>         (<span class="math inline">\(X \perp\!\!\!\!\perp Y, W \Rightarrow X \perp\!\!\!\!\perp Y\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp W\)</span>)</p>
<p>    (3) Weak union: <span class="math inline">\(X \perp\!\!\!\!\perp Y, W \mid Z \Rightarrow X \perp\!\!\!\!\perp W \mid Y, Z\)</span>;</p>
<p>         (<span class="math inline">\(X \perp\!\!\!\!\perp Y, W \Rightarrow X \perp\!\!\!\!\perp W \mid Y\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid W\)</span>)</p>
<p>    (4) Contraction: <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y, Z \Rightarrow X \perp\!\!\!\!\perp Y, W \mid Z\)</span>;</p>
<p>         (<span class="math inline">\(X \perp\!\!\!\!\perp Y\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y \Rightarrow X \perp\!\!\!\!\perp Y, W\)</span>)</p>
<p>    (5) Intersection: if <span class="math inline">\(p(x, y, z, w)&gt;0\)</span>, then <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y, Z\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid W, Z \Rightarrow X \perp\!\!\!\!\perp Y, W \mid Z\)</span>.</p>
<p><strong><em>Proof.</em></strong></p>
<p>(i) Since <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span>, then <span class="math inline">\(p(x \mid y, z)=p(x \mid z)\)</span>, and thus <span class="math inline">\(p(x, y, z)=p(x, z)p(y \mid z)\)</span>. Hence, <span class="math display">\[p(y \mid x, z)=\frac{p(x, y, z)}{p(x, z)}=\frac{p(x, z)p(y \mid z)}{p(x, z)}=p(y \mid z),\]</span> i.e., <span class="math inline">\(Y \perp\!\!\!\!\perp X \mid Z\)</span>.</p>
<p>(ii) Since <span class="math inline">\(X \perp\!\!\!\!\perp Y, W \mid Z\)</span>, then <span class="math inline">\(p(x \mid y, w, z)=p(x \mid z)\)</span>, and thus <span class="math inline">\(p(x, y, w \mid z)=p(x \mid z)p(y, w \mid z)\)</span>. Hence, <span class="math display">\[p(x, y \mid z)=\int_\mathcal{W} p(x, y, w \mid z)=p(x \mid z)\int_\mathcal{W} p(y, w \mid z)=p(x \mid z)p(y \mid z),\]</span> i.e., <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span>.</p>
<p>(iii) Since <span class="math inline">\(X \perp\!\!\!\!\perp Y, W \mid Z\)</span>, then <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span> and thus <span class="math inline">\(p(x \mid y, z)=p(x \mid z)\)</span>. Besides, <span class="math inline">\(p(x \mid y, w, z)=p(x \mid z)\)</span>. Hence, <span class="math inline">\(p(x \mid y, w, z)=p(x \mid y, z)\)</span>, i.e., <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y, Z\)</span>.</p>
<p>(iv) Since <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid Z\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y, Z\)</span>, then <span class="math inline">\(p(x \mid y, z)=p(x \mid z)\)</span> and <span class="math inline">\(p(x \mid w, y, z)=p(x \mid y, z)\)</span>. Hence, <span class="math inline">\(p(x \mid w, y, z)=p(x \mid z)\)</span>, i.e., <span class="math inline">\(X \perp\!\!\!\!\perp W, Y \mid Z\)</span>.</p>
(v) Since <span class="math inline">\(X \perp\!\!\!\!\perp W \mid Y, Z\)</span> and <span class="math inline">\(X \perp\!\!\!\!\perp Y \mid W, Z\)</span>, then <span class="math inline">\(p(x, w, y, z)=f(x, y, z)g(w, y, z)\)</span> for some functions <span class="math inline">\(f, g\)</span>, and <span class="math inline">\(p(x, y, w, z)=\widetilde{f}(x, w, z)\widetilde{g}(y, w, z)\)</span> for some functions <span class="math inline">\(\widetilde{f}, \widetilde{g}\)</span>. Because of positivity, we have <span class="math display">\[f(x, y, z)=\frac{\widetilde{f}(x, w, z)\widetilde{g}(y, w, z)}{g(w, y, z)}=\frac{\widetilde{f}(x, w_0, z)\widetilde{g}(y, w_0, z)}{g(w_0, y, z)}\]</span> for any <span class="math inline">\(w_0\)</span> since <span class="math inline">\(f(x, y, z)\)</span> does not depend on <span class="math inline">\(w\)</span>. Then <span class="math inline">\(f(x, y, z)=a(x, z)b(y, z)\)</span>, and thus <span class="math display">\[p(x, w, y, z)=a(x, z)b(y, z)g(w, y, z)=\widetilde{a}(x, z)\widetilde{b}(y, w, z).\]</span> Therefore, <span class="math inline">\(X \perp\!\!\!\!\perp Y, W \mid Z\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <ul><li><p><strong>Symmetry</strong> means that if <span class="math inline">\(Y\)</span> is irrelevant to <span class="math inline">\(X\)</span> then <span class="math inline">\(X\)</span> is irrelevant to <span class="math inline">\(Y\)</span>.</p></li><li><p><strong>Decomposition</strong> means that if some information is irrelevant to <span class="math inline">\(X\)</span> then part of it is also irrelevant to <span class="math inline">\(X\)</span>.</p></li><li><p><strong>Weak union</strong> means that if <span class="math inline">\(Y\)</span> and <span class="math inline">\(W\)</span> together are irrelevant to <span class="math inline">\(X\)</span> then learning one of them does not make the other one relevant.</p></li><li><p><strong>Contraction</strong> means that if <span class="math inline">\(Y\)</span> is irrelevant to <span class="math inline">\(X\)</span>, and if learning <span class="math inline">\(Y\)</span> does not make <span class="math inline">\(W\)</span> relevant to <span class="math inline">\(X\)</span>, then both <span class="math inline">\(Y\)</span> and <span class="math inline">\(W\)</span> are irrelevant to <span class="math inline">\(X\)</span>.</p></li></ul>
          </div>
<h1 id="functional-conditional-independence">3. Functional Conditional Independence</h1>
<p><strong>Remark 3.1</strong>    Since the events <span class="math inline">\(\{Y=y\}\)</span> and <span class="math inline">\(\{Y=y, h(Y)=h(y)\}\)</span> are equal for any measurable function <span class="math inline">\(h\)</span>, it follows that <span class="math display">\[p(x \mid y, z)=p(x \mid y, h(y), z).\]</span></p>
<p><strong>Example 3.1</strong>    Suppose <span class="math inline">\(X \sim f_\theta\)</span> for some parameter <span class="math inline">\(\theta \in \Theta\)</span>. We say that <span class="math inline">\(T=t(X)\)</span> is a <strong>sufficient statistic</strong> for <span class="math inline">\(\theta\)</span> if the likelihood can be written as <span class="math inline">\(L(\theta \mid X=x)=f_\theta(x)=g(t(x), \theta)h(x)\)</span>. Under a Bayesian interpretation of <span class="math inline">\(\theta\)</span>, this is equivalent to saying <span class="math inline">\(X \perp\!\!\!\!\perp \theta \mid T\)</span>.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\begin{aligned}
p(\theta \mid x)&amp;=\frac{p(x \mid \theta)p(\theta)}{\int_\Theta p(x \mid \phi)p(\phi)\text{d}\phi}
\\&amp;=\frac{g(t(x), \theta)h(x)p(\theta)}{\int_\Theta g(t(x), \phi)h(x)p(\phi)\text{d}\phi}
\\&amp;=\frac{g(t(x), \theta)p(\theta)}{\int_\Theta g(t(x), \phi)p(\phi)\text{d}\phi}
\\&amp;=p(\theta \mid t(x)).
\end{aligned}\]</span> Since <span class="math inline">\(p(\theta \mid x)=p(\theta \mid x, t(x))\)</span>, then <span class="math inline">\(\theta \perp\!\!\!\!\perp X \mid T\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="关于多元高斯分布的补充">4. 关于多元高斯分布的补充</h1>
<p>由<a href="#eg1.2">例1.2</a>，我们对多元高斯分布进行补充.</p>
<h2 id="条件高斯分布">4.1. 条件高斯分布</h2>
<p>假设<span class="math inline">\(\mathbf{x} \in \mathbb{R}^p\)</span>服从<span class="math inline">\(\mathcal{N}(\mu, \Sigma)\)</span>. 将<span class="math inline">\(\mathbf{x}\)</span>和对应的均值<span class="math inline">\(\mu\)</span>做如下划分：<span class="math inline">\(\mathbf{x}=\begin{bmatrix} \mathbf{x}_a \\ \mathbf{x}_b \end{bmatrix}\)</span>和<span class="math inline">\(\mu=\begin{bmatrix} \mu_a \\ \mu_b \end{bmatrix}\)</span>，其中<span class="math inline">\(\mathbf{x}_a, \mu_a \in \mathbb{R}^k\)</span>，<span class="math inline">\(\mathbf{x}_b, \mu_b \in \mathbb{R}^{p-k}\)</span>. 将协方差矩阵<span class="math inline">\(\Sigma\)</span>划分为<span class="math inline">\(\Sigma=\begin{bmatrix} \Sigma_{aa} &amp; \Sigma_{ab} \\ \Sigma_{ba} &amp; \Sigma_{bb} \end{bmatrix}\)</span>. 由<span class="math inline">\(\Sigma\)</span>的对称性易知<span class="math inline">\(\Sigma_{aa}\)</span>和<span class="math inline">\(\Sigma_{bb}\)</span>对称，且<span class="math inline">\(\Sigma_{ba}=\Sigma_{ab}^T\)</span>.</p>
<p>定义<strong>精度矩阵</strong><span class="math inline">\(\Lambda=\Sigma^{-1}\)</span>，并将其划分为<span class="math inline">\(\begin{bmatrix} \Lambda_{aa} &amp; \Lambda_{ab} \\ \Lambda_{ba} &amp; \Lambda_{bb} \end{bmatrix}\)</span>. 由于对称矩阵的逆仍然对称，故<span class="math inline">\(\Lambda_{aa}\)</span>和<span class="math inline">\(\Lambda_{bb}\)</span>对称，且<span class="math inline">\(\Lambda_{ba}=\Lambda_{ab}^T\)</span>.</p>
<p>注意，对于一个一般的高斯分布，其指数项可以写成<span class="math display">\[(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)=\mathbf{x}^T\Lambda\mathbf{x}-2\mathbf{x}^T\Lambda\mu+C.\]</span></p>
<p>下面考虑条件分布<span class="math inline">\(p(\mathbf{x}_a \mid \mathbf{x}_b)\)</span>，只考虑指数项：<span class="math display">\[(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)=(\mathbf{x}_a-\mu_a)^T\Lambda_{aa}(\mathbf{x}_a-\mu_a)+2(\mathbf{x}_a-\mu_a)^T\Lambda_{ab}(\mathbf{x}_b-\mu_b)+(\mathbf{x}_b-\mu_b)^T\Lambda_{bb}(\mathbf{x}_b-\mu_b).\]</span> 由于<span class="math inline">\(\mathbf{x}_b\)</span>固定，我们可以将上式视为关于<span class="math inline">\(\mathbf{x}_a\)</span>的函数，不难看出上式仍是一个二次型，故对应的条件分布<span class="math inline">\(p(\mathbf{x}_a \mid \mathbf{x}_b)\)</span>仍是一个高斯分布. 改写可得<span class="math display">\[\begin{aligned}
(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)&amp;=\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T\Lambda_{aa}\mu_a+2\mathbf{x}_a^T\Lambda_{ab}\mathbf{x}_b-2\mathbf{x}_a^T\Lambda_{ab}\mu_b+C
\\&amp;=\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T(\Lambda_{aa}\mu_a-\Lambda_{ab}(\mathbf{x}_b-\mu_b))+C
\\&amp;=\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T\Lambda_{aa}(\mu_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\mu_b))+C.
\end{aligned}\]</span></p>
<p>因此<span class="math inline">\(\boxed{\mathbb{E}[\mathbf{x}_a \mid \mathbf{x}_b]=\mu_a-\Lambda_{aa}^{-1}\Lambda_{ab}(\mathbf{x}_b-\mu_b)}\)</span>以及<span class="math inline">\(\boxed{\text{Var}[\mathbf{x}_a \mid \mathbf{x}_b]=\Lambda_{aa}^{-1}}\)</span>.</p>
<p>利用分块矩阵的恒等式<span class="math display">\[\begin{bmatrix}
A &amp; B \\ C &amp; D
\end{bmatrix}^{-1}=\begin{bmatrix}
M &amp; -MBD^{-1} \\
-D^{-1}CM &amp; D^{-1}+D^{-1}CMBD^{-1}
\end{bmatrix}\]</span>其中<span class="math display">\[M=(A-BD^{-1}C)^{-1}.\]</span> 因为<span class="math inline">\(\begin{bmatrix} \Sigma_{aa} &amp; \Sigma_{ab} \\ \Sigma_{ba} &amp; \Sigma_{bb} \end{bmatrix}^{-1}=\begin{bmatrix} \Lambda_{aa} &amp; \Lambda_{ab} \\ \Lambda_{ba} &amp; \Lambda_{bb} \end{bmatrix}\)</span>，所以<span class="math display">\[\Lambda_{aa}=(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\]</span>以及<span class="math display">\[\Lambda_{ab}=-(\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba})^{-1}\Sigma_{ab}\Sigma_{bb}^{-1}.\]</span></p>
<p>因此<span class="math inline">\(\boxed{\mathbb{E}[\mathbf{x}_a \mid \mathbf{x}_b]=\mu_a+\Sigma_{ab}\Sigma_{bb}^{-1}(\mathbf{x}_b-\mu_b)}\)</span>以及<span class="math inline">\(\boxed{\text{Var}[\mathbf{x}_a \mid \mathbf{x}_b]=\Sigma_{aa}-\Sigma_{ab}\Sigma_{bb}^{-1}\Sigma_{ba}}\)</span>.</p>
<h2 id="边缘高斯分布">4.2. 边缘高斯分布</h2>
<p>设<span class="math inline">\(p(\mathbf{x}_a, \mathbf{x}_b)\)</span>为联合高斯分布，则其边缘概率分布为<span class="math display">\[p(\mathbf{x}_a)=\int p(\mathbf{x}_a, \mathbf{x}_b)\text{d}\mathbf{x}_b\]</span>也是一个高斯分布.</p>
<p>由于上式涉及到关于<span class="math inline">\(\mathbf{x}_b\)</span>的积分，因此<span class="math display">\[\begin{aligned}
(\mathbf{x}-\mu)^T\Lambda(\mathbf{x}-\mu)&amp;=\mathbf{x}_b^T\Lambda_{bb}\mathbf{x}_b-2\mathbf{x}_b^T\Lambda_{bb}\mu_b+2\mathbf{x}_b^T\Lambda_{ba}\mathbf{x}_a-2\mathbf{x}_b^T\Lambda_{ba}\mu_a+C
\\&amp;=\mathbf{x}_b^T\Lambda_{bb}\mathbf{x}_b-2\mathbf{x}_b^T(\Lambda_{bb}\mu_b-\Lambda_{ba}(\mathbf{x}_a-\mu_a))+C
\\&amp;=\mathbf{x}_b^T\Lambda_{bb}\mathbf{x}_b-2\mathbf{x}_b^T\mathbf{m}+C
\\&amp;=(\mathbf{x}_b-\Lambda_{bb}^{-1}\mathbf{m})^T\Lambda_{bb}(\mathbf{x}_b-\Lambda_{bb}^{-1}\mathbf{m})-\mathbf{m}^T\Lambda_{bb}^{-1}\mathbf{m}+C.
\end{aligned}\]</span></p>
<p>所以，关于<span class="math inline">\(\mathbf{x}_b\)</span>的积分为<span class="math display">\[\begin{aligned}
\int p(\mathbf{x}_a, \mathbf{x}_b)\text{d}\mathbf{x}_b &amp;\propto \int \exp\left[(\mathbf{x}_b-\Lambda_{bb}^{-1}\mathbf{b})^T\Lambda_{bb}(\mathbf{x}_b-\Lambda_{bb}^{-1}\mathbf{m})-\mathbf{m}^T\Lambda_{bb}^{-1}\mathbf{m}\right]\text{d}\mathbf{x}_b
\\&amp;\propto \exp\left[-\mathbf{m}^T\Lambda_{bb}^{-1}\mathbf{m}\right].
\end{aligned}\]</span></p>
<p>于是，边缘高斯分布中关于<span class="math inline">\(\mathbf{x}_a\)</span>的指数项为<span class="math display">\[\begin{aligned}
&amp;\ \ \ \ \ -\mathbf{m}^T\Lambda_{bb}^{-1}\mathbf{m}+\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T\Lambda_{aa}\mu_a-2\mathbf{x}_a^T\Lambda_{ab}\mu_b
\\&amp;=-\mathbf{m}^T\Lambda_{bb}^{-1}\mathbf{m}+\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T(\Lambda_{aa}\mu_a+\Lambda_{ab}\mu_b)
\\&amp;=-(\Lambda_{bb}\mu_b-\Lambda_{ba}(\mathbf{x}_a-\mu_a))^T\Lambda_{bb}^{-1}(\Lambda_{bb}\mu_b-\Lambda_{ba}(\mathbf{x}_a-\mu_a))+\mathbf{x}_a^T\Lambda_{aa}\mathbf{x}_a-2\mathbf{x}_a^T(\Lambda_{aa}\mu_a+\Lambda_{ab}\mu_b)
\\&amp;=\mathbf{x}_a^T(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})\mathbf{x}_a-2\mathbf{x}_a^T(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})\mu_a+C.
\end{aligned}\]</span></p>
<p>因此，<span class="math inline">\(\boxed{\mathbb{E}[\mathbf{x}_a]=\mu_a}\)</span>以及<span class="math inline">\(\boxed{\text{Var}[\mathbf{x}_a]=(\Lambda_{aa}-\Lambda_{ab}\Lambda_{bb}^{-1}\Lambda_{ba})^{-1}=\Sigma_{aa}}\)</span>.</p>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Conditional independence</tag>
      </tags>
  </entry>
  <entry>
    <title>Junction Tree and Message Passing</title>
    <url>/Statistics/Graphical-model/Junction_Tree_and_Message_Passing.html</url>
    <content><![CDATA[<h1 id="junction-tree">1. Junction Tree</h1>
<p><strong>Definition 1.1</strong>    A <strong>junction tree</strong> is a connected undirected graph without cycles (a tree), which has vertex <span class="math inline">\(C_i\)</span> that consists of subset of <span class="math inline">\(V\)</span>, and satisfies the property that if <span class="math inline">\(C_i \cap C_j=S\)</span>, then every vertex on the unique path from <span class="math inline">\(C_i\)</span> to <span class="math inline">\(C_j\)</span> contains <span class="math inline">\(S\)</span>.</p>
<p><strong>Example 1.1</strong>    Given sets <span class="math inline">\(\{1, 2\}\)</span>, <span class="math inline">\(\{2, 3, 4\}\)</span>, <span class="math inline">\(\{2, 4, 5\}\)</span>, <span class="math inline">\(\{4, 6\}\)</span>, <span class="math inline">\(\{6, 7, 8\}\)</span>, we can build a junction tree:</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1, 2)) --- 2((2, 3, 4)) --- 3((4, 6))
2 --- 4((2, 4, 5))
3 --- 5((6, 7, 8))
          </pre>
<p>Equally, we could use a different ordering: <span class="math inline">\(\{6, 7, 8\}\)</span>, <span class="math inline">\(\{4, 6\}\)</span>, <span class="math inline">\(\{2, 4, 5\}\)</span>, <span class="math inline">\(\{1, 2\}\)</span>, <span class="math inline">\(\{2, 3, 4\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1, 2))
2((2, 3, 4))
3((4, 6))
1 & 2 & 3 --- 4((2, 4, 5))
3 --- 5((6, 7, 8))
          </pre>
<p><strong>Theorem 1.1</strong>    If <span class="math inline">\(\mathcal{T}\)</span> is a junction tree, then its vertices satisfy the running intersection property. Conversely, if <span class="math inline">\(C_1, \ldots, C_k\)</span> satisfy the running intersection property, they can be arranged into a junction tree.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(\mathcal{T}\)</span> is a junction tree. Pick a leaf <span class="math inline">\(C_k\)</span> and remove it to obtain <span class="math inline">\(\mathcal{T}^{-k}\)</span>. By the induction hypothesis, the remaining sets, <span class="math inline">\(C_1, \ldots, C_{k-1}\)</span> satisfy the running intersection property. Pick <span class="math inline">\(C_{\sigma(k)}\)</span> to be the unique neighbor of <span class="math inline">\(C_k\)</span> in <span class="math inline">\(\mathcal{T}\)</span>. By the junction tree property, <span class="math display">\[C_k \cap \left(\bigcup_{i&lt;k} C_i\right)=C_k \cap C_{\sigma(k)}.\]</span></p>
Suppose <span class="math inline">\(C_1, \ldots, C_k\)</span> satisfy the running intersection property, and assume <span class="math inline">\(\{C_1, \ldots, C_{k-1}\}\)</span> is a junction tree. Let <span class="math inline">\(C_k \sim C_{\sigma(k)}\)</span>. We know <span class="math inline">\(C_k \cap C_i \subseteq C_{\sigma(k)}\)</span> for all <span class="math inline">\(i\)</span>. If <span class="math inline">\(C_k \cap C_i=\varnothing\)</span>, then every vertex from <span class="math inline">\(C_i\)</span> to <span class="math inline">\(C_k\)</span> contains <span class="math inline">\(\varnothing\)</span>. If <span class="math inline">\(C_k \cap C_i=S \neq \varnothing\)</span>, assume <span class="math inline">\(S \not\subseteq C_j\)</span> for some <span class="math inline">\(j\)</span>, then <span class="math inline">\(C_i \cap C_{\sigma(k)} \subseteq C_{\sigma(k)} \not\subseteq C_j\)</span>, which is a contradction. Hence <span class="math inline">\(S \subseteq C_j\)</span> for all <span class="math inline">\(j\)</span>, and thus <span class="math inline">\(\{C_1, \ldots, C_k\}\)</span> is a junction tree.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 1.2</strong>    If <span class="math inline">\(C_1, \ldots, C_k\)</span> satisfy the running intersection property, then they satisfy the running intersection property starting with any node <span class="math inline">\(C_j\)</span>.</p>
<p><strong>Definition 1.2</strong>    We associate each node <span class="math inline">\(C\)</span> in junction tree with a <strong>potential</strong> <span class="math inline">\(\psi_C(x_C) \geq 0\)</span>, which is a function over the variables in the corresponding set. We say that two potentials are <strong>consistent</strong> if they have the same margin over their intersection, i.e., for <span class="math inline">\(\psi_C(x_C)\)</span> and <span class="math inline">\(\psi_D(x_D)\)</span>, <span class="math display">\[\sum_{x_{C-S}} \psi_C(x_C)=f(x_{S})=\sum_{x_{D-S}} \psi_D(x_D)\]</span> where <span class="math inline">\(S=C \cap D\)</span>.</p>
<p><strong>Theorem 1.3</strong>    Suppose <span class="math inline">\(C_1, \ldots, C_k\)</span> satisfy the running intersection property with separator sets <span class="math inline">\(S_2, \ldots, S_k\)</span>, and let <span class="math display">\[p(x_V)=\prod_{i=1}^k \frac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})}\]</span> for all <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>. By convention, <span class="math inline">\(S_1=\varnothing\)</span> and <span class="math inline">\(\psi_{S_1}=1\)</span>. Then the potentials are all consistent iff <span class="math inline">\(\psi_{C_i}(x_{C_i})=p(x_{C_i})\)</span> and <span class="math inline">\(\psi_{S_i}(x_{S_i})=p(x_{S_i})\)</span> for all <span class="math inline">\(i=1, \ldots, k\)</span>.</p>
<p><strong><em>Proof.</em></strong> (<span class="math inline">\(\Leftarrow\)</span>) If <span class="math inline">\(\psi_{C_i}(x_{C_i})=p(x_{C_i})\)</span>, then <span class="math display">\[\sum_{x_{C_i-S_{ij}}} \psi_{C_i}(x_{C_i})=\sum_{x_{C_i-S_{ij}}} p(x_{C_i})=p(x_{S_{ij}})=\sum_{x_{C_j-S_{ij}}} p(x_{C_j})=\sum_{x_{C_j-S_{ij}}} \psi_{C_j}(x_{C_j})\]</span> for any <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>, i.e., the potentials are all consistent.</p>
<p>(<span class="math inline">\(\Rightarrow\)</span>) If <span class="math inline">\(k=1\)</span>, there is nothing to prove. Otherwise, let <span class="math display">\[R_k=C_k-S_k=C_k-\bigcup_{i&lt;k} C_i.\]</span> Then <span class="math display">\[p(x_{V-R_k})=\sum_{x_{R_k}} p(x_V)=\prod_{i=1}^{k-1} \frac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})} \cdot \frac{1}{\psi_{S_k}(x_{S_k})} \sum_{x_{R_k}} \psi_{C_k}(x_{C_k}).\]</span> Since all potentials are consistent, we have <span class="math display">\[\sum_{x_{R_k}} \psi_{C_k}(x_{C_k})=\psi_{S_k}(x_{S_k})\]</span> and thus <span class="math display">\[p(x_{V-R_k})=\prod_{i=1}^{k-1} \frac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})},\]</span> i.e., <span class="math inline">\(p(x_{V-R_k})\)</span> only has <span class="math inline">\(k-1\)</span> cliques.</p>
<p>By the induction hypothesis, <span class="math inline">\(\psi_{C_i}(x_{C_i})=p(x_{C_i})\)</span> and <span class="math inline">\(\psi_{S_i}(x_{S_i})=p(x_{S_i})\)</span> for all <span class="math inline">\(i&lt;k\)</span>.</p>
<p>Since <span class="math inline">\(S_k \subseteq C_{\sigma(k)}\)</span>, then by consistency <span class="math display">\[\psi_{S_k}(x_{S_k})=\sum_{C_{\sigma(k)}-S_k} \psi_{C_{\sigma(k)}}(x_{C_{\sigma(k)}})=p(x_{S_k}).\]</span></p>
<p>Then <span class="math display">\[p(x_V)=p(x_{V-R_k})\frac{\psi_{C_k}(x_{C_k})}{\psi_{S_k}(x_{S_k})}=p(x_{V-R_k})\frac{\psi_{C_k}(x_{C_k})}{p(x_{S_k})},\]</span> and thus <span class="math display">\[p(x_{R_k} \mid x_{V-R_k})=\frac{\psi_{C_k}(x_{C_k})}{p(x_{S_k})}=p(x_{R_k} \mid x_{S_k}).\]</span> Hence, <span class="math inline">\(\psi_{C_k}(x_{C_k})=p(x_{C_k})\)</span>.</p>
<center>
<strong><em>TBC...</em></strong>
</center>
<h1 id="message-passing">2. Message Passing</h1>
<p><strong>Definition 2.1</strong>    Suppose we have potentials <span class="math inline">\(\psi_C\)</span>, <span class="math inline">\(\psi_D\)</span> and <span class="math inline">\(\psi_S\)</span>, where <span class="math inline">\(S=C \cap D\)</span>. Then <strong>passing a message</strong> from <span class="math inline">\(\psi_C\)</span> to <span class="math inline">\(\psi_D\)</span> involves:</p>
<ul>
<li><p><span class="math inline">\(\displaystyle \psi_S&#39;(x_S)=\sum_{x_{C-S}}\psi_C(x_C)\)</span>;</p></li>
<li><p><span class="math inline">\(\displaystyle \psi_D&#39;(x_D)=\psi_D(x_D)\frac{\psi_S&#39;(x_S)}{\psi_S(x_S)}\)</span>.</p></li>
</ul>
<div class="note info">
            <p>We have three important properties:</p><ul><li><p><span class="math inline">\(\displaystyle \prod_{i=1}^k \frac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})}\)</span> is unchanged because <span class="math inline">\(\displaystyle \frac{\psi_D&#39;(x_D)}{\psi_S&#39;(x_S)}=\frac{\psi_D(x_D)}{\psi_S(x_S)}\)</span>;</p></li><li><p><span class="math inline">\(\psi_{S}&#39;\)</span> and <span class="math inline">\(\psi_C\)</span> are consistent;</p></li><li><p>If <span class="math inline">\(\psi_D\)</span> and <span class="math inline">\(\psi_S\)</span> are consistent, so are <span class="math inline">\(\psi_D&#39;\)</span> and <span class="math inline">\(\psi_S&#39;\)</span>, since <span class="math display">\[\sum_{x_{D-S}} \psi_D&#39;(x_D)=\frac{\psi_S&#39;(x_S)}{\psi_S(x_S)} \sum_{x_{D-S}} \psi_D(x_D)=\psi_S&#39;(x_S).\]</span></p></li></ul>
          </div>
<h1 id="junction-tree-algorithm">3. Junction Tree Algorithm</h1>
<table frame="hsides" style="line-height:20px;">
<tbody>
<tr>
<td>
<b>Algorithm 1</b> Collect and Distribute Steps of the Junction Tree Algorithm.
</td>
</tr>
<tr>
<td>
    <b>function</b> Collect(rooted tree <span class="math inline">\(\mathcal{T}\)</span>, potentials <span class="math inline">\(\psi_t\)</span>)<br>         let <span class="math inline">\(1 &lt; \cdots &lt; k\)</span> be a topological ordering of <span class="math inline">\(\mathcal{T}\)</span>;<br>         <b>for</b> <span class="math inline">\(t\)</span> in <span class="math inline">\(k, \ldots, 2\)</span> <b>do</b><br>             send message from <span class="math inline">\(\psi_t\)</span> to <span class="math inline">\(\psi_{\sigma(t)}\)</span>;<br>         <b>end for</b><br>         <b>return</b> updated potentials <span class="math inline">\(\psi_t\)</span><br>     <b>end function</b><br>     <b>function</b> Distribute(rooted tree <span class="math inline">\(\mathcal{T}\)</span>, potentials <span class="math inline">\(\psi_t\)</span>)<br>         let <span class="math inline">\(1 &lt; \cdots &lt; k\)</span> be a topological ordering of <span class="math inline">\(\mathcal{T}\)</span>;<br>         <b>for</b> <span class="math inline">\(t\)</span> in <span class="math inline">\(k, \ldots, 2\)</span> <b>do</b><br>             send message from <span class="math inline">\(\psi_{\sigma(t)}\)</span> to <span class="math inline">\(\psi_t\)</span>;<br>         <b>end for</b><br>         <b>return</b> updated potentials <span class="math inline">\(\psi_t\)</span><br>     <b>end function</b>
</td>
</tr>
</tbody>
</table>
<p><strong>Theorem 3.1</strong>    Let <span class="math inline">\(\mathcal{T}\)</span> be a junction tree with potentials <span class="math inline">\(\psi_{C_i}(x_{C_i})\)</span>. Then after running the junction tree algorithm, all potentials will be consistent.</p>
<p><strong><em>Proof.</em></strong> Collection ensures all cliques are consistent with <span class="math inline">\(\psi_{S_i}\)</span>. Distribution makes <span class="math inline">\(\psi_{C_{\sigma(i)}}\)</span> consistent with <span class="math inline">\(\psi_{S_i}\)</span> for each <span class="math inline">\(i\)</span>, and preserves the consistency between <span class="math inline">\(\psi_{S_i}\)</span> and <span class="math inline">\(\psi_{C_i}\)</span>.</p>
<center>
<strong><em>TBC...</em></strong>
</center>
<div class="note info">
            <p>In practice, message passing is often done in parallel and we have the convergence after at most <span class="math inline">\(d\)</span> iterations, where <span class="math inline">\(d\)</span> is the length of the longest path of the tree.</p><p>Junction graphs in general (<em>loopy belief propagation</em>) also seem to lead to convergence, but it is still an open area of study.</p>
          </div>
<h1 id="evidence">4. Evidence</h1>
<p>Denote evidence set <span class="math inline">\(E\)</span>, then <span class="math display">\[p(x_{V-E} \mid x_E=x_E^*)=\frac{p(x_{V-E}, x_E^*)}{p(x_E^*)}.\]</span> In junction tree situation, <span class="math display">\[p(x_V)=\prod_i \frac{\psi_{C_i}(x_{C_i})}{\psi_{S_i}(x_{S_i})}.\]</span> Replace <span class="math inline">\(\psi_{C_i}(x_{C_i})\)</span> with <span class="math inline">\(\displaystyle \frac{\psi_{C_i}(x_{C_i})}{p(x_E^*)}\)</span> if <span class="math inline">\(E \subseteq C_i\)</span>, then <span class="math display">\[\psi_{C_i}(x_{C_i})=p(x_{C_i} \mid x_E^*).\]</span></p>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Junction tree</tag>
        <tag>Message passing</tag>
      </tags>
  </entry>
  <entry>
    <title>Gaussian Graphical Model</title>
    <url>/Statistics/Graphical-model/Gaussian_Graphical_Model.html</url>
    <content><![CDATA[<h1 id="multivariate-gaussian-distribution">1. Multivariate Gaussian Distribution</h1>
<p><strong>Definition 1.1</strong>    <span class="math inline">\(X_V\)</span> has a <strong>multivariate Gaussian distribution</strong> with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> if the joint density is <span class="math display">\[f(x_V; \mu, \Sigma)=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left[-\frac{1}{2}(x_V-\mu)^T\Sigma^{-1}(x_V-\mu)\right],\]</span> where <span class="math inline">\(x_V \in \mathbb{R}^p\)</span>. Note that we usually consider <span class="math display">\[\ln f(x_V; \mu, \Sigma)=C-\frac{1}{2}(x_V-\mu)^T\Sigma^{-1}(x_V-\mu).\]</span> or <span class="math display">\[\ln f(x_V; \mu, K)=C-\frac{1}{2}(x_V-\mu)^TK(x_V-\mu).\]</span></p>
<p><strong>Theorem 1.1</strong>    Let <span class="math inline">\(X_V\)</span> has a multivariate Gaussian distribution with concentration matrix <span class="math inline">\(K=\Sigma^{-1}\)</span>. Then <span class="math inline">\(X_i \perp\!\!\!\!\perp X_j \mid X_{V-\{i, j\}}\)</span> iff <span class="math inline">\(k_{ij}=0\)</span>, where <span class="math inline">\(k_{ij}\)</span> is the corresponding entry in the concentration matrix.</p>
<h1 id="gaussian-graphical-model-ggm">2. Gaussian Graphical Model (GGM)</h1>
<div class="note ">
            <p>If <span class="math inline">\(M\)</span> is a matrix whose rows and columns are indexed by <span class="math inline">\(A \subseteq V\)</span>, we write <span class="math inline">\(\{M\}_{A, A}\)</span> to indicate the matrix indexed by <span class="math inline">\(V\)</span> (i.e., it has <span class="math inline">\(|V|\)</span> rows and columns) whose <span class="math inline">\(A, A\)</span>-entries are <span class="math inline">\(M\)</span> and with zeroes elsewhere. For example, if <span class="math inline">\(|V|=3\)</span> then <span class="math display">\[M=\begin{bmatrix}a &amp; b \\ b &amp; c\end{bmatrix} \quad \text{and} \quad \{M\}_{12, 12}=\begin{bmatrix}a &amp; b &amp; 0 \\b &amp; c &amp; 0 \\0 &amp; 0 &amp; 0\end{bmatrix},\]</span> where <span class="math inline">\(12\)</span> is used as an abbreviation for <span class="math inline">\(\{1, 2\}\)</span> in the subscript.</p>
          </div>
<p><strong>Lemma 2.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be a graph with decomposition <span class="math inline">\((A, S, B)\)</span>, and <span class="math inline">\(X_V \sim \mathcal{N}_p(0, \Sigma)\)</span>. Then <span class="math inline">\(p(x_V)\)</span> is Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span> iff <span class="math display">\[\Sigma^{-1}=\{(\Sigma_{AS, AS})^{-1}\}_{AS, AS}+\{(\Sigma_{BS, BS})^{-1}\}_{BS, BS}-\{(\Sigma_{S, S})^{-1}\}_{S, S},\]</span> and <span class="math inline">\(\Sigma_{AS, AS}\)</span> and <span class="math inline">\(\Sigma_{BS, BS}\)</span> are Markov w.r.t. <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> respectively.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\((A, S, B)\)</span> is a decomposition and <span class="math inline">\(p(x_V)\)</span> is Markov, then <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_S\)</span>, which implies for all <span class="math inline">\(x_V \in \mathbb{R}^{|V|}\)</span>, <span class="math display">\[p(x_V)p(x_S)=p(x_A, x_S)p(x_B, x_S).\]</span> Then <span class="math display">\[x_V^T\Sigma^{-1}x_V+x_S^T(\Sigma_{S, S})^{-1}x_S=x_{AS}^T(\Sigma_{AS, AS})^{-1}x_{AS}+x_{BS}^T(\Sigma_{BS, BS})^{-1}x_{BS},\]</span> i.e., <span class="math display">\[x_V^T\Sigma^{-1}x_V+x_V^T\{(\Sigma_{S, S})^{-1}\}_{S, S}x_V=x_V^T\{(\Sigma_{AS, AS})^{-1}\}_{AS, AS}x_V+x_V^T\{(\Sigma_{BS, BS})^{-1}\}_{BS, BS}x_V.\]</span> Therefore, <span class="math display">\[\Sigma^{-1}=\{(\Sigma_{AS, AS})^{-1}\}_{AS, AS}+\{(\Sigma_{BS, BS})^{-1}\}_{BS, BS}-\{(\Sigma_{S, S})^{-1}\}_{S, S}.\]</span></p>
The converse holds similarly.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="maximum-likelihood-estimation">3. Maximum Likelihood Estimation</h1>
<p>Let <span class="math inline">\(X_V^{(1)}, \ldots, X_V^{(n)} \overset{\text{i.i.d.}}{\sim} \mathcal{N}_p(0, \Sigma)\)</span>. The sufficient statistic for <span class="math inline">\(\Sigma\)</span> is the sample covariance matrix <span class="math display">\[W=\frac{1}{n}\sum_{i=1}^n X_V^{(i)}(X_V^{(i)})^T.\]</span> In addition, <span class="math inline">\(\widehat{\Sigma}=W\)</span> is the MLE for <span class="math inline">\(\Sigma\)</span> under the <em>unrestricted model</em> (i.e., when all edges are present in the graph). Let <span class="math inline">\(\widehat{\Sigma}^\mathcal{G}\)</span> denote the MLE for <span class="math inline">\(\Sigma\)</span> under the <em>restriction</em> that the distribution satisfies the Markov property for <span class="math inline">\(\mathcal{G}\)</span>, and <span class="math inline">\(\widehat{K}^\mathcal{G}\)</span> denote the inverse of <span class="math inline">\(\widehat{\Sigma}^\mathcal{G}\)</span>.</p>
<p>For a decomposable graph <span class="math inline">\(\mathcal{G}\)</span> with cliques <span class="math inline">\(C_1, \ldots, C_k\)</span>, the MLE can be written in the form <span class="math display">\[(\widehat{\Sigma}^\mathcal{G})^{-1}=\sum_{i=1}^k\{(W_{C_i, C_i})^{-1}\}_{C_i, C_i}-\sum_{i=2}^k \{(W_{S_i, S_i})^{-1}\}_{S_i, S_i}.\]</span></p>
<p><strong>Example 3.1</strong>    Whittaker (1990) analyzed data on five maths test results administered to 88 students, in analysis, algebra, vectors, mechanics and statistics. Some of the entries in the concentration matrix are quite small, suggesting that conditional independence holds. We want to fit a graphical model and check if it gives an excellent fit.</p>
<p><img data-src="/images/Gaussian_Graphical_Model_1.png"></p>
<center>
Figure 1: A graph for the maths test data.
</center>
<p>By computation, <span class="math inline">\(\widehat{\Sigma}^\mathcal{G}\)</span> is:</p>
<table style="table-layout:fixed">
<tbody>
<tr>
<th>
<center>
</center>
</th>
<th>
<center>
Mechanics
</center>
</th>
<th>
<center>
Vectors
</center>
</th>
<th>
<center>
Algebra
</center>
</th>
<th>
<center>
Analysis
</center>
</th>
<th>
<center>
Statistics
</center>
</th>
</tr>
<tr>
<th>
<center>
Mechanics
</center>
</th>
<td>
<center>
<span class="math inline">\(0.00524\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00244\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00287\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
</tr>
<tr>
<th>
<center>
Vectors
</center>
</th>
<td>
<center>
<span class="math inline">\(-0.00244\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0.01035\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00561\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
</tr>
<tr>
<th>
<center>
Algebra
</center>
</th>
<td>
<center>
<span class="math inline">\(-0.00287\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00561\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0.02849\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00755\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00493\)</span>
</center>
</td>
</tr>
<tr>
<th>
<center>
Analysis
</center>
</th>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00755\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0.00982\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00204\)</span>
</center>
</td>
</tr>
<tr>
<th>
<center>
Statistics
</center>
</th>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00493\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-0.00204\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(0.00644\)</span>
</center>
</td>
</tr>
</tbody>
</table>
<p>We carry out a likelihood ratio test to see whether this model is a good fit to the data. We want to test <span class="math display">\[H_0: \text{Restricted model} \quad \text{against} \quad H_1: \text{Unrestricted model}.\]</span> The test statistic is <span class="math display">\[2(l(\widehat{\Sigma})-l(\widehat{\Sigma}^\mathcal{G}))=0.8958244 \to \chi^2_{(4)}\]</span> and the <span class="math inline">\(p\)</span>-value is <span class="math inline">\(0.925&gt;0.05\)</span>, i.e., the model is a good fit.</p>
<p>Here is the relevant code in R:</p>
<figure class="highlight r"><table><tr><td class="code"><pre><span class="line">library(ggm)</span><br><span class="line">data(marks)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MLE of the covariance matrix under the unrestricted model.</span></span><br><span class="line">S = cov(marks) </span><br><span class="line"></span><br><span class="line"><span class="comment"># Concentration matrix under the restriction.</span></span><br><span class="line">K = matrix(<span class="number">0</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">K[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>] = solve(S[<span class="number">1</span>:<span class="number">3</span>, <span class="number">1</span>:<span class="number">3</span>])</span><br><span class="line">K[<span class="number">3</span>:<span class="number">5</span>, <span class="number">3</span>:<span class="number">5</span>] = K[<span class="number">3</span>:<span class="number">5</span>, <span class="number">3</span>:<span class="number">5</span>] + solve(S[<span class="number">3</span>:<span class="number">5</span>, <span class="number">3</span>:<span class="number">5</span>])</span><br><span class="line">K[<span class="number">3</span>, <span class="number">3</span>] = K[<span class="number">3</span>, <span class="number">3</span>] - <span class="number">1</span> / S[<span class="number">3</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># MLE of the covariance matrix under the restriction.</span></span><br><span class="line">Simga_hat = solve(K)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test statistic.</span></span><br><span class="line">tr = <span class="keyword">function</span>(x) <span class="built_in">sum</span>(diag(x))</span><br><span class="line">n = nrow(marks)</span><br><span class="line">test_sta = - n * ((<span class="built_in">log</span>(det(S)) - <span class="built_in">log</span>(det(Simga_hat))) </span><br><span class="line">	   - tr(S %*% (solve(S) - solve(Simga_hat))))</span><br><span class="line">p = pchisq(q=test_sta, df=<span class="number">4</span>, lower.tail=<span class="literal">FALSE</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Gaussian graphical model</tag>
      </tags>
  </entry>
  <entry>
    <title>Exponential Family and Contingency Table</title>
    <url>/Statistics/Graphical-model/Exponential_Family_and_Contingency_Table.html</url>
    <content><![CDATA[<h1 id="definition">1. Definition</h1>
<p><strong>Definition 1.1</strong>    Let <span class="math inline">\(\{p_\theta: \theta \in \Theta\}\)</span> be a collection of probability densities over <span class="math inline">\(\mathcal{X}\)</span>. We say that <span class="math inline">\(p\)</span> forms an <strong>exponential family</strong> if it can be written as <span class="math display">\[p_\theta(x)=\exp\left[\sum_{i=1}^p \theta_i\phi_i(x)-A(\theta)-C(x)\right].\]</span> If <span class="math inline">\(\Theta\)</span> is non-empty open set, we say that <span class="math inline">\(p_\theta\)</span> is <strong>regular</strong>. The function <span class="math inline">\(\phi_i\)</span> is the <strong>sufficient statistic</strong>, and the component <span class="math inline">\(\theta_i\)</span> is the <strong>canonical parameter</strong> or <strong>natural parameter</strong>. We can replace the sum with an inner product of vectors <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span>: <span class="math display">\[p_\theta(x)=\exp[\langle \theta, \phi(x) \rangle-A(\theta)-C(x)].\]</span></p>
<div class="note info">
            <p>Note that <span class="math display">\[1=\int_\mathcal{X} p_\theta(x)\text{d}x=\exp[-A(\theta)]\int_\mathcal{X} \exp[\langle \theta, \phi(x) \rangle-C(x)]\text{d}x.\]</span> Hence, <span class="math display">\[A(\theta)=\ln\int_\mathcal{X} \exp[\langle \theta, \phi(x) \rangle-C(x)]\text{d}x.\]</span> The function <span class="math inline">\(A(\theta)\)</span> is the <strong>cumulant function</strong>, and <span class="math inline">\(\exp[A(\theta)]\)</span> is the <strong>partition function</strong>.</p>
          </div>
<p><strong>Theorem 1.1</strong>    We have <span class="math display">\[\nabla_\theta A(\theta)=\mathbb{E}_\theta\phi(X) \quad \text{and} \quad \nabla\nabla^TA(\theta)=\text{Cov}_\theta\phi(X).\]</span> Consequently <span class="math inline">\(A(\theta)\)</span> is convex, because <span class="math inline">\(\text{Cov}_\theta\phi(X) \succeq 0\)</span>.</p>
<p><span id="eg1.1"><strong>Example 1.1</strong></span>    Let <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>, we have <span class="math display">\[\ln f(x; \lambda)=-\lambda+x\ln\lambda-\ln x!,\]</span> then <span class="math inline">\(\phi(x)=x\)</span>, <span class="math inline">\(\theta=\ln\lambda\)</span>, and <span class="math inline">\(A(\theta)=\lambda=e^\theta\)</span>. Then <span class="math display">\[A&#39;(\theta)=e^\theta=\mathbb{E}X\]</span> and <span class="math display">\[A&#39;&#39;(\theta)=e^\theta=\text{Var}X.\]</span></p>
<h1 id="empirical-moment-matching">2. Empirical Moment Matching</h1>
<p>To find the MLE in an exponential family, we maximize the log-likelihood (ignoring <span class="math inline">\(C\)</span>, since it is constant in <span class="math inline">\(\theta\)</span>). We have <span class="math display">\[\begin{aligned}
l(\theta)&amp;=\sum_{i=1}^n \langle \theta, \phi(X_i) \rangle -nA(\theta)+C
\\&amp;=\left\langle \theta, \sum_{i=1}^n \phi(X_i) \right\rangle-nA(\theta)+C
\\&amp;=n\langle \theta, \overline{\phi(X)} \rangle-nA(\theta)+C.
\end{aligned}\]</span> Then <span class="math display">\[\nabla_\theta l(\theta)=n\overline{\phi(X)}-n\nabla_\theta A(\theta).\]</span> Let <span class="math inline">\(\nabla_\theta l(\theta)=0\)</span>, we have <span class="math display">\[\nabla_\theta A(\theta)=\overline{\phi(X)}.\]</span> Hence, we should choose <span class="math inline">\(\theta\)</span> so that <span class="math inline">\(\mathbb{E}_\theta\phi(X)=\overline{\phi(X)}\)</span>. i.e., the mean of the sufficient statistics matches the empirical mean from the data.</p>
<p><strong>Example 2.1</strong>    From <a href="#eg1.1">example 1.1</a>, we know that <span class="math inline">\(\overline{\phi(X)}=\overline{X}\)</span> and the MLE is <span class="math inline">\(\widehat{\lambda}=\overline{X}\)</span>.</p>
<p><strong>Example 2.2</strong>    Let <span class="math inline">\(X \sim \text{Binomial}(n, p)\)</span> with <span class="math inline">\(n\)</span> fixed. We have <span class="math display">\[\ln f(x; p)=x \ln p+(n-x)\ln(1-p)+C=x\ln\frac{p}{1-p}+n\ln(1-p)+C,\]</span> then <span class="math inline">\(\phi(x)=x\)</span>, <span class="math inline">\(\displaystyle \theta=\ln\frac{p}{1-p}\)</span>, and <span class="math inline">\(A(\theta)=-n\ln(1-p)=n\ln(1+e^\theta)\)</span>. Then <span class="math display">\[A&#39;(\theta)=\frac{ne^\theta}{1+e^\theta}=np\]</span> and <span class="math display">\[A&#39;&#39;(\theta)=\frac{ne^\theta}{(1+e^\theta)^2}=np(1-p).\]</span></p>
<p>Since <span class="math inline">\(\overline{\phi(X)}=\overline{X}\)</span> and thus the MLE is <span class="math inline">\(\displaystyle \widehat{p}=\overline{X}\)</span>.</p>
<h1 id="multivariate-gaussian-distribution">3. Multivariate Gaussian Distribution</h1>
<p><strong>Definition 3.1</strong>    Let <span class="math inline">\(X_V=(X_1, \ldots, X_p)^T \in \mathbb{R}^p\)</span> be a random vector. Let <span class="math inline">\(\mu \in \mathbb{R}^p\)</span> and <span class="math inline">\(\Sigma \in \mathbb{R}^{p \times p}\)</span> be a positive definite symmetric matrix. We say that <span class="math inline">\(X_V\)</span> has a <strong>multivariate Gaussian distribution</strong> or <strong>multivariate normal distribution</strong> with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> if for all <span class="math inline">\(x_V \in \mathbb{R}^p\)</span>, the joint density is <span class="math display">\[\begin{aligned}
f(x_V; \mu, \Sigma)&amp;=\frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}}\exp\left[-\frac{1}{2}(x_V-\mu)^T\Sigma^{-1}(x_V-\mu)\right]
\\&amp;=\frac{1}{(2\pi)^{p/2}} \exp\left[-\frac{1}{2}x_V^TKx_V+\mu^TKx_V-\frac{1}{2}\mu^TK\mu+\frac{1}{2}\ln|K|\right],
\end{aligned}\]</span> where the <strong>concentration matrix</strong> is <span class="math inline">\(K=\Sigma^{-1}\)</span>.</p>
<div class="note info">
            <p>Note that <span class="math display">\[\begin{aligned}-\frac{1}{2}x_V^TKx_V+\mu^TKx_V-\frac{1}{2}\mu^TK\mu+\frac{1}{2}\ln|K|&amp;=-\frac{1}{2}\text{Tr}(x_V^TKx_V)+\mu^TKx_V-A(\theta)\\&amp;=-\frac{1}{2}\text{Tr}(Kx_Vx_V^T)+\mu^TKx_V-A(\theta)\\&amp;=-\frac{1}{2}\text{vec}(K)^T\text{vec}(x_Vx_V^T)+\mu^TKx_V-A(\theta).\end{aligned}\]</span> Hence, the canonical parameter is <span class="math inline">\(\begin{bmatrix} -\frac{1}{2}\text{vec}(K) \\ K\mu \end{bmatrix}\)</span> and the sufficient statistic is <span class="math inline">\(\begin{bmatrix} \text{vec}(x_Vx_V^T) \\ x_V \end{bmatrix}\)</span>.</p>
          </div>
<h1 id="contingency-table">4. Contingency Table</h1>
<p><strong>Definition 4.1</strong>    Consider multivariate systems of vectors <span class="math inline">\(X_V=(X_v: v \in V)\)</span> for some set <span class="math inline">\(V=\{1, \ldots, p\}\)</span>, and <span class="math inline">\(X_A=(X_v: v \in A)\)</span> for any <span class="math inline">\(A \subseteq V\)</span>. We assume that each <span class="math inline">\(X_v \in \mathcal{X}_v=\{1, \ldots, d_v\}\)</span> (usually <span class="math inline">\(d_v=2\)</span>). If we have <span class="math inline">\(n\)</span> i.i.d. observations, then for <span class="math inline">\(i=1, \ldots, n\)</span>, <span class="math display">\[X_V^{(i)}=(X_1^{(i)}, \ldots, X_p^{(i)})^T.\]</span> Define <span class="math display">\[n(x_V)=\sum_{i=1}^n \mathbb{1}\{X_1^{(i)}=x_1, \ldots, X_p^{(i)}=x_p\},\]</span> i.e., the number of individuals who have the response pattern <span class="math inline">\(x_V\)</span>.</p>
<p>A <strong>marginal table</strong> only counts some of the variables: <span class="math display">\[n(x_A)=\sum_{i=1}^n \mathbb{1}\{X_a^{(i)}=x_a: a \in A\}=\sum_{x_{V-A}} n(x_A, x_{V-A}).\]</span></p>
<p><strong>Example 4.1</strong>    Consider a contingency table:</p>
<table style="table-layout:fixed">
<tbody>
<tr>
<th rowspan="2">
</th>
<th colspan="2">
<center>
Heart attack (<span class="math inline">\(X_1\)</span>)
</center>
</th>
<th rowspan="2">
<center>
Total
</center>
</th>
</tr>
<tr>
<th>
<center>
<span class="math inline">\(YE\)</span>
</center>
</th>
<th>
<center>
<span class="math inline">\(NO\)</span>
</center>
</th>
</tr>
<tr>
<th>
<center>
No aspirin (<span class="math inline">\(NA\)</span>)
</center>
</th>
<th>
<center>
<span class="math inline">\(28\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(656\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(684\)</span>
</center></th>
</tr>
<tr>
<th>
<center>
Aspirin (<span class="math inline">\(AS\)</span>)
</center>
</th>
<th>
<center>
<span class="math inline">\(18\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(658\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(676\)</span>
</center></th>
</tr>
<th>
<center>
Total
</center>
</th>
<th>
<center>
<span class="math inline">\(46\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(1314\)</span>
</center></th>
<th>
<center>
<span class="math inline">\(1360\)</span>
</center></th>

</tbody>
</table>
<p>Suppose <span class="math inline">\(x_V=(YE, NA)\)</span>, then <span class="math display">\[n(x_V)=\sum_{i=1}^n \mathbb{1}\{X_1^{(i)}=YE, X_2^{(i)}=NA\}=28.\]</span> Suppose <span class="math inline">\(A=\{1\}\)</span> and <span class="math inline">\(x_A=(YE)\)</span>. then <span class="math display">\[n(x_A)=\sum_{i=1}^n \mathbb{1}\{X_1^{(i)}=YE\}=46\]</span> or <span class="math display">\[n(x_A)=\sum_{x_2} n(x_A, x_2)=28+18=46.\]</span></p>
<h2 id="probabilistic-model">4.1. Probabilistic Model</h2>
<p>Suppose we have i.i.d. data, and we have <span class="math inline">\(P(X_V^{(i)}=x_V)=p(x_V)\)</span> for each <span class="math inline">\(X_v \in \{1, \ldots, d_v\}\)</span>. The distribution of the counts is <span class="math display">\[P(n(x_V): x_V \in \mathcal{X}_V)=\frac{n!}{\displaystyle \prod_{y_V \in \mathcal{X}_V}n(y_V)!}\prod_{y_V \in \mathcal{X}_V} p(y_V)^{n(y_V)}\]</span> for all <span class="math inline">\(p\)</span> s.t. <span class="math inline">\(\displaystyle \sum_{x_V} p(x_V)=1\)</span>.</p>
<p>Note that <span class="math display">\[\begin{aligned}
\ln P(n(x_V))&amp;=C+\sum_{y_V} n(y_V) \ln p(y_V)\ \ \left(\sum_{x_V} p(x_V)=1\right)
\\&amp;=C+\sum_{y_V \neq 0_V} n(y_V) \ln\frac{p(y_V)}{p(0_V)}+n \ln p(0_V).
\end{aligned}\]</span> Then the multinomial model is an exponential family with <span class="math inline">\(\phi(\mathbf{n})=\mathbf{n}\)</span> and <span class="math inline">\(\displaystyle \theta(x_V)=\ln\frac{p(x_V)}{p(0_V)}\)</span>. The MLE for multinomial is <span class="math inline">\(\displaystyle \widehat{\theta}(x_V)=\ln\frac{n(x_V)}{n(0_V)}\)</span>.</p>
<h1 id="log-linear-model">5. Log-Linear Model</h1>
<p><strong>Definition 5.1</strong>    Let <span class="math inline">\(p(x_V)&gt;0\)</span>, then the <strong>log-linear parameter</strong> <span class="math inline">\(\lambda_A(x_A)\)</span> for <span class="math inline">\(A \subseteq V\)</span> is defined by <span class="math display">\[\ln p(x_V)=\sum_{A \subseteq V} \lambda_A(x_A)=\lambda_\varnothing+\lambda_1(x_1)+\cdots+\lambda_V(x_V)\]</span> s.t. identifiability constraint <span class="math inline">\(\lambda_A(x_A)=0\)</span> if <span class="math inline">\(x_a=1\)</span> for any <span class="math inline">\(a \in A\)</span>.</p>
<div class="note info">
            <p>In the case of binary variables (i.e., each variable takes only two states, <span class="math inline">\(d_v=2\)</span>, or <span class="math inline">\(\mathcal{X}_v=\{1, 2\}\)</span>), there is only one possible non-zero level for each log-linear parameter <span class="math inline">\(\lambda_A(x_A)\)</span> which is when <span class="math inline">\(x_A=(2, \ldots, 2)\)</span>. In this case, we write <span class="math inline">\(\lambda_A=\lambda_A(2, \ldots, 2)\)</span>.</p>
          </div>
<p><strong>Example 5.1</strong>    Suppose <span class="math inline">\(\pi_{xy}=p(x, y)\)</span>, and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are binary. Then <span class="math inline">\(\ln \pi_{11}=\lambda_\varnothing\)</span>, <span class="math inline">\(\ln \pi_{21}=\lambda_\varnothing+\lambda_X\)</span>, <span class="math inline">\(\ln \pi_{12}=\lambda_\varnothing+\lambda_Y\)</span>, and <span class="math inline">\(\ln \pi_{22}=\lambda_\varnothing+\lambda_X+\lambda_Y+\lambda_{XY}\)</span>. Then <span class="math display">\[\lambda_{XY}=\ln\frac{\pi_{11}\pi_{22}}{\pi_{12}\pi_{21}}\]</span> where <span class="math inline">\(\displaystyle \frac{\pi_{11}\pi_{22}}{\pi_{12}\pi_{21}}\)</span> is the <strong>odds ratio</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p><strong>Theorem 5.1</strong>    Let <span class="math inline">\(p&gt;0\)</span> be a discrete distribution. Then <span class="math inline">\(X_a \perp\!\!\!\!\perp X_b \mid X_{V-\{a, b\}}\)</span> iff <span class="math inline">\(\lambda_{abC}=0\)</span> for all <span class="math inline">\(C \subseteq V-\{a, b\}\)</span>.</p>
<p><strong>Example 5.2</strong>    Suppose <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_C\)</span>, where <span class="math inline">\(V=A \cup B \cup C\)</span>, then <span class="math display">\[p(x_C)p(x_A, x_B, x_C)=p(x_A, x_C)p(x_B, x_C),\]</span> and <span class="math display">\[\ln p(x_A, x_B, x_C)=\ln p(x_A, x_C)+\ln p(x_B, x_C)-\ln p(x_C).\]</span> The log-linear expansion is <span class="math display">\[\sum_{W \subseteq V} \lambda_W(x_W)=\sum_{W \subseteq A \cup C} \lambda_W^{AC}(x_W)+\sum_{W \subseteq B \cup C} \lambda_W^{BC}(x_W)-\sum_{W \subseteq C} \lambda_W^C(x_W).\]</span></p>
<p>Hence, <span class="math inline">\(\lambda_W(x_W)=\lambda_W^{AC}(x_W)\)</span> for any <span class="math inline">\(W \subseteq A \cup C\)</span> with <span class="math inline">\(W \cap A \neq \varnothing\)</span>, <span class="math inline">\(\lambda_W(x_W)=\lambda_W^{BC}(x_W)\)</span> for any <span class="math inline">\(W \subseteq B \cup C\)</span> with <span class="math inline">\(W \cap B \neq \varnothing\)</span>, and <span class="math inline">\(\lambda_W(x_W)=\lambda_W^{AC}(x_W)+\lambda_W^{BC}(x_W)-\lambda_W^C(x_W)\)</span> for any <span class="math inline">\(W \subseteq C\)</span>.</p>
<p>Under this conditional independence, the log-linear parameters for <span class="math inline">\(p(x_V)\)</span> are easily obtainable from those for <span class="math inline">\(p(x_A, x_C)\)</span> and <span class="math inline">\(p(x_B, x_C)\)</span>.</p>
<p><strong><em><a href="https://github.com/litianyang0211/OxfordNotes/blob/main/Graphical%20Models/Log-Linear%20Model.ipynb">HERE</a></em></strong> is the full demo of log-linear model.</p>
<h2 id="poisson-multinomial-equivalence">5.1. Poisson-Multinomial Equivalence</h2>
<p><strong>Theorem 5.2</strong>    Let <span class="math inline">\(X_i \sim \text{Poisson}(\mu_i)\)</span> independently, and let <span class="math inline">\(\displaystyle N=\sum_{i=1}^k X_i\)</span>. Then <span class="math display">\[N \sim \text{Poisson}(\mu)\]</span> and <span class="math display">\[(X_1, \ldots, X_k)^T \mid N=n \sim \text{Multinom}(n, (\pi_1, \ldots, \pi_k)^T),\]</span> where <span class="math inline">\(\displaystyle \mu=\sum_{i=1}^k \mu_i\)</span> and <span class="math inline">\(\displaystyle \pi_i=\frac{\mu_i}{\mu}\)</span>.</p>
<strong><em>Proof.</em></strong> The Poisson likelihood is <span class="math display">\[\begin{aligned}
L(\mu_1, \ldots, \mu_k; x_1, \ldots, x_k) &amp;\propto \prod_{i=1}^k e^{-\mu_i}\mu_i^{x_i}
\\&amp;=\prod_{i=1}^k e^{-\mu\pi_i}\mu^{x_i}\pi_i^{x_i}
\\&amp;=\mu^{\sum_i x_i}e^{-\mu\sum_i \pi_i}\prod_{i=1}^k \pi_i^{x_i}
\\&amp;=\mu^ne^{-\mu}\prod_{i=1}^k \pi_i^{x_i}
\\&amp;=L(\mu; n) \cdot L(\pi_1, \ldots, \pi_k; x_1, \ldots, x_k \mid n).
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>Multinomial model can be fitted as Poisson GLM.</p>
          </div>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Contingency table</tag>
        <tag>Exponential family</tag>
      </tags>
  </entry>
  <entry>
    <title>Directed Graphical Model</title>
    <url>/Statistics/Graphical-model/Directed_Graphical_Model.html</url>
    <content><![CDATA[<h1 id="directed-graph">1. Directed Graph</h1>
<p><strong>Definition 1.1</strong>    A <strong>directed graph</strong> <span class="math inline">\(\mathcal{G}\)</span> is a pair <span class="math inline">\((V, D)\)</span> where <span class="math inline">\(V\)</span> is a finite set of <strong>vertices</strong>, and <span class="math inline">\(D \subseteq V \times V\)</span> is a set of <strong>ordered</strong> pairs <span class="math inline">\((i, j)\)</span> with <span class="math inline">\(i, j \in V\)</span> and <span class="math inline">\(i \neq j\)</span>. If <span class="math inline">\((i, j) \in D\)</span>, we write <span class="math inline">\(i \to j\)</span>. If <span class="math inline">\(i \to j\)</span> or <span class="math inline">\(i \leftarrow j\)</span>, we say <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are <strong>adjacent</strong> and write <span class="math inline">\(i \sim j\)</span>.</p>
<p><strong>Definition 1.2</strong>    A <strong>path</strong> is a sequence of adjacent vertices without repetition. The path is <strong>directed</strong> if all the arrows point away from the start.</p>
<p><strong>Example 1.1</strong>    Consider <span class="math inline">\(\mathcal{G}\)</span> with <span class="math inline">\(V=\{1, 2, 3, 4, 5\}\)</span> and <span class="math inline">\(D=\{(1, 3), (2, 3), (2, 4), (3, 5) ,(4, 5)\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p>Note that <span class="math inline">\(1 \to 3 \leftarrow 2 \to 4 \to 5\)</span> and <span class="math inline">\(1 \to 3 \to 5\)</span> are two paths from <span class="math inline">\(1\)</span> to <span class="math inline">\(5\)</span>, and the second path is directed.</p>
<p><strong>Definition 1.3</strong>    A <strong>directed cycle</strong> is a directed path from <span class="math inline">\(i\)</span> to <span class="math inline">\(j \neq i\)</span>, together with <span class="math inline">\(j \to i\)</span>.</p>
<p><strong>Definition 1.4</strong>    Graph that contains no directed cycles is called <strong>acyclic</strong>, or more specifically, <strong>directed acyclic graph (DAG)</strong>.</p>
<p><strong>Definition 1.5</strong>    If <span class="math inline">\(i \to j\)</span>, then <span class="math inline">\(i\)</span> is a <strong>parent</strong> of <span class="math inline">\(j\)</span>, and <span class="math inline">\(j\)</span> is a <strong>child</strong> of <span class="math inline">\(i\)</span>, denoted <span class="math inline">\(i \in \text{pa}_\mathcal{G}(j)\)</span> and <span class="math inline">\(j \in \text{ch}_\mathcal{G}(i)\)</span>. If <span class="math inline">\(a \to \cdots \to b\)</span> or <span class="math inline">\(a=b\)</span>, then <span class="math inline">\(a\)</span> is an <strong>ancestor</strong> of <span class="math inline">\(b\)</span>, and <span class="math inline">\(b\)</span> is a <strong>descendant</strong> of <span class="math inline">\(a\)</span>, denoted <span class="math inline">\(a \in \text{an}_\mathcal{G}(b)\)</span> and <span class="math inline">\(b \in \text{de}_\mathcal{G}(a)\)</span>. If <span class="math inline">\(w \notin \text{de}_\mathcal{G}(v)\)</span>, then <span class="math inline">\(w\)</span> is a <strong>non-descendant</strong> of <span class="math inline">\(v\)</span>, and we denote <span class="math inline">\(\text{nd}_\mathcal{G}(v)=V-\text{de}_\mathcal{G}(v)\)</span>.</p>
<div class="note warning">
            <p>A vertex is an ancestor and descendant of itself, and thus no vertex is a non-descendant of itself.</p>
          </div>
<p><strong>Example 1.2</strong>    Consider the graph <span class="math inline">\(\mathcal{G}\)</span>:</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p>We have <span class="math inline">\(\text{pa}_\mathbf{G}(3)=\{1, 2\}\)</span>, <span class="math inline">\(\text{ch}_\mathcal{G}(5)=\varnothing\)</span>, <span class="math inline">\(\text{an}_\mathcal{G}(4)=\{2, 4\}\)</span>, <span class="math inline">\(\text{de}_\mathcal{G}(1)=\{1, 3 ,5\}\)</span>, and <span class="math inline">\(\text{nd}_\mathcal{G}(1)=\{2, 4\}\)</span>.</p>
<p><strong>Definition 1.6</strong>    A <strong>topological ordering</strong> of the vertices of the graph is an ordering <span class="math inline">\(1, \ldots, k\)</span> s.t. <span class="math inline">\(i \in \text{pa}_\mathcal{G}(j)\)</span> implies that <span class="math inline">\(i&lt;j\)</span>. Acyclicity ensures that a topological ordering always exists.</p>
<p><strong>Example 1.3</strong>    Consider the graph <span class="math inline">\(\mathcal{G}\)</span>:</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p>The topological orderings are <span class="math display">\[\begin{aligned}
&amp;1, 2, 3, 4, 5
\\&amp;1, 2, 4, 3, 5
\\&amp;2, 1, 3, 4, 5
\\&amp;2, 1, 4, 3, 5
\\&amp;2, 4, 1, 3, 5
\end{aligned}\]</span></p>
<h1 id="markov-property">2. Markov Property</h1>
<div class="note no-icon">
            <p>We associate a <strong>model</strong> with each DAG via various Markov properties.</p>
          </div>
<p>Consider any multivariate distribution with density <span class="math inline">\(p(x_V)\)</span>, where <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>, and <span class="math inline">\(x_V=(x_1, \ldots, x_k)^T\)</span>. For all <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>, <span class="math inline">\(p\)</span> factorizes as <span class="math display">\[p(x_V)=\prod_{i=1}^k p(x_i \mid x_1, \ldots, x_{i-1})=\prod_{i=1}^k p(x_i \mid x_{\text{pre}&lt;(i)}).\]</span></p>
<p>A <strong>DAG model</strong> uses the form with a topological ordering of the graph, and states that the right-hand side of each factor only depends on the parents of <span class="math inline">\(i\)</span>.</p>
<p><strong>Definition 2.1</strong>    We say that <span class="math inline">\(p\)</span> <strong>factorizes</strong> according to a directed graph <span class="math inline">\(\mathcal{G}\)</span> if for all <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>, <span class="math display">\[p(x_V)=\prod_{i=1}^k p(x_i \mid x_{\text{pa}(i)}),\]</span> which implies that if ordering <span class="math inline">\(&lt;\)</span> is topological, for <span class="math inline">\(i \in V\)</span>, <span class="math display">\[p(x_i \mid x_{\text{pre}&lt;(i)})=p(x_i \mid x_{\text{pa}(i)}),\]</span> i.e., for all <span class="math inline">\(i \in V\)</span>, <span class="math display">\[X_i \perp\!\!\!\!\perp X_{\text{pre}&lt;(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p],\]</span> which is called the <strong>ordered local Markov property</strong> corresponding to the ordering of the variables.</p>
<p>Since the ordering is arbitrary provided that it is topological, we can pick <span class="math inline">\(&lt;\)</span> so that as many vertices come before <span class="math inline">\(i\)</span> as possible, then for all <span class="math inline">\(i \in V\)</span>, <span class="math display">\[X_i \perp\!\!\!\!\perp X_{\text{nd}(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p],\]</span> which is called the <strong>local Markov property</strong> for <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong>Example 2.1</strong>    Consider the graph <span class="math inline">\(\mathcal{G}\)</span>:</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p>If we apply ordered local Markov property, we have <span class="math display">\[\begin{aligned}
X_1 &amp;\perp\!\!\!\!\perp X_\varnothing \mid X_\varnothing \\
X_2 &amp;\perp\!\!\!\!\perp X_1\mid X_\varnothing \\
X_3 &amp;\perp\!\!\!\!\perp X_\varnothing \mid X_1, X_2 \\
X_4 &amp;\perp\!\!\!\!\perp X_1, X_3 \mid X_2 \\
X_5 &amp;\perp\!\!\!\!\perp X_1, X_2 \mid X_3, X_4
\end{aligned}\]</span> and get three independences <span class="math display">\[\begin{aligned}
X_2 &amp;\perp\!\!\!\!\perp X_1 \\
X_4 &amp;\perp\!\!\!\!\perp X_1, X_3 \mid X_2 \\
X_5 &amp;\perp\!\!\!\!\perp X_1, X_2 \mid X_3, X_4
\end{aligned}\]</span></p>
<p>If we apply local Markov property, we have <span class="math display">\[\begin{aligned}
X_1 &amp;\perp\!\!\!\!\perp X_2, X_4 \\
X_2 &amp;\perp\!\!\!\!\perp X_1 \\
X_3 &amp;\perp\!\!\!\!\perp X_4 \mid X_1, X_2 \\
X_4 &amp;\perp\!\!\!\!\perp X_1, X_3 \mid X_2 \\
X_5 &amp;\perp\!\!\!\!\perp X_1, X_2 \mid X_3, X_4
\end{aligned}\]</span></p>
<p>Moreover, for instance, since <span class="math inline">\(X_4 \perp\!\!\!\!\perp X_1, X_3 \mid X_2\)</span> and <span class="math inline">\(X_5 \perp\!\!\!\!\perp X_1, X_2 \mid X_3, X_4\)</span>, we have <span class="math inline">\(X_4 \perp\!\!\!\!\perp X_1 \mid X_2, X_3\)</span> and <span class="math inline">\(X_5 \perp\!\!\!\!\perp X_1 \mid X_2, X_3, X_4\)</span>, and thus <span class="math inline">\(X_1 \perp\!\!\!\!\perp X_4, X_5 \mid X_2, X_3\)</span>.</p>
<div class="note no-icon">
            <p>We might wonder if there is a way to tell all independences immediately from the graph, and thus we need the concept of <strong>global Markov property</strong>.</p>
          </div>
<p><strong>Definition 2.2</strong>    An <strong>ancestral set</strong> is the set that contains all its own ancestors, i.e., <span class="math inline">\(\text{an}(A)=A\)</span>.</p>
<p><strong>Example 2.2</strong>    Consider the graph <span class="math inline">\(\mathcal{G}\)</span>:</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p><span class="math inline">\(\{1, 2, 3\}\)</span> is ancestral, but <span class="math inline">\(\{4\}\)</span> is not ancestral.</p>
<p><span id="thm2.1"><strong>Theorem 2.1</strong></span>    Let <span class="math inline">\(A\)</span> be an ancestral set in <span class="math inline">\(\mathcal{G}\)</span>. If <span class="math inline">\(p(x_V)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math inline">\(p(x_A)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}_A\)</span>.</p>
<strong><em>Proof.</em></strong> Suppose <span class="math inline">\(p(x_V)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math display">\[p(x_V)=\prod_{i \in V} p(x_i \mid x_{\text{pa}(i)})=\prod_{i \in A} p(x_i \mid x_{\text{pa}(i)}) \prod_{i \in V-A} p(x_i \mid x_{\text{pa}(i)}).\]</span> Since <span class="math inline">\(A\)</span> is ancestral, then the first product does not depend on <span class="math inline">\(V-A\)</span>. Hence, for all <span class="math inline">\(x_A \in \mathcal{X}_A\)</span>, <span class="math display">\[p(x_A)=\sum_{x_{V-A}} p(x_V)=\prod_{i \in A} p(x_i \mid x_{\text{pa}(i)})\]</span> and thus <span class="math inline">\(p(x_A)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}_A\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>In particular, if we wish to interrogate whether a conditional independence <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_C\ [p]\)</span> holds under a DAG model, we can restrict to checking if the independence holds in the induced subgraph over the ancestral set <span class="math inline">\(\text{an}(A \cup B \cup C)\)</span>, i.e., <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_c\ [p(x_{\text{an}(A, B, C)})]\)</span>.</p>
          </div>
<p><strong>Definition 2.3</strong>    A <strong>v-structure</strong> is a triple <span class="math inline">\(i \to k \leftarrow j\)</span> s.t. <span class="math inline">\(i \not\sim j\)</span>. The <strong>moral graph</strong> <span class="math inline">\(\mathcal{G}^m\)</span> is formed by joining any parents in a v-structure, and dropping the direction of edges.</p>
<p><span id="thm2.2"><strong>Theorem 2.2</strong></span>    If <span class="math inline">\(p(x_V)\)</span> factorizes according to a DAG <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math inline">\(p(x_V)\)</span> factorizes according to the undirected graph <span class="math inline">\(\mathcal{G}^m\)</span>.</p>
<strong><em>Proof.</em></strong> If <span class="math inline">\(p(x_V)\)</span> factorizes according to a DAG <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math display">\[p(x_V)=\prod_{i \in V} p(x_i \mid x_{\text{pa}(i)}).\]</span> Note that <span class="math inline">\(\{i\} \cup \text{pa}(i)\)</span> is complete in <span class="math inline">\(\mathcal{G}^m\)</span>. Let <span class="math inline">\(C_1\)</span> be the first clique with the smallest <span class="math inline">\(i\)</span> and let <span class="math display">\[\psi_{C_1}(x_{C_1})=\prod_{i \in C_1} p(x_i \mid x_{\text{pa}(i)}).\]</span> Let <span class="math inline">\(C_k\)</span> be the <span class="math inline">\(k\)</span>th clique with <span class="math inline">\(k\)</span>th smallest <span class="math inline">\(i\)</span> and let <span class="math display">\[\psi_{C_k}(x_{C_k})=\prod_{i \in C_k} p(x_i \mid x_{\text{pa}(i)})\]</span> but without the terms included in <span class="math inline">\(\psi_{C_j}\)</span> where <span class="math inline">\(j&lt;k\)</span>. We can find all cliques and <span class="math display">\[p(x_V)=\prod_{C_k} \psi_{C_k}(x_{C_k}),\]</span> i.e., <span class="math inline">\(p(x_V)\)</span> factorizes according to the undirected graph <span class="math inline">\(\mathcal{G}^m\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 2.4</strong>    We say <span class="math inline">\(p(x_V)\)</span> satisfies the <strong>global Markov property</strong> w.r.t. a directed graph <span class="math inline">\(\mathcal{G}\)</span> if whenever <span class="math inline">\(A\)</span> is separated from <span class="math inline">\(B\)</span> by <span class="math inline">\(C\)</span> in <span class="math inline">\((\mathcal{G}_{\text{an}(A \cup B \cup C)})^m\)</span>, we have <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_C\ [p]\)</span>.</p>
<p><strong>Example 2.3</strong>    Consider the graph <span class="math inline">\(\mathcal{G}\)</span>:</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1)) & 2((2)) --> 3((3))
3 & 4((4)) --> 5((5))
2 --> 4
          </pre>
<p>Suppose we want to know whether <span class="math inline">\(X_1 \perp\!\!\!\!\perp X_5 \mid X_3\)</span>. The moral graph <span class="math inline">\((\mathcal{G}_{\text{an}(X_1 \cup X_3 \cup X_5)})^m=\mathcal{G}^m\)</span>, i.e.,</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) & 2((2)) --- 3((3))
3 & 4((4)) --- 5((5))
2 --- 4
1 --- 2
3 --- 4
          </pre>
<p><span class="math inline">\(\mathcal{G}^m\)</span> suggests that <span class="math inline">\(X_1\)</span> is not independent of <span class="math inline">\(X_5\)</span> given <span class="math inline">\(X_3\)</span>.</p>
<p>Suppose we want to know whether <span class="math inline">\(X_2 \perp\!\!\!\!\perp X_1 \mid X_4\)</span>. The moral graph <span class="math inline">\((\mathcal{G}_{\text{an}(X_1 \cup X_2 \cup X_4)})^m\)</span> is</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1))
2((2)) --- 4((4))
          </pre>
<p>Hence, <span class="math inline">\(X_2 \perp\!\!\!\!\perp X_1 \mid X_4\)</span>. Furthermore, <span class="math inline">\(X_1 \perp\!\!\!\!\perp X_2, X_4\)</span>.</p>
<p><span id="thm2.3"><strong>Theorem 2.3 (Completeness of Global Markov Property)</strong></span>    Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG, then there exists a probability distribution <span class="math inline">\(p\)</span> s.t. <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_C\ [p]\)</span> iff <span class="math inline">\(A \perp_s B \mid C\ [(\mathcal{G}_{\text{an}(A \cup B \cup C)})^m]\)</span>.</p>
<div class="note info">
            <p>Any independence not exhibited by a separation will not generally hold in distributions Markov to <span class="math inline">\(\mathcal{G}\)</span>. In other words, the global Markov property gives all conditional independences that are implied by the DAG model.</p>
          </div>
<p><strong>Theorem 2.4</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be a DAG and <span class="math inline">\(p\)</span> be a probability density. The following are equivalent:</p>
<p>    (1) <span class="math inline">\(p\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>;</p>
<p>    (2) <span class="math inline">\(p\)</span> is globally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>;</p>
<p>    (3) <span class="math inline">\(p\)</span> is locally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong><em>Proof.</em></strong> We need to show (1) <span class="math inline">\(\Rightarrow\)</span> (2) <span class="math inline">\(\Rightarrow\)</span> (3) <span class="math inline">\(\Rightarrow\)</span> (1).</p>
<p>(i) Let <span class="math inline">\(W=\text{an}(A \cup B \cup C)\)</span> and suppose there is a separation <span class="math inline">\(A \perp_s B \mid C\ [(\mathcal{G}_W)^m]\)</span>. By <a href="#thm2.1">theorem 2.1</a>, we have for all <span class="math inline">\(x_W\)</span>, <span class="math display">\[p(x_W)=\prod_{i \in W} p(x_i \mid x_{\text{pa}(i)}).\]</span> Then by <a href="#thm2.2">theorem 2.2</a>, we know <span class="math inline">\(p(x_W)\)</span> factorizes according to <span class="math inline">\((\mathcal{G}_W)^m\)</span>. Therefore, <span class="math inline">\(p(x_W)\)</span> satisfies <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_C\ [p]\)</span>. Since <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are arbitrary, then <span class="math inline">\(p\)</span> is globally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>(ii) Moralizing <span class="math inline">\(\mathcal{G}_{\{i\} \cup \text{nd}(i)}\)</span> will not add any edges adjacent to <span class="math inline">\(i\)</span>, and <span class="math inline">\(\{i\} \cup \text{nd}(i)\)</span> is an ancestral set. Hence, <span class="math display">\[i \perp_s \text{nd}(i)-\text{pa}(i) \mid \text{pa}(i)\ [(\mathcal{G}_{\{i\} \cup \text{nd}(i)})^m].\]</span> By the global Markov property, <span class="math inline">\(X_i \perp\!\!\!\!\perp X_{\text{nd}(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p]\)</span>, i.e., <span class="math inline">\(p\)</span> is locally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.</p>
(iii) We have <span class="math inline">\(X_i \perp\!\!\!\!\perp X_{\text{nd}(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p]\)</span>. Let <span class="math inline">\(1, \ldots, k\)</span> be a topological order. Note that for all <span class="math inline">\(i \in V\)</span>, <span class="math display">\[X_i \perp\!\!\!\!\perp X_{\text{pre}&lt;(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p].\]</span> Therefore, for all <span class="math inline">\(i \in V\)</span>, <span class="math display">\[p(x_i \mid x_{\text{pre}&lt;(i)})=p(x_i \mid x_{\text{pa}(i)})\]</span> and thus for all <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>, <span class="math display">\[p(x_V)=\prod_{i=1}^k p(x_i \mid x_{\text{pre}&lt;(i)})=\prod_{i=1}^k p(x_i \mid x_{\text{pa}(i)}).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="maximum-likelihood-estimation">3. Maximum Likelihood Estimation</h1>
<p>Consider a contingency table with counts <span class="math inline">\(n(x_V)\)</span>, the likelihood is <span class="math display">\[\begin{aligned}
l(p; n)&amp;=\sum_{x_V \in \mathcal{X}_V} n(x_V) \ln p(x_V)
\\&amp;=\sum_{x_V \in \mathcal{X}_V} n(x_V) \sum_{i \in V} \ln p(x_i \mid x_{\text{pa}(i)})
\\&amp;=\sum_{i \in V} \sum_{x_V \in \mathcal{X}_V} n(x_V)\ln p(x_i \mid x_{\text{pa}(i)})
\\&amp;=\sum_{i \in V} \sum_{x_i \in \mathcal{X}_i} \sum_{x_{\text{pa}(i)} \in \mathcal{X}_{\text{pa}(i)}} \ln p(x_i \mid x_{\text{pa}(i)}) \sum_{x_O \in \mathcal{X}_V-\mathcal{X}_i-\mathcal{X}_{\text{pa}(i)}} n(x_V)
\\&amp;=\sum_{i \in V} \sum_{x_{\text{pa}(i)} \in \mathcal{X}_{\text{pa}(i)}} \sum_{x_i \in \mathcal{X}_i} n(x_i, x_{\text{pa}(i)}) \ln p(x_i \mid x_{\text{pa}(i)})
\end{aligned}\]</span> where each of the conditional distributions <span class="math inline">\(p(x_i \mid x_{\text{pa}(i)})\)</span> can be dealt with separately, i.e., we can separately maximize each inner sum <span class="math inline">\(\displaystyle \sum_{x_i \in \mathcal{X}_i} n(x_i, x_{\text{pa}(i)}) \ln p(x_i \mid x_{\text{pa}(i)})\)</span> s.t. <span class="math inline">\(\displaystyle \sum_{x_i \in \mathcal{X}_i} p(x_i \mid x_{\text{pa}(i)})=1\)</span>. Hence, the MLE of <span class="math inline">\(\mathcal{G}\)</span> is for all <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_\text{pa}(i)\)</span>, <span class="math display">\[\widehat{p}(x_i \mid x_{\text{pa}(i)})=\frac{n(x_i, x_{\text{pa}(i)})}{n(x_{\text{pa}(i)})}.\]</span></p>
<p>Suppose each <span class="math inline">\(v \in V\)</span> has a model for <span class="math inline">\(p(x_v \mid x_{\text{pa}(v)})\)</span> and we have independent priors for each factor, i.e., <span class="math display">\[\pi(\theta)=\prod_{v \in V} \pi(\theta_v).\]</span> Therefore <span class="math display">\[p(x_V; \theta)=\prod_{i \in V} p(x_i \mid x_{\text{pa}(i)}; \theta_i)\]</span> and thus <span class="math display">\[\pi(\theta)p(x_V; \theta)=\prod_{i \in V} p(x_i \mid x_{\text{pa}(i)}; \theta_i)\pi(\theta_i).\]</span> Hence, <span class="math display">\[\theta_i \perp\!\!\!\!\perp X_{V-(\{i\} \cup \text{pa}(i))}, \theta_{-i} \mid X_i, X_{\text{pa}(i)}.\]</span></p>
<h1 id="markov-equivalence">4. Markov Equivalence</h1>
<p><strong>Definition 4.1</strong>    <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are <strong>Markov equivalent</strong> if any <span class="math inline">\(p\)</span> which is Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span> is also Markov w.r.t. <span class="math inline">\(\mathcal{G}&#39;\)</span>, and vice versa.</p>
<p><strong>Definition 4.2</strong>    Let <span class="math inline">\(\mathcal{G}=(V, D)\)</span> be a DAG, then the <strong>skeleton</strong> of <span class="math inline">\(\mathcal{G}\)</span> is the undirected graph <span class="math inline">\(\text{skel}(\mathcal{G})\)</span> obtained by dropping the orientations.</p>
<p><strong>Lemma 4.1</strong>    If <span class="math inline">\(\text{skel}(\mathcal{G}) \neq \text{skel}(\mathcal{G}&#39;)\)</span>, then <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are not Markov equivalent.</p>
<p><strong><em>Proof.</em></strong> Suppose w.l.o.g. that <span class="math inline">\(i \to j\)</span> in <span class="math inline">\(\mathcal{G}\)</span> but not in <span class="math inline">\(\mathcal{G}&#39;\)</span>. Let <span class="math display">\[p(x_V)=p(x_j \mid x_i) \prod_{k \neq j} p(x_k).\]</span> Since <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are dependent, and there is no way that we can separate them in a moral graph, then <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> cannot be conditionally independent given any subset of <span class="math inline">\(V-\{i, j\}\)</span>.</p>
In <span class="math inline">\(\mathcal{G}&#39;\)</span>, <span class="math inline">\(X_i \perp\!\!\!\!\perp X_{\text{nd}(i)-\text{pa(i)}} \mid X_{\text{pa}(i)}\)</span> and <span class="math inline">\(X_j \perp\!\!\!\!\perp X_{\text{nd}(j)-\text{pa(j)}} \mid X_{\text{pa}(j)}\)</span>. Therefore, <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> are conditionally independent given some subsets of <span class="math inline">\(V-\{i, j\}\)</span>. Hence, <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are not Markov equivalent.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.2</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> be DAGs, then <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are Markov equivalent iff they have the same skeletons and v-structures.</p>
<p><strong><em>Proof.</em></strong> We only show the only if direction.</p>
<p>If <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> have different skeletons, then <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are not Markov equivalent.</p>
<p>Suppose now <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> have same skeletons, and <span class="math inline">\(\mathcal{G}\)</span> has a v-structure <span class="math inline">\(i \to k \leftarrow j\)</span> but <span class="math inline">\(\mathcal{G}&#39;\)</span> does not. Let <span class="math display">\[p(x_V)=p(x_k \mid x_i, x_j) \prod_{l \neq k} p(x_l)\]</span> for all <span class="math inline">\(x_V \in \mathcal{X}_V\)</span>. Note that <span class="math inline">\(p\)</span> factorizes w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.</p>
Since <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> have same skeletons, then either <span class="math inline">\(i \to k \to j\)</span> or <span class="math inline">\(i \leftarrow k \to j\)</span> or <span class="math inline">\(i \leftarrow k \leftarrow j\)</span>. Suppose there exists a <span class="math inline">\(d \in V\)</span> s.t. <span class="math inline">\(d \in W=\text{an}_{\mathcal{G}&#39;}(\{i, j, k\})\)</span> and <span class="math inline">\(i \to d \leftarrow j\)</span>, then we can find contradiction immediately (the graph is cyclic). Hence, there is no common child of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> in <span class="math inline">\(\mathcal{G}&#39;\)</span>, and thus <span class="math inline">\(i \not\sim j\)</span> in <span class="math inline">\((\mathcal{G}&#39;_W)^m\)</span>. Therefore, <span class="math inline">\(X_i \perp\!\!\!\!\perp X_j \mid X_{W-\{i, j\}}\)</span>. Since either <span class="math inline">\(k \in \text{pa}_{\mathcal{G}&#39;}(i)\)</span> or <span class="math inline">\(k \in \text{pa}_{\mathcal{G}&#39;}(j)\)</span>, then <span class="math inline">\(k \in W\)</span>. However, the independence does not hold in <span class="math inline">\(p\)</span> since we have a factor <span class="math inline">\(p(x_k \mid x_i, x_j)\)</span>. Hence, <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}&#39;\)</span> are not Markov equivalent.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="decomposability">5. Decomposability</h1>
<p><strong>Theorem 5.1</strong>    A DAG <span class="math inline">\(\mathcal{G}\)</span> is Markov equivalent to an undirected graph iff it contains no v-structures. In this case, the equivalent undirected graph is the skeleton of <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong><em>Proof.</em></strong> (<span class="math inline">\(\Leftarrow\)</span>) We know that if <span class="math inline">\(p\)</span> is Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>, then it is also Markov w.r.t. <span class="math inline">\(\mathcal{G}^m\)</span>. Since there is no v-structure, then <span class="math inline">\(\mathcal{G}^m=\text{skel}(\mathcal{G})\)</span>.</p>
<p>Suppose <span class="math inline">\(p\)</span> is Markov w.r.t. <span class="math inline">\(\mathcal{G}^m\)</span>. Let <span class="math inline">\(v\)</span> be a vertex with no child in <span class="math inline">\(\mathcal{G}\)</span>. Since <span class="math inline">\(v\)</span> does not have child and <span class="math inline">\(\text{pa}_\mathcal{G}(v)\)</span> must be complete in <span class="math inline">\(\mathcal{G}^m\)</span>, then <span class="math inline">\(v \perp_s V-(\text{pa}_\mathcal{G}(v) \cup \{v\}) \mid \text{pa}_\mathcal{G}(v)\ [\mathcal{G}^m]\)</span>. Hence, by the global Markov property for <span class="math inline">\(\mathcal{G}^m\)</span>, <span class="math inline">\(X_v \perp\!\!\!\!\perp X_{V-(\text{pa}_\mathcal{G}(v) \cup \{v\})} \mid X_{\text{pa}_\mathcal{G}(v)}\ [p]\)</span>, i.e., <span class="math inline">\(X_v \perp\!\!\!\!\perp X_{\text{nd}_\mathcal{G}(v)-\text{pa}_\mathcal{G}(v)} \mid X_{\text{pa}_\mathcal{G}(v)}\ [p]\)</span>.</p>
<p><span class="math inline">\(\mathcal{G}_{-v}\)</span> also has no v-structure, and so by induction, <span class="math inline">\(p(x_{V-\{v\}})\)</span> is Markov w.r.t. <span class="math inline">\(\mathcal{G}_{-v}\)</span>, and thus for all <span class="math inline">\(i \in V-\{v\}\)</span>, <span class="math inline">\(X_i \perp\!\!\!\!\perp X_{\text{nd}(i)-\text{pa}(i)} \mid X_{\text{pa}(i)}\ [p]\)</span>. Since <span class="math inline">\(X_v \perp\!\!\!\!\perp X_{\text{nd}_\mathcal{G}(v)-\text{pa}_\mathcal{G}(v)} \mid X_{\text{pa}_\mathcal{G}(v)}\ [p]\)</span>, then local Markov property for <span class="math inline">\(\mathcal{G}\)</span> holds. Hence, <span class="math inline">\(p\)</span> is Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>, and thus <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(\mathcal{G}^m\)</span> are Markov equivalent.</p>
<p>(<span class="math inline">\(\Rightarrow\)</span>) If <span class="math inline">\(\mathcal{G}\)</span> has a v-structure <span class="math inline">\(i \to k \leftarrow j\)</span>, then there is an independence between <span class="math inline">\(X_i\)</span> and <span class="math inline">\(X_j\)</span> by the local Markov property. If we add the edge <span class="math inline">\(i \sim j\)</span>, then the undirected graph will not be Markov equivalent to <span class="math inline">\(\mathcal{G}\)</span>.</p>
However, since <span class="math inline">\(i \sim j\)</span> in <span class="math inline">\(\mathcal{G}^m\)</span>, by <a href="#thm2.3">theorem 2.3</a>, we know there exists a distribution, Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>, where <span class="math inline">\(X_i\)</span> is not conditionally independent of <span class="math inline">\(X_j\)</span> given <span class="math inline">\(X_{V-\{i, j\}}\)</span>. Hence, if we do not add the edge <span class="math inline">\(i \sim j\)</span>, we also will not obtain a Markov equivalent graph. Hence, no undirected graph can be Markov equivalent to <span class="math inline">\(\mathcal{G}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 5.2</strong>    A undirected graph is Markov equivalent to a directed graph iff the undirected graph is decomposable.</p>
<div class="note info">
            <p>Decomposable models represent the intersection of undirected and directed graphical models.</p>
          </div>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Directed graphical model</tag>
      </tags>
  </entry>
  <entry>
    <title>Undirected Graphical Model</title>
    <url>/Statistics/Graphical-model/Undirected_Graphical_Model.html</url>
    <content><![CDATA[<h1 id="undirected-graph">1. Undirected Graph</h1>
<p><strong>Definition 1.1</strong>    Let <span class="math inline">\(V\)</span> be a finite set. An <strong>undirected graph</strong> is a pair <span class="math inline">\(\mathcal{G}=(V, E)\)</span> where <span class="math inline">\(V\)</span> is a finite set of vertices, and <span class="math inline">\(E \subseteq \{\{i, j\}: i, j \in V, i \neq j\}\)</span> is a set of unordered distinct pairs of <span class="math inline">\(V\)</span>, called <strong>edges</strong>. If <span class="math inline">\(\{i, j\} \in E\)</span>, we say <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are <strong>adjacent</strong>, and we write <span class="math inline">\(i \sim j\)</span>. The vertices adjacent to <span class="math inline">\(i\)</span> are called the <strong>neighbors</strong> of <span class="math inline">\(i\)</span>, and the set of neighbors is called the <strong>boundary</strong> of <span class="math inline">\(i\)</span>, denoted <span class="math inline">\(\text{bd}_\mathcal{G}(i)\)</span>.</p>
<p><strong>Example 1.1</strong>    The graph has <span class="math inline">\(V=\{1, 2, 3, 4\}\)</span>, and <span class="math inline">\(E=\{\{1, 2\}, \{2, 3\}, \{3, 4\}\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2)) --- 3((3)) --- 4((4))
          </pre>
<h1 id="markov-property">2. Markov Property</h1>
<p>We associate a collection of probability distributions with a graph via a <strong>Markov property</strong>. Vertices correspond to random variables, and the absence of an edge corresponds to some kind of independence.</p>
<p><span id="def2.1"><strong>Definition 2.1</strong></span>    Let <span class="math inline">\(\mathcal{G}=(V, E)\)</span> be an undirected graph, and <span class="math inline">\(p(x_V)\)</span> be a probability distribution over r.v.s. <span class="math inline">\(X_V \in \mathcal{X}_V=\times_{v \in V} \mathcal{X}_v\)</span>. We say that <span class="math inline">\(p\)</span> obeys the <strong>pairwise Markov property</strong> w.r.t. <span class="math inline">\(\mathcal{G}\)</span> if whenever <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> are not adjacent, <span class="math display">\[X_i \perp\!\!\!\!\perp X_j \mid X_{V-\{i, j\}}\ [p].\]</span></p>
<p><strong>Example 2.1</strong>    A distribution is pairwise Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span> iff <span class="math inline">\(X_1 \perp\!\!\!\!\perp X_4 \mid X_2, X_3\)</span> and <span class="math inline">\(X_2 \perp\!\!\!\!\perp X_4 \mid X_1, X_3\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p>If <span class="math inline">\(p&gt;0\)</span>, then <span class="math inline">\(X_1, X_2 \perp\!\!\!\!\perp X_4 \mid X_3\)</span>.</p>
<p><strong>Definition 2.2</strong>    A set of vertices <span class="math inline">\(C \subseteq V\)</span> is <strong>complete</strong> if every <span class="math inline">\(i, j \in C\)</span> are adjacent. We say <span class="math inline">\(C\)</span> is a <strong>clique</strong> if it is an inclusion maximal complete set.</p>
<p><strong>Example 2.2</strong>    <span class="math inline">\(\mathcal{C}(\mathcal{G})=\{\{1, 2\}, \{2, 3\}, \{3, 4\}, \{1, 4\}\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2))
1 --- 4((4)) --- 3((3))
2 --- 3
          </pre>
<p><strong>Example 2.3</strong>    <span class="math inline">\(\mathcal{C}(\mathcal{G})=\{\{1, 2, 3\}, \{3, 4\}\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><strong>Definition 2.3</strong>    We say a distribution satisfying the conditions of <a href="#def2.1">definition 2.1</a> <strong>factorizes</strong> according to <span class="math inline">\(\mathcal{G}\)</span> if we can write <span class="math display">\[p(x_V)=\prod_{C \in \mathcal{C}(\mathcal{G})} \psi_C(x_C), \psi_C \geq 0.\]</span></p>
<p><strong>Example 2.4</strong>    We say the distribution factorizes according to the graph if <span class="math inline">\(p(x_{1234})=\psi_{123}(x_{123})\psi_{34}(x_{34})\)</span>, which implies <span class="math inline">\(X_1, X_2 \perp\!\!\!\!\perp X_4 \mid X_3\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><strong>Definition 2.4</strong>    A <strong>path</strong> is a sequence of adjacent vertices without repetition.</p>
<p><strong>Definition 2.5</strong>    Let <span class="math inline">\(A, B, S \subseteq V\)</span>, we say <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>separated</strong> by <span class="math inline">\(S\)</span> if every path from any <span class="math inline">\(a \in A\)</span> to <span class="math inline">\(b \in B\)</span> includes some vertex in <span class="math inline">\(S\)</span>. If <span class="math inline">\(S=\varnothing\)</span>, then <span class="math inline">\(A \perp_s B\)</span> iff they are in disconnected components of <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong>Example 2.5</strong>    <span class="math inline">\(\{1, 2\} \perp_s \{4\} \mid \{3\}\)</span>.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><strong>Definition 2.6</strong>    Let <span class="math inline">\(W \subseteq V\)</span>, the <strong>induced subgraph</strong> over <span class="math inline">\(W\)</span>, denoted <span class="math inline">\(\mathcal{G}_W\)</span>, has vertices consisting of <span class="math inline">\(W\)</span>, and edges <span class="math inline">\(E_W=\{\{i, j\} \in E, i, j \in W\}\)</span>.</p>
<p><strong>Example 2.6</strong>   <br>
</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><span class="math inline">\(\mathcal{G}_{\{1, 4\}}\)</span> is</p>
<pre class="mermaid" style="text-align: center;">
            graph TB
            1((1))
4((4))
          </pre>
<p><span class="math inline">\(\mathcal{G}_{\{1, 2, 3\}}\)</span> is</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2)) --- 3((3)) --- 1
          </pre>
<div class="note info">
            <p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are separated by <span class="math inline">\(S\)</span> (where <span class="math inline">\(S \cap A=S \cap B=\varnothing\)</span>) iff <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are separated by <span class="math inline">\(\varnothing\)</span> in <span class="math inline">\(\mathcal{G}_{V-S}\)</span>.</p>
          </div>
<p><strong>Definition 2.7</strong>    We say that <span class="math inline">\(p(x_V)\)</span> obeying all the conditions of <a href="#def2.1">definition 2.1</a> is <strong>globally Markov</strong> w.r.t. <span class="math inline">\(\mathcal{G}\)</span> if whenever <span class="math display">\[A \perp_s B \mid S\ [\mathcal{G}] \Rightarrow X_A \perp\!\!\!\!\perp X_B \mid X_S\ [p].\]</span></p>
<p><strong>Theorem 2.1</strong>    The global Markov property implies the pairwise Markov property.</p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(i \not\sim j\)</span> in <span class="math inline">\(\mathcal{G}\)</span>. Since any path from <span class="math inline">\(i\)</span> to <span class="math inline">\(j\)</span> must pass through some <span class="math inline">\(k \in V-\{i, j\}\)</span>, then <span class="math display">\[i \perp_s j \mid V-\{i, j\}\ [\mathcal{G}] \Rightarrow X_i \perp\!\!\!\!\perp X_j \mid X_{V-\{i, j\}}\ [p].\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.2</strong>    If <span class="math inline">\(p(x_V)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math inline">\(p(x_V)\)</span> is globally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(A \perp_s B \mid S\)</span>. Let <span class="math inline">\(\widetilde{A}=\{v \in V: \{v\} \not\perp_s A \mid S\}\)</span> (i.e., any vertex in <span class="math inline">\(\widetilde{A}\)</span> reaches vertex in <span class="math inline">\(A\)</span> without going through <span class="math inline">\(S\)</span>). Let <span class="math inline">\(\widetilde{B}=V-(\widetilde{A} \cup S)\)</span> so that <span class="math inline">\(\widetilde{A} \perp_s \widetilde{B} \mid S\)</span>, <span class="math inline">\(V=\widetilde{A} \dot\cup \widetilde{B} \dot\cup S\)</span>, <span class="math inline">\(A \subseteq \widetilde{A}\)</span>, and <span class="math inline">\(B \subseteq \widetilde{B}\)</span>. Furthermore, every clique is a subset of either <span class="math inline">\(\widetilde{A} \cup S\)</span> or <span class="math inline">\(\widetilde{B} \cup S\)</span>, since there are no edges between <span class="math inline">\(\widetilde{A}\)</span> and <span class="math inline">\(\widetilde{B}\)</span>.</p>
Let <span class="math inline">\(\mathcal{C}_A(\mathcal{G})=\{C \in \mathcal{C}(\mathcal{G}): C \subseteq \widetilde{A} \cup S\}\)</span> and <span class="math inline">\(\mathcal{C}_B(\mathcal{G})=\mathcal{C}(\mathcal{G})-\mathcal{C}_A(\mathcal{G})\)</span>, then <span class="math display">\[p(x_V)=\prod_{C \in \mathcal{C}(\mathcal{G})} \psi_C(x_C)=\prod_{C \in \mathcal{C}_A(\mathcal{G})} \psi_C(x_C) \cdot \prod_{D \in \mathcal{C}_B(\mathcal{G})} \psi_D(x_D)=f(x_{\widetilde{A}}, x_S)g(x_{\widetilde{B}}, x_S).\]</span> Hence, <span class="math inline">\(X_{\widetilde{A}} \perp\!\!\!\!\perp X_{\widetilde{B}} \mid X_S\)</span>. Since <span class="math inline">\(A \subseteq \widetilde{A}\)</span>, and <span class="math inline">\(B \subseteq \widetilde{B}\)</span>, then by Graphoid axioms, <span class="math inline">\(X_A \perp\!\!\!\!\perp X_B \mid X_S\ [p]\)</span>. Hence <span class="math inline">\(p\)</span> is globally Markov w.r.t. <span class="math inline">\(\mathcal{G}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.3 (Hammersley-Clifford Theorem)</strong>    If <span class="math inline">\(p(x_V)&gt;0\)</span> has the pairwise Markov property w.r.t. <span class="math inline">\(\mathcal{G}\)</span>, then <span class="math inline">\(p\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="note info">
            <p><span class="math inline">\(p \in M_p(\mathcal{G}) \Leftarrow p \in M_g(\mathcal{G}) \Leftarrow p \in M_f(\mathcal{G})\)</span>. If <span class="math inline">\(p&gt;0\)</span>, then <span class="math inline">\(p \in M_f(\mathcal{G}) \Leftarrow p \in M_p(\mathcal{G})\)</span>.</p>
          </div>
<h1 id="decomposability">3. Decomposability</h1>
<p><strong>Definition 3.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be an undirected graph, with vertices <span class="math inline">\(V=A \dot\cup B \dot\cup S\)</span>. We say <span class="math inline">\((A, S, B)\)</span> is a <strong>decomposition</strong> of <span class="math inline">\(\mathcal{G}\)</span> if <span class="math inline">\(\mathcal{G}_S\)</span> is complete, and <span class="math inline">\(A \perp_s B \mid S\ [\mathcal{G}]\)</span>. We say the decomposition is <strong>proper</strong> if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are non-empty.</p>
<p><strong>Example 3.1</strong>    For <span class="math inline">\(A=\{1, 2\}\)</span>, <span class="math inline">\(B=\{4\}\)</span> and <span class="math inline">\(S=\{3\}\)</span>, <span class="math inline">\((A, S, B)\)</span> is a proper decomposition. For <span class="math inline">\(A=\{1\}\)</span>, <span class="math inline">\(B=\{4\}\)</span> and <span class="math inline">\(S=\{2, 3\}\)</span>, <span class="math inline">\((A, S, B)\)</span> is a proper decomposition.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><strong>Example 3.2</strong>    The graph below has no proper decomposition.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2))
1 --- 4((4)) --- 3((3))
2 --- 3
          </pre>
<p><strong>Definition 3.2</strong>    We say that <span class="math inline">\(\mathcal{G}\)</span> is <strong>decomposable</strong> if it is either complete, or there is a proper decomposition <span class="math inline">\((A, S, B)\)</span>, and <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are decomposable.</p>
<p><strong>Example 3.3</strong>    For <span class="math inline">\(A=\{1, 2\}\)</span>, <span class="math inline">\(B=\{4\}\)</span> and <span class="math inline">\(S=\{3\}\)</span>, <span class="math inline">\((A, S, B)\)</span> is a proper decomposition. Both <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are complete, then <span class="math inline">\(\mathcal{G}\)</span> is decomposable.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) --- 4((4))
1 --- 2((2)) --- 3
          </pre>
<p><strong>Example 3.4</strong>    For <span class="math inline">\(A=\{1, 2\}\)</span>, <span class="math inline">\(B=\{5, 6\}\)</span> and <span class="math inline">\(S=\{3, 4\}\)</span>, <span class="math inline">\((A, S, B)\)</span> is a proper decomposition. Since <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are decomposable, then <span class="math inline">\(\mathcal{G}\)</span> is decomposable.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2))
2 --- 3((3)) --- 6((6))
2 --- 4((4)) --- 6((6))
3 --- 4
4 --- 5((5))
          </pre>
<p><strong>Definition 3.3</strong>    Let <span class="math inline">\(\mathcal{C}\)</span> be a collection of subsets of <span class="math inline">\(V\)</span>. We say that <span class="math inline">\(\mathcal{C}\)</span> has the <strong>running intersection property</strong> if we can order the elements <span class="math inline">\(C_1, \ldots, C_k\)</span> s.t. for all <span class="math inline">\(j \geq 2\)</span>, <span class="math display">\[C_j \cap \left(\bigcup_{i&lt;j} C_i\right)=C_j \cap C_{\sigma(j)},\]</span> where <span class="math inline">\(\sigma(j)&lt;j\)</span>.</p>
<p><strong>Example 3.5</strong>    <span class="math inline">\(\{1, 2\}\)</span>, <span class="math inline">\(\{2, 3, 4\}\)</span>, <span class="math inline">\(\{3, 4, 6\}\)</span> and <span class="math inline">\(\{4, 5\}\)</span> satisfy the running intersection property.</p>
<table style="table-layout:fixed">
<tbody>
<tr>
<td>
<center>
<span class="math inline">\(C\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{1, 2\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{2, 3, 4\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{3, 4, 6\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{4, 5\}\)</span>
</center>
</td>
</tr>
<tr>
<td>
<center>
Separator set <span class="math inline">\(S\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{2\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{3, 4\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{4\}\)</span>
</center>
</td>
</tr>
<tr>
<td>
<center>
<span class="math inline">\(\sigma(j)\)</span>
</center>
</td>
<td>
<center>
</center>
</td>
<td>
<center>
<span class="math inline">\(1\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(2\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span>
</center>
</td>
</tr>
</tbody>
</table>
<p><strong>Example 3.6</strong>    <span class="math inline">\(\{1, 2\}\)</span>, <span class="math inline">\(\{2, 3\}\)</span>, <span class="math inline">\(\{3, 4\}\)</span> and <span class="math inline">\(\{1, 4\}\)</span> do not satisfy the running intersection property.</p>
<table style="table-layout:fixed">
<tbody>
<tr>
<td>
<center>
<span class="math inline">\(C\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{1, 2\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{2, 3\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{3, 4\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{1, 4\}\)</span>
</center>
</td>
</tr>
<tr>
<td>
<center>
Separator set <span class="math inline">\(S\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(-\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{2\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{3\}\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(\{1, 4\}\)</span>
</center>
</td>
</tr>
<tr>
<td>
<center>
<span class="math inline">\(\sigma(j)\)</span>
</center>
</td>
<td>
<center>
</center>
</td>
<td>
<center>
<span class="math inline">\(1\)</span>
</center>
</td>
<td>
<center>
<span class="math inline">\(2\)</span>
</center>
</td>
<td>
<center>
N/A
</center>
</td>
</tr>
</tbody>
</table>
<p><strong>Definition 3.4</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be an undirected graph. A <strong>cycle</strong> is a sequence of vertices <span class="math inline">\(\{v_1, v_2, \ldots, v_k\}\)</span> for <span class="math inline">\(k \geq 3\)</span> s.t. <span class="math inline">\(v_1-\ldots-v_k\)</span> and <span class="math inline">\(v_1 \sim v_k\)</span>. A <strong>chord</strong> on a cycle is an edge between two vertices that are not adjacent on the cycle. A graph is <strong>triangulated</strong> if it does not contain any <strong>chordless</strong> cycles of length at least <span class="math inline">\(4\)</span>.</p>
<p><strong>Theorem 3.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be an undirected graph. The following are equivalent:</p>
<p>    (1) <span class="math inline">\(\mathcal{G}\)</span> is decomposable;</p>
<p>    (2) <span class="math inline">\(\mathcal{G}\)</span> is triangulated;</p>
<p>    (3) every minimal <span class="math inline">\((a, b)\)</span>-separator is complete in <span class="math inline">\(\mathcal{G}\)</span> (i.e., <span class="math inline">\(a \perp_s b \mid S \Rightarrow a \not\perp_s b \mid T \subset S\)</span>);</p>
<p>    (4) <span class="math inline">\(\mathcal{C}(\mathcal{G})\)</span> satisfies the running intersection property.</p>
<p><strong><em>Proof.</em></strong> We need to show (1) <span class="math inline">\(\Rightarrow\)</span> (2) <span class="math inline">\(\Rightarrow\)</span> (3) <span class="math inline">\(\Rightarrow\)</span> (4) <span class="math inline">\(\Rightarrow\)</span> (1).</p>
<p>(i) Suppose <span class="math inline">\(\mathcal{G}\)</span> is decomposable. Let <span class="math inline">\(p=|V|\)</span>. If <span class="math inline">\(\mathcal{G}\)</span> is complete, then it is clearly triangulated, so the result holds for <span class="math inline">\(p=1\)</span>. If <span class="math inline">\(\mathcal{G}\)</span> is not complete, then by definition, there exists a proper decomposition <span class="math inline">\((A, S, B)\)</span> so that <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> have strictly fewer vertices than <span class="math inline">\(\mathcal{G}\)</span> and are decomposable. By induction hypothesis, <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are triangulated. Suppose there is a cycle containing <span class="math inline">\(a \in A\)</span> and <span class="math inline">\(b \in B\)</span>, then the cycle passes twice through <span class="math inline">\(S\)</span>, which is complete, then there is a chord on the cycle. Hence, <span class="math inline">\(\mathcal{G}\)</span> is triangulated.</p>
<p>(ii) Suppose <span class="math inline">\(\mathcal{G}\)</span> is triangulated and <span class="math inline">\(S\)</span> separates <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> minimally. If <span class="math inline">\(S\)</span> is not complete, then there exist <span class="math inline">\(s_1, s_2 \in S\)</span> s.t. <span class="math inline">\(s_1 \not\sim s_2\)</span>. Let <span class="math inline">\(\pi_1\)</span> be the path from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> via <span class="math inline">\(s_1 \in S\)</span>, and <span class="math inline">\(\pi_2\)</span> be the path from <span class="math inline">\(a\)</span> to <span class="math inline">\(b\)</span> via <span class="math inline">\(s_2 \in S\)</span>, and neither these paths intersects any other element of <span class="math inline">\(S\)</span>. Let <span class="math inline">\(a&#39;\)</span> be the vertex on <span class="math inline">\(\pi_1\)</span>, which is closest to <span class="math inline">\(s_1\)</span> on the same side as <span class="math inline">\(a\)</span>, and is also on <span class="math inline">\(\pi_2\)</span>. We reduce <span class="math inline">\(\pi_1\)</span> to start at <span class="math inline">\(a&#39;\)</span>, and the same for <span class="math inline">\(\pi_2\)</span>. We repeat it on the side of <span class="math inline">\(b\)</span>. Hence, we have a chordless cycle of length at least <span class="math inline">\(4\)</span>, which is a contradiction.</p>
<p>(iii) If the graph is complete, there is nothing to proof since <span class="math inline">\(\mathcal{C}(\mathcal{G})=V\)</span>. Let <span class="math inline">\(p=|V|\)</span>, the the result holds trivially for <span class="math inline">\(p=1\)</span>.</p>
<p>Pick <span class="math inline">\(a \not\sim b\)</span> with a minimal separator <span class="math inline">\(S\)</span>, which is complete. Let <span class="math inline">\(\widetilde{A}=\{v \in V: v \not\perp_s a \mid S\}\)</span>, and <span class="math inline">\(\widetilde{B}=V-(\widetilde{A} \cup S)\)</span>. Note that <span class="math inline">\(\widetilde{A} \perp_s \widetilde{B}\mid S\)</span>, <span class="math inline">\(V=\widetilde{A} \dot\cup \widetilde{B} \dot\cup S\)</span>. Since <span class="math inline">\(a \in \widetilde{A}\)</span> and <span class="math inline">\(b \in \widetilde{B}\)</span>, then <span class="math inline">\(\widetilde{A} \neq \varnothing\)</span> and <span class="math inline">\(\widetilde{B} \neq \varnothing\)</span>. Hence, <span class="math inline">\((\widetilde{A}, S, \widetilde{B})\)</span> is a proper decomposition. Therefore, <span class="math inline">\(\mathcal{G}_{\widetilde{A} \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{\widetilde{B} \cup S}\)</span> have strictly fewer vertices and are decomposable.</p>
<p>By induction hypothesis, <span class="math inline">\(\mathcal{C}(\mathcal{G}_{\widetilde{A} \cup S})\)</span> and <span class="math inline">\(\mathcal{C}(\mathcal{G}_{\widetilde{B} \cup S})\)</span> satisfy the running intersection property. The set <span class="math inline">\(S\)</span> is complete in both subgraphs, so there is some clique <span class="math inline">\(D\)</span> in <span class="math inline">\(\mathcal{G}_{\widetilde{A} \cup S}\)</span> that contains <span class="math inline">\(S\)</span>. In addition, each clique in <span class="math inline">\(\mathcal{G}\)</span> is a clique in one of the two subgraphs. Hence, if we order the cliques to satisfy running intersection for <span class="math inline">\(\mathcal{G}_{\widetilde{A} \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{\widetilde{B} \cup S}\)</span> respectively, and being sure to start with the clique of <span class="math inline">\(\mathcal{G}_{\widetilde{B} \cup S}\)</span> that contains <span class="math inline">\(S\)</span>, then together they will satisfy running intersection for <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p>(iv) Suppose <span class="math inline">\(C_1, \ldots, C_k\)</span> satisfy the running intersection property. If <span class="math inline">\(k=1\)</span>, then the graph is complete, and thus is decomposable.</p>
<p>Let <span class="math display">\[H_k=\bigcup_{i&lt;k} C_i \quad \text{and} \quad S_k=C_k \cap H_k=C_k \cap C_{\sigma(j)}.\]</span> Since <span class="math inline">\(C_k\)</span> and <span class="math inline">\(C_{\sigma(j)}\)</span> are complete, then <span class="math inline">\(S_k \subseteq C_k\)</span> is complete.</p>
<p>Suppose there exists an edge connecting <span class="math inline">\(a \in C_k-S_k\)</span> and <span class="math inline">\(b \in H_k-S_k\)</span>, then <span class="math inline">\(\{a, b\}\)</span> must be a subset of some cliques. But <span class="math inline">\(\{a, b\} \not\subseteq C_k\)</span> since <span class="math inline">\(b \notin C_k\)</span> (as <span class="math inline">\(b \in H_k-S_k=H_k \cap S_k^C\)</span>, then <span class="math inline">\(b \in H_k\)</span> and <span class="math inline">\(b \in S_k^C=C_k^C \cup H_k^C\)</span>; since <span class="math inline">\(b \notin H_k^C\)</span>, then <span class="math inline">\(b \in C_k^C\)</span>), and <span class="math inline">\(\{a, b\} \not\subseteq C_j\)</span> for <span class="math inline">\(j&lt;k\)</span> since <span class="math inline">\(a \notin H_k\)</span>. Therefore, the edge cannot exist and thus <span class="math inline">\(C_k-S_k \perp_s H_k-S_k \mid S_k\)</span>.</p>
<p>Note that <span class="math inline">\(H_k-S_k=H_k \cap C_k^C \neq \varnothing\)</span> since <span class="math inline">\(H_k=C_1 \cup \cdots \cup C_{k-1}\)</span> and <span class="math inline">\(C_k \neq V\)</span>.</p>
<p>If <span class="math inline">\(C_k-S_k=\varnothing\)</span>, where <span class="math inline">\(S_k \subseteq C_j\)</span> for some <span class="math inline">\(j&lt;k\)</span>, then <span class="math inline">\(C_k-C_j=C_k \cap C_j^C=\varnothing\)</span>. By assumption, <span class="math inline">\(C_i \neq \varnothing\)</span>, then <span class="math inline">\(C_j=C_k\)</span> or <span class="math inline">\(C_j=V\)</span>. However, <span class="math inline">\(C_j \neq C_k\)</span> since cliques as elements in a set cannot repeat; <span class="math inline">\(C_j \neq V\)</span> since if the graph is complete, then there is only one clique in <span class="math inline">\(\mathcal{C}(\mathcal{G}_S)\)</span>. Hence, <span class="math inline">\(C_k-S_k \neq \varnothing.\)</span></p>
<p>Therefore, <span class="math inline">\((H_k-S_k, S_k, C_k-S_k)\)</span> is a proper decomposition of <span class="math inline">\(\mathcal{G}\)</span>.</p>
Since <span class="math inline">\(\mathcal{G}_{C_k}\)</span> is decomposable (<span class="math inline">\(C_k\)</span> is complete), and <span class="math inline">\(\mathcal{G}_{H_k}\)</span> is decomposable (by induction hypothesis), then <span class="math inline">\(\mathcal{G}\)</span> is decomposable (by definition).
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm3.2"><strong>Corollary 3.2</strong></span>    If <span class="math inline">\((A, S, B)\)</span> is a decomposition, and <span class="math inline">\(\mathcal{G}\)</span> is decomposable, then <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are also decomposable.</p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(\mathcal{G}\)</span> is decomposable, then <span class="math inline">\(\mathcal{G}\)</span> is triangulated. Therefore, <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are triangulated, which implies <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> are decomposable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><a href="#thm3.2">Corollary 3.2</a> reassures us that if we want to check a graph is decomposable, we can just go ahead and start decomposing, and we will never have to back track.</p>
          </div>
<p><strong>Definition 3.5</strong>    Let <span class="math inline">\(C_1, \ldots, C_k\)</span> be the cliques of <span class="math inline">\(\mathcal{G}\)</span> ordered according to running intersection. The <span class="math inline">\(i\)</span>th <strong>separator set</strong> is <span class="math display">\[S_i=C_i \cap \bigcup_{j&lt;i} C_j,\]</span> where by convention, <span class="math inline">\(S_1=\varnothing\)</span>.</p>
<p><span id="thm3.3"><strong>Lemma 3.3</strong></span>    Let <span class="math inline">\(\mathcal{G}\)</span> be a graph with a decomposition <span class="math inline">\((A, S, B)\)</span>, and <span class="math inline">\(p\)</span> be a distribution. Then <span class="math inline">\(p\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span> iff its margins <span class="math inline">\(p(x_A, x_S)\)</span> and <span class="math inline">\(p(x_B, x_S)\)</span> factorize according to <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> and <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span>, and <span class="math display">\[p(x_V)p(x_S)=p(x_A, x_S)p(x_B, x_S).\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Leftarrow)\)</span> Suppose <span class="math inline">\(p(x_V)p(x_S)=p(x_A, x_S)p(x_B, x_S)\)</span>. Since every clique in <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span> or <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span> is a clique in <span class="math inline">\(\mathcal{G}\)</span> and <span class="math inline">\(S\)</span> is complete, then we have <span class="math display">\[\begin{aligned}
p(x_V)&amp;=p(x_A, x_S)p(x_B, x_S)\frac{1}{p(x_S)}
\\&amp;=\prod_{C \in \mathcal{C}(\mathcal{G}_{A \cup S})} \psi_C(x_C) \prod_{C&#39; \in \mathcal{C}(\mathcal{G}_{B \cup S})} \psi_{C&#39;}(x_{C&#39;})\frac{1}{p(x_S)}
\\&amp;=\prod_{C \in \mathcal{C}(\mathcal{G})} \widetilde{\psi}_C(x_C).
\end{aligned}\]</span> Hence, <span class="math inline">\(p\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>.</p>
<span class="math inline">\((\Rightarrow)\)</span> Suppose <span class="math inline">\(p \in M_f(\mathcal{G})\)</span>, then <span class="math inline">\(p \in M_g(\mathcal{G})\)</span>. Hence, <span class="math display">\[p(x_V)p(x_S)=p(x_A, x_S)p(x_B, x_S).\]</span> Besides, <span class="math display">\[\begin{aligned}
p(x_V)&amp;=\prod_{C \in \mathcal{C}(\mathcal{G})} \psi_C(x_C)
\\&amp;=\prod_{C \in \mathcal{C}_A(\mathcal{G})} \psi_C(x_C) \prod_{C&#39; \in \mathcal{C}_B(\mathcal{G})} \psi_{C&#39;}(x_{C&#39;}),
\end{aligned}\]</span> where <span class="math inline">\(C_A(\mathcal{G})\)</span> is the cliques contained in <span class="math inline">\(A \cup S\)</span>, and <span class="math inline">\(C_B(\mathcal{G})\)</span> is the rest. Then <span class="math display">\[\begin{aligned}
p(x_B, x_S)&amp;=\prod_{C&#39; \in \mathcal{C}_B(\mathcal{G})} \psi_{C&#39;}(x_{C&#39;}) \int \prod_{C \in \mathcal{C}_A(\mathcal{G})} \psi_C(x_C)\text{d}x_A
\\&amp;=\prod_{C&#39; \in \mathcal{C}(\mathcal{G}_{B \cup S})} \widetilde{\psi}_{C&#39;}(x_{C&#39;}) \cdot f(x_S)
\\&amp;=\prod_{C \in \mathcal{C}(\mathcal{G}_{B \cup S})} \widehat{\psi}_C(x_C).
\end{aligned}\]</span> Therefore, <span class="math inline">\(p(x_B, x_S)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}_{B \cup S}\)</span>. Similarly, <span class="math inline">\(p(x_A, x_S)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}_{A \cup S}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm3.4"><strong>Theorem 3.4</strong><span>    Let <span class="math inline">\(\mathcal{G}\)</span> be a decomposable graph with cliques <span class="math inline">\(C_1, \ldots, C_k\)</span>. Then <span class="math inline">\(p(x_V)\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span> iff for all <span class="math inline">\(x_V\)</span>, <span class="math display">\[p(x_V)=\prod_{i=1}^k p(x_{C_i-S_i} \mid x_{S_i})=\prod_{i=1}^k \frac{p(x_{C_i})}{p(x_{S_i})}.\]</span> Furthermore, the quantities <span class="math inline">\(p(x_{C_i-S_i} \mid x_{S_i})\)</span> are variation independent (i.e., they may jointly take any set of values that would be valid individually), so inference for <span class="math inline">\(p(x_V)\)</span> can be based on separate inferences for each <span class="math inline">\(p(x_{C_i})\)</span>.</span></span></p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Leftarrow)\)</span> Since <span class="math display">\[p(x_V)=\prod_{i=1}^k \frac{p(x_{C_i})}{p(x_{S_i})}:=\prod_{C \in \mathcal{C}(\mathcal{G})} \psi_C(x_C),\]</span> then <span class="math inline">\(p\)</span> factorizes according to <span class="math inline">\(\mathcal{G}\)</span>.</p>
<p><span class="math inline">\((\Rightarrow)\)</span> If <span class="math inline">\(k=1\)</span>, then <span class="math inline">\(C_1=V\)</span>, i.e., <span class="math inline">\(p(x_V)=p(x_{C_1})\)</span>. Let <span class="math display">\[H_k=\left(\bigcup_{i&lt;k} C_i\right)-S_k,\]</span> we have proven that <span class="math inline">\(C_k-S_k \perp_s H_k \mid S_k\)</span>, and <span class="math inline">\((H_k, S_k, C_k-S_k)\)</span> is a proper decomposition.</p>
The graph <span class="math inline">\(\mathcal{G}_{H_k \cup S_k}\)</span> has <span class="math inline">\(k-1\)</span> cliques, and by <a href="#thm3.3">lemma 3.3</a>, <span class="math inline">\(p(x_{H_k}, x_{S_k})\)</span> factorizes according to <span class="math inline">\(\mathcal{G}_{H_k \cup S_k}\)</span>. Then by induction hypothesis <span class="math display">\[p(x_{H_k}, x_{S_k})=\prod_{i=1}^{k-1} p(x_{C_i-S_i} \mid x_{S_i}).\]</span> By <a href="#thm3.3">lemma 3.3</a>, <span class="math inline">\(p(x_V)p(x_{S_k})=p(x_{H_k}, x_{S_k})p(x_{C_K})\)</span>, then <span class="math display">\[p(x_V)=\prod_{i=1}^{k-1} p(x_{C_i-S_i} \mid x_{S_i})p(x_{C_k-S_k} \mid x_{S_k}).\]</span> The variation independence follows from the fact that <span class="math inline">\(p(x_{C_k-S_k} \mid x_{S_k})\)</span> can take the form of any valid probability distribution.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="multinomial-model">4. Multinomial Model</h1>
<div class="note info">
            <p><a href="#thm3.4">Theorem 3.4</a> is useful for statistical inference, since we only need to consider the margins of variables corresponding to cliques. Suppose we have a contingency table with counts <span class="math inline">\(n(x_V)\)</span>, the log likelihood for a decomposable graph is <span class="math display">\[\begin{aligned}l(p; n)&amp;=\sum_{x_V \in \mathcal{X}_V} n(x_V) \ln p(x_V)\\&amp;=\sum_{x_V \in \mathcal{X}_V} n(x_V)\sum_{i=1}^k \ln p(x_{C_i-S_i} \mid x_{S_i})\\&amp;=\sum_{i=1}^k \sum_{x_{C_i} \in \mathcal{X}_{C_i}} n(x_{C_i})\ln p(x_{C_i-S_i} \mid x_{S_i}).\end{aligned}\]</span> Using Lagrange multiplier, we can see that the likelihood is maximized by choosing <span class="math display">\[\widehat{p}(x_{C_i-S_i} \mid x_{S_i})=\frac{n(x_{C_i})}{n(x_{S_i})} \Rightarrow \widehat{p}(x_{C_i})=\frac{n(x_{C_i})}{n}.\]</span></p>
          </div>
<p><strong>Theorem 4.1</strong>    Let <span class="math inline">\(\mathcal{G}\)</span> be an undirected graph with counts <span class="math inline">\(n(x_V)\)</span>. Then the MLE under <span class="math inline">\(\mathcal{G}\)</span> is the unique distribution for which <span class="math display">\[n\widehat{p}(x_C)=n(x_C)\]</span> for all cliques <span class="math inline">\(C \in \mathcal{C}(\mathcal{G})\)</span>.</p>
<h2 id="iterative-proportional-fitting-ipf-algorithm">4.1. Iterative Proportional Fitting (IPF) Algorithm</h2>
<p>The <strong>iterative proportional fitting (IPF) algorithm</strong>, a.k.a. the <strong>iterative proportional scaling (IPS) algorithm</strong>, starts with a discrete distribution that satisfies the Markov property for the graph <span class="math inline">\(\mathcal{G}\)</span> (usually we pick the uniform distribution so that everything is independent), and then iteratively fixes each margin <span class="math inline">\(p(x_C)\)</span> to match the required distribution (or match moment) using the update step.</p>
<table frame="hsides" style="line-height:20px;">
<tbody>
<tr>
<td>
<b>Algorithm 1</b> Iterative Proportional Fitting (IPF) Algorithm.
</td>
</tr>
<tr>
<td>
    <b>function</b> IPF(collection of consistent margins <span class="math inline">\(q(x_{C_i})\)</span> for sets <span class="math inline">\(C_1, \ldots, C_k\)</span>)<br>         set <span class="math inline">\(p(x_V)\)</span> to uniform distribution;<br>         <b>while</b> <span class="math inline">\(\max_i \max_{x_{C_i}} |p(x_{C_i})-q(x_{C_i})|&gt;\text{tol}\)</span> <b>do</b><br>             <b>for</b> <span class="math inline">\(i\)</span> in <span class="math inline">\(1, \ldots, k\)</span> <b>do</b><br>                 update <span class="math inline">\(p(x_V)\)</span> to <span class="math inline">\(p(x_{V-C_i} \mid x_{C_i})q(x_{C_i})\)</span>;<br>             <b>end for</b><br>         <b>end while</b><br>         <b>return</b> distribution <span class="math inline">\(p\)</span> with margins <span class="math inline">\(p(x_{C_i}) \approx q(x_{C_i})\)</span><br>     <b>end function</b>
</td>
</tr>
</tbody>
</table>
<p><strong>Theorem 4.2</strong>    The IPF algorithm does not decrease the likelihood at any iteration, and converges to a solution.</p>
<p><strong><em>Proof.</em></strong> Pick a margin <span class="math inline">\(A\)</span>, and let <span class="math inline">\(B=V-A\)</span>. The algorithm replaces <span class="math display">\[p^{(t)}(x_A, x_B)=p^{(t)}(x_A)p^{(t)}(x_B \mid x_A) \mapsto q(x_A)p^{(t)}(x_B \mid x_A).\]</span> The log likelihood is <span class="math display">\[l(p; n)=\sum_{x_V} n(x_V)\ln p^{(t)}(x_A, x_B)=\sum_{x_V} n(x_V)\ln p^{(t)}(x_A)+\sum_{x_V} n(x_V)\ln p^{(t)}(x_B \mid x_A)\]</span> where the first term is maximized by the IPF algorithm, while the second term is unchanged.</p>
Since log likelihood function is concave, then it converges to maximum.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The algorithm is closely related to the <strong>message passing algorithm</strong>.</p>
          </div>
]]></content>
      <categories>
        <category>Statistics</category>
        <category>Graphical model</category>
      </categories>
      <tags>
        <tag>Undirected graphical model</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to PyTorch</title>
    <url>/Computer-science/PyTorch/Introduction_to_PyTorch.html</url>
    <content><![CDATA[<h1 id="tensor">1. Tensor</h1>
<ul>
<li><p>创建一个随机初始化的张量（<span class="math inline">\(5 \times 3\)</span>矩阵）：<code>torch.rand(5, 3)</code>.</p></li>
<li><p>创建一个全为<span class="math inline">\(0\)</span>的张量（<span class="math inline">\(5 \times 3\)</span>矩阵）：<code>torch.zeros(5, 3, dtype=torch.long)</code>, 其中数据类型是<code>long</code>.</p></li>
<li><p>创建一个张量：<code>torch.tensor([1.3, 1.4])</code>.</p></li>
<li><p>基于已经存在的张量创建一个张量：</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.double)</span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>获取张量的维度信息：<code>x.size()</code>.</li>
</ul>
<h1 id="operation">2. Operation</h1>
<ul>
<li>加法的几种方式：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法一</span></span><br><span class="line">x + y</span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法二</span></span><br><span class="line">torch.add(x, y)</span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result) <span class="comment"># 将结果储存于result中</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 方法三</span></span><br><span class="line">y.add_(x)</span><br></pre></td></tr></table></figure>
<div class="note warning">
            <p><code>_</code>会使张量发生变化，例如<code>x.copy_(y)</code>或<code>x.t_()</code>都会改变<code>x</code>.</p>
          </div>
<ul>
<li>使用<code>torch.view</code>改变张量的大小或形状：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">6</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">24</span>)</span><br><span class="line">z = x.view(-<span class="number">1</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <p><code>view(-1, n)</code>中<code>-1</code>的大小是由其它位置推断出来的，比如上例中<code>z</code>是一个<span class="math inline">\(3 \times 8\)</span>的矩阵。</p>
          </div>
<ul>
<li>如果张量仅为一个元素，可以使用<code>.item()</code>来获得这个元素的值：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">x.item()</span><br></pre></td></tr></table></figure>
<h1 id="automatic-differentiation">3. Automatic Differentiation</h1>
<ul>
<li>如果将<code>torch.Tensor</code>的属性<code>.requires_grad</code>设置为<code>True</code>, 则会开始跟踪针对张量的所有操作。在完成计算后，调用<code>.backward()</code>来自动计算所有梯度。张量的梯度将累积到<code>.grad</code>属性中。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"></span><br><span class="line">out.backward()</span><br><span class="line">x.grad <span class="comment"># d(out)/dx</span></span><br></pre></td></tr></table></figure>
<div class="note default">
            <p>Let <code>out</code> be <span class="math inline">\(o\)</span>, then <span class="math inline">\(\displaystyle o=\frac{1}{4}\sum_i z_i\)</span>, where <span class="math inline">\(z_i=3(x_i+2)^2\)</span>. Hence, <span class="math inline">\(\displaystyle \frac{\partial o}{\partial x_i}=\frac{3}{2}(x_i+2)\)</span>, and thus <span class="math inline">\(\displaystyle \frac{\partial o}{\partial x_i}\bigg|_{x_i-1}=4.5\)</span>.</p>
          </div>
<div class="note info">
            <p>当调用<code>.backward()</code>时，一个张量会自动传递为<code>.backward(torch.tensor(1.0))</code>, 其中<code>torch.tensor(1.0)</code>是用来终止链式法则梯度乘法的外部梯度。</p>
          </div>
<ul>
<li><code>.backward()</code>中的张量的维数必须与正在计算梯度的张量的维数相同：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">8.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5.</span>, <span class="number">9.</span>, <span class="number">7.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x * y</span><br><span class="line"></span><br><span class="line">z.backward() <span class="comment"># RuntimeError: grad can be implicitly created only for scalar outputs</span></span><br><span class="line">z.backward(torch.tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]))</span><br><span class="line">x.grad <span class="comment"># tensor([5., 9., 7.])</span></span><br><span class="line">y.grad <span class="comment"># tensor([0., 2., 8.])</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>Tensor</code>和<code>Function</code>互相连接并构建一个保存整个完整计算过程的历史信息的非循环图。每一个张量都一个<code>.grad_fn</code>属性，这一属性保存着创建张量的<code>Function</code>的引用。如果是用户自己创建张量，则<code>.grad_fn</code>为<code>None</code>:</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x.grad_fn) <span class="comment"># None</span></span><br><span class="line"></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y.grad_fn) <span class="comment"># &lt;AddBackward0 object at 0x7fe7900ccd00&gt;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>如果要停止张量历史记录的跟踪，调用<code>.detach()</code>来将其与计算历史记录分离，并防止将来的计算被跟踪。此外，还可以使用<code>with torch.no_grad():</code>包装起来——在评估模型时尤其有用，因为模型在训练阶段具有<code>requires_grad=True</code>的可训练参数有利于有利于调参，但在评估阶段我们不需要梯度。</p></li>
<li><p><code>.requires_grad_()</code>会改变张量的<code>requires_grad</code>标记。如果没有提供相应的参数，输入的标记默认为<code>False</code>.</p></li>
</ul>
<h1 id="neural-network">4. Neural Network</h1>
<p>一个典型的神经网络训练过程包括：</p>
<ol type="1">
<li><p>定义一个包含可训练参数的神经网络。</p></li>
<li><p>迭代整个输入。</p></li>
<li><p>通过神经网络处理输入。</p></li>
<li><p>计算损失。</p></li>
<li><p>反向传播梯度到神经网络的参数。</p></li>
<li><p>更新网络的参数。</p></li>
</ol>
<h2 id="example-lenet-5">4.1. Example: LeNet-5</h2>
<p>考虑一个简单的前馈神经网络。</p>
<span id="f1"><img data-src="/images/Introduction_to_PyTorch_1.png"></span>
<center>
Figure 1: Architecture of LeNet-5 <a href="#LC+98">(LeCun et al., 1998)</a>.
</center>
LeNet-5神经网络一共五层，其中卷基层和池化层可以考虑为一个整体。网络的结构为：
<center>
输入 <span class="math inline">\(\to\)</span> 卷积 <span class="math inline">\(\to\)</span> 池化 <span class="math inline">\(\to\)</span> 卷积 <span class="math inline">\(\to\)</span> 池化 <span class="math inline">\(\to\)</span> 全连接 <span class="math inline">\(\to\)</span> 全连接 <span class="math inline">\(\to\)</span> 全连接 <span class="math inline">\(\to\)</span> 输出
</center>
<p>在<code>PyTorch</code>中，图像数据集的存储顺序为<code>(batch, channels, height, width)</code>，依次为批大小、通道数、高度、宽度。按照网络结构，我们可以整理得：</p>
<center>
Table 1: Parameters of LeNet-5.
</center>
<table style="table-layout:fixed">
<tbody>
<tr>
<th>
<center>
操作
</center>
</th>
<th>
<center>
操作参数
</center>
</th>
<th>
<center>
输入/输出尺寸
</center>
</th>
</tr>
<tr>
<td>
<center>
<code>input</code>
</center>
</td>
<td>
<center>
<code>channels=1</code><br> <code>height=32</code><br> <code>width=32</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 1, 32, 32)</code><br> Output: <code>(b, 1, 32, 32)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>conv1</code>
</center>
</td>
<td>
<center>
<code>in_channels=1</code><br> <code>out_channels=6</code><br> <code>kernel_size=5</code><br> <code>padding=0</code><br> <code>stride=1</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 1, 32, 32)</code><br> Output: <code>(b, 6, 28, 28)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>pool1</code>
</center>
</td>
<td>
<center>
<code>kernel_size=2</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 6, 28, 28)</code><br> Output: <code>(b, 6, 14, 14)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>conv2</code>
</center>
</td>
<td>
<center>
<code>in_channels=6</code><br> <code>out_channels=16</code><br> <code>kernel_size=5</code><br> <code>padding=0</code><br> <code>stride=1</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 6, 14, 14)</code><br> Output: <code>(b, 16, 10, 10)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>pool2</code>
</center>
</td>
<td>
<center>
<code>kernel_size=2</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 16, 10, 10)</code><br> Output: <code>(b, 16, 5, 5)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>fc1</code>
</center>
</td>
<td>
<center>
<code>in=16*5*5</code><br> <code>out=120</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 16*5*5)</code><br> Output: <code>(b, 120)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>fc2</code>
</center>
</td>
<td>
<center>
<code>in=120</code><br> <code>out=84</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 120)</code><br> Output: <code>(b, 84)</code>
</center>
</td>
</tr>
<tr>
<td>
<center>
<code>fc3</code>
</center>
</td>
<td>
<center>
<code>in=84</code><br> <code>out=10</code>
</center>
</td>
<td>
<center>
Input: <code>(b, 84)</code><br> Output: <code>(b, 10)</code>
</center>
</td>
</tr>
</tbody>
</table>
<h3 id="torch.nn.conv2d">4.1.1. torch.nn.Conv2d</h3>
<p>卷积层用于提取特征。<code>torch.nn.Conv2d</code>是二维卷积方法（常用于二维图像），相对应的还有一维卷积方法<code>torch.nn.Conv1d</code>（常用于文本数据的处理）。<code>torch.nn.Conv2d</code>有9个参数，我们先介绍6个较为重要的参数：</p>
<ol type="1">
<li><p><code>in_channels</code>及<code>out_channels</code>: <code>channels</code>意为通道数。在输入层中的RGB图片，<code>channels=3</code>（红、绿、蓝）；而单色图，<code>channels=1</code>. 一般而言，<code>channels</code>指每个卷基层中卷积核的数量。</p></li>
<li><p><code>kernel_size</code>: 卷积核的大小，可以用<code>int</code>表示长宽相等的卷积核，或者用<code>tuple</code>.表示长宽不同的卷积核。</p></li>
<li><p><code>stride</code>: 步长，用来控制卷积核移动间隔，默认<code>stride=1</code>.</p></li>
<li><p><code>padding</code>: 填充宽度。例如当<code>padding=1</code>的时候，如果原来的大小为<span class="math inline">\(3 \times 3\)</span>, 那么填充后的大小为<span class="math inline">\(5 \times 5\)</span>, 即在外围加了一圈, 默认<code>padding=0</code>.</p></li>
<li><p><code>padding_mode</code>: 填充宽度的方式，默认<code>padding_mode="zeros"</code>.</p></li>
</ol>
<h3 id="torch.nn.maxpool2d">4.1.2. torch.nn.MaxPool2d</h3>
<p>池化层用于提取重要信息，可以去掉不重要的信息，减少计算开销。<code>torch.nn.MaxPool2d</code>在提取数据时，保留相邻信息中的最大值，并去掉其它值。我们先介绍<code>torch.nn.MaxPool2d</code>中2个较为重要的参数：</p>
<ol type="1">
<li><p><code>kernel_size</code>: 池化核的大小，与卷积核类似。</p></li>
<li><p><code>stride</code>: 步长，默认<code>stride=</code>kernel_size`.</p></li>
</ol>
<h3 id="torch.nn.linear">4.1.3. torch.nn.Linear</h3>
<p><code>torch.nn.Linear</code>用于设置网络中的全连接层。在二维图像处理的任务中，全连接层的输入与输出一般都设置为二维张量<code>(batch, size)</code>. <code>torch.nn.Linear</code>有三个参数，分别为<code>in_features</code>, <code>out_features</code>和<code>bias</code>.</p>
<h2 id="code">4.2. Code</h2>
<ul>
<li>神经网络可以通过<code>torch.nn</code>来构建。首先我们定义图片中的神经网络：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet5</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LeNet5, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">1</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">5</span>)</span><br><span class="line">        self.pool1 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool2 = nn.MaxPool2d(<span class="number">2</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.pool1(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool2(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(x.size()[<span class="number">0</span>], -<span class="number">1</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<ul>
<li><p>一个模型可训练的参数可以通过调用<code>LeNet5.parameters()</code>返回。</p></li>
<li><p>一个简单的损失函数是均方误差<code>nn.MSELoss(output, target)</code>.</p></li>
<li><p>调用<code>LeNet5.zero_grad()</code>将梯度清零，并随机的梯度来反向传播<code>loss.vackward()</code>.</p></li>
<li><p>利用梯度更新神经网络，最简单的更新规则是随机梯度下降：</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> LeNet5.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>此外，我们可以使用不同的更新规则，比如<code>Adam</code>等：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(LeNet5.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练过程循环：</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">output = LeNet5(<span class="built_in">input</span>)</span><br><span class="line">loss = nn.MSELoss(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
<h1 id="reference">5. Reference</h1>
<div id="LC+98" style="line-height: 18px; font-size: 15px;">
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-Based Learning Applied to Document Recognition. <em>Proceedings of the IEEE</em>, 86(11): 2278-2324, 1998.
</div>
]]></content>
      <categories>
        <category>Computer science</category>
        <category>PyTorch</category>
      </categories>
      <tags>
        <tag>Automatic differentiation</tag>
        <tag>Neural network</tag>
        <tag>PyTorch</tag>
        <tag>Tensor</tag>
      </tags>
  </entry>
  <entry>
    <title>Computation With Large Matrix</title>
    <url>/Mathematics/Matrix-method/Computation_with_Large_Matrix.html</url>
    <content><![CDATA[<h1 id="least-squares">1. Least Squares</h1>
<ul>
<li>Four ways to solve least squares:</li>
</ul>
<p>         1. The SVD of <span class="math inline">\(A\)</span> leads to its <strong>pseudo-inverse</strong> <span class="math inline">\(A^+\)</span>, then <span class="math inline">\(\mathbf{x}^+=A^+\mathbf{b}\)</span>.</p>
<p>         2. When <span class="math inline">\(A\)</span> has independent columns, solving <span class="math inline">\(A^TA\widehat{\mathbf{x}}=A^T\mathbf{b}\)</span> to minimize <span class="math inline">\(\|A\mathbf{x}-\mathbf{b}\|^2\)</span>.</p>
<p>         3. When <span class="math inline">\(A\)</span> has independent columns, Gram-Schmidt <span class="math inline">\(A=QR\)</span> leads to <span class="math inline">\(\widehat{\mathbf{x}}=R^{-1}Q^T\mathbf{b}\)</span>.</p>
<p>         4. Minimize <span class="math inline">\(\|A\mathbf{x}-\mathbf{B}\|^2+\delta^2\|\mathbf{x}\|^2\)</span>, and the best <span class="math inline">\(\mathbf{x}\)</span> is the limit of <span class="math inline">\((A^TA+\delta I)^{-1}A^T\mathbf{b}\)</span> as <span class="math inline">\(\delta \to 0\)</span>.</p>
<h2 id="pseudo-inverse-method">1.1. Pseudo-Inverse Method</h2>
<ul>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(A^+=A^{-1}\)</span>.</p></li>
<li><p>The pseudo-inverse of <span class="math inline">\(A=U\Sigma V^T\)</span> is <span class="math inline">\(A^+=V\Sigma^+U^T\)</span>.</p></li>
<li><p>Sometimes we denote <span class="math inline">\(A^+\)</span> to be <span class="math inline">\(A^\dagger\)</span>.</p></li>
</ul>
<p><strong>Example 1.1</strong>    Consider <span class="math inline">\(\Sigma=\begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>, then <span class="math inline">\(\Sigma^+=\begin{bmatrix} 1/\sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; 1/\sigma_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbf{x}^+=A^+\mathbf{b}\)</span> is the <strong>minimum norm least squares</strong> solution.</li>
</ul>
<p><strong>Example 1.2</strong>    Consider <span class="math inline">\(\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}=\begin{bmatrix} 6 \\ 8 \end{bmatrix}\)</span>, then <span class="math inline">\(x^+=A^+\mathbf{b}=\begin{bmatrix} 1/3 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\begin{bmatrix} 6 \\ 8 \end{bmatrix}=\begin{bmatrix} 2 \\ 0 \end{bmatrix}\)</span>.</p>
<h2 id="normal-equation-method">1.2. Normal Equation Method</h2>
<ul>
<li><p>Suppose <span class="math inline">\(A\)</span> has independent columns, then <span class="math inline">\(\widehat{\mathbf{x}}=(A^TA)^{-1}A^T\mathbf{b}\)</span>.</p></li>
<li><p><strong>Projection</strong> of <span class="math inline">\(\mathbf{b}\)</span> onto the column space of <span class="math inline">\(A\)</span> is <span class="math inline">\(\mathbf{p}=A\widehat{\mathbf{x}}=A(A^TA)^{-1}A^T\mathbf{b}\)</span>.</p></li>
</ul>
<h2 id="gram-schmidt-method">1.3. Gram-Schmidt Method</h2>
<div class="note info">
            <p>Suppose <span class="math inline">\(A\)</span> has independent columns <span class="math inline">\(\mathbf{a}_1, \ldots, \mathbf{a}_n\)</span>.</p><p>Let <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span>, <span class="math inline">\(\displaystyle A_k=\mathbf{a}_k-\sum_{i=1}^{k-1} (\mathbf{a}_k^T\mathbf{q}_i)\mathbf{q}_i\)</span>, and <span class="math inline">\(\displaystyle \mathbf{q}_k=\frac{A_k}{\|A_k\|}\)</span>. Then each <span class="math inline">\(\mathbf{a}_k\)</span> is a combination of <span class="math inline">\(\mathbf{q}_1\)</span> to <span class="math inline">\(\mathbf{q}_k\)</span>: <span class="math inline">\(\mathbf{a}_1=\|\mathbf{a}_1\|\mathbf{q}_1\)</span> and <span class="math inline">\(\displaystyle \mathbf{a}_k=\sum_{i=1}^{k-1} (\mathbf{a}_k^T\mathbf{q}_i)\mathbf{q}_i+\|A_k\|\mathbf{q}_k\)</span>.</p><p>Hence, the matrix <span class="math inline">\(R=Q^TA\)</span> with <span class="math inline">\(r_{ij}=\mathbf{q}_i^T\mathbf{a}_j\)</span> is upper triangular, i.e., <span class="math display">\[\begin{bmatrix}&amp; \\ \mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_n \\ &amp; \end{bmatrix}=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \cdots &amp; \mathbf{q}_n \\ &amp;\end{bmatrix}\begin{bmatrix}r_{11} &amp; \cdots &amp; r_{1n} \\ 0 &amp; \ddots &amp; \vdots \\0 &amp; 0 &amp; r_{nn}\end{bmatrix}\]</span> is <span class="math inline">\(A=QR\)</span>.</p><p>The MATLAB command is <code>[Q, R]=qr(A)</code>.</p>
          </div>
<ul>
<li><p>Suppose <span class="math inline">\(A\)</span> has independent columns, then by Gram-Schmidt, we have <span class="math inline">\(A^TA=R^TQ^TQR=R^TR\)</span>, and thus <span class="math inline">\(\widehat{\mathbf{x}}=(R^TR)^{-1}R^TQ^T\mathbf{b}=R^{-1}Q^T\mathbf{b}\)</span>.</p></li>
<li><p>We can modify Gram-Schmidt with <strong>column pivoting</strong>.</p></li>
</ul>
<h2 id="penalty-method">1.4. Penalty Method</h2>
<p>Penalty method regularizes a singular problem.</p>
<ul>
<li><p>The new normal equation is <span class="math inline">\((A^*)^TA^*\widehat{\mathbf{x}}=(A^*)^T\mathbf{b}^*\)</span>, where <span class="math inline">\(A^*=\begin{bmatrix} A \\ \delta I \end{bmatrix}\)</span> and <span class="math inline">\(b^*=\begin{bmatrix} \mathbf{b} \\ \mathbf{0} \end{bmatrix}\)</span>, i.e., <span class="math display">\[(A^TA+\delta^2I)\widehat{\mathbf{x}}=A^T\mathbf{b}.\]</span></p></li>
<li><p>For any <span class="math inline">\(A\)</span>, <span class="math inline">\((A^TA+\delta^2I)^{-1}A^T \to A^+\)</span> as <span class="math inline">\(\delta \to 0\)</span>.</p></li>
</ul>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(A=U\Sigma V^T\)</span>, then we have <span class="math display">\[A^TA+\delta^2I=V\Sigma^TU^TU\Sigma V^T+\delta^2I=V\Sigma^T\Sigma V^T+\sigma^2I=V(\Sigma^T\Sigma+\delta^2I)V^T\]</span> Therefore, <span class="math display">\[(A^TA+\delta^2I)^{-1}A^T=V(\Sigma^T\Sigma+\delta^2I)^{-1}V^TV\Sigma^TU^T=V[(\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T]U^T.\]</span></p>
Since <span class="math inline">\((\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T\)</span> has positive diagonal entries <span class="math inline">\(\displaystyle \frac{\sigma_i}{\sigma_i^2+\delta^2}\)</span> and otherwise all zeros, then as <span class="math inline">\(\delta \to 0\)</span>, <span class="math inline">\((\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T \to \Sigma^+\)</span>. Hence, as <span class="math inline">\(\delta \to 0\)</span>, <span class="math display">\[V[(\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T]U^T \to V\Sigma^+U^T=A^+.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="numerical-linear-algebra">2. Numerical Linear Algebra</h1>
<h2 id="krylov-subspace-and-arnoldi-iteration">2.1. Krylov Subspace and Arnoldi Iteration</h2>
<ul>
<li><p>Matrix-vector multiplication is fast, especially if <span class="math inline">\(A\)</span> is sparse. If we start with <span class="math inline">\(A\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, we can quickly compute each of the vectors <span class="math inline">\(\mathbf{b}, A\mathbf{b}, A^2\mathbf{b}=A(A\mathbf{b}), \ldots, A^{j-1}\mathbf{b}\)</span>. The combinations of vectors above give <strong>Krylov subspace</strong> <span class="math inline">\(\mathcal{K}_j\)</span>. We want to find a closest approximation <span class="math inline">\(\mathbf{x}_j \in K_j\)</span> to the desired solution <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
<li><p><strong>(Arnoldi Iteration)</strong> An orthogonal basis is better than the original basis <span class="math inline">\(\mathbf{b}, A\mathbf{b}, \ldots, A^{j-1}\mathbf{b}\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{b}}{\|\mathbf{b}\|}\)</span>, and <span class="math inline">\(\mathbf{q}_2, \ldots, \mathbf{q}_k\)</span> are known;</li>
<li>Let <span class="math inline">\(\mathbf{v}=A\mathbf{q}_k\)</span>;</li>
<li>For <span class="math inline">\(j=1\)</span> to <span class="math inline">\(k\)</span>:
<ul>
<li><span class="math inline">\(h_{jk}=\mathbf{q}_j^T\mathbf{v}\)</span>;</li>
<li><span class="math inline">\(\mathbf{v}=\mathbf{v}-h_{jk}\mathbf{q}_j\)</span>;</li>
</ul></li>
<li>Let <span class="math inline">\(h_{k+1, k}=\|\mathbf{v}\|\)</span>;</li>
<li>Let <span class="math inline">\(\displaystyle \mathbf{q}_{k+1}=\frac{\mathbf{v}}{h_{k+1, k}}\)</span>.</li>
</ul></li>
</ul>
<div class="note info">
            <p>The Arnoldi iteration follows the Gram-Schmidt idea.</p><p>Assume we use the standard Gram-Schmidt to attain an orthogonal basis for <span class="math inline">\(\mathcal{K}_{j+1}\)</span>, then <span class="math display">\[\mathbf{y}_j=A^j\mathbf{b}-\sum_{i=1}^j (\mathbf{q}_i^TA^j\mathbf{b})\mathbf{q}_i\]</span> and <span class="math display">\[\mathbf{q}_{j+1}=\frac{\mathbf{y}_j}{\|\mathbf{y}_j\|}.\]</span> Hence, <span class="math inline">\(\{\mathbf{q}_1, \ldots, \mathbf{q}_{j+1}\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{K}_{j+1}\)</span>.</p><p>Since <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{b}}{\|\mathbf{b}\|}\)</span> and <span class="math inline">\(A\mathbf{q}_1=c_1\mathbf{q}_1+c_2\mathbf{q}_2\)</span>, then <span class="math display">\[\begin{aligned}\mathcal{K}_{j+1}&amp;=\text{span}\{\mathbf{b}, A\mathbf{b}, A^2\mathbf{b}, \ldots, A^j\mathbf{b}\}\\&amp;=\text{span}\{\mathbf{q}_1, A\mathbf{q}_1, A^2\mathbf{q}_1, \ldots, A^j\mathbf{q}_1\}\\&amp;=\text{span}\{\mathbf{q}_1, c_1\mathbf{q}_1+c_2\mathbf{q}_2, A(c_1\mathbf{q}_1+c_2\mathbf{q}_2), \ldots, A^{j-1}(c_1\mathbf{q}_1+c_2\mathbf{q}_2)\}\\&amp;=\text{span}\{\mathbf{q}_1, \mathbf{q}_2, A\mathbf{q}_2, \ldots, A^{j-1}\mathbf{q}_2\}\\&amp;\ \ \vdots\\&amp;=\text{span}\{\mathbf{q}_1, \mathbf{q}_2, \ldots, \mathbf{q}_j, A\mathbf{q}_j\}.\end{aligned}\]</span></p><p>Apply Gram-Schmidt again, we have <span class="math display">\[\mathbf{v}_j=A\mathbf{q}_j-\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\]</span> and replace <span class="math display">\[\mathbf{q}_{j+1}=\frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}.\]</span></p><p>Since <span class="math display">\[\mathbf{q}_{j+1}^T\mathbf{v}_j=\mathbf{q}_{j+1}^TA\mathbf{q}_j-\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_{j+1}^T\mathbf{q}_i=\mathbf{q}_{j+1}^TA\mathbf{q}_j\]</span> and <span class="math display">\[\mathbf{q}_{j+1}^T\mathbf{v}_j=\frac{\mathbf{v}_j^T\mathbf{v}_j}{\|\mathbf{v}_j\|}=\|\mathbf{v}_j\|,\]</span> then <span class="math inline">\(\|\mathbf{v}_j\|=\mathbf{q}_{j+1}^TA\mathbf{q}_j\)</span>.</p><p>Therefore, <span class="math display">\[\begin{aligned}A\mathbf{q}_j&amp;=\mathbf{v}_j+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\|\mathbf{v}_j\|\mathbf{q}_{j+1}+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\mathbf{q}_{j+1}^TA\mathbf{q}_j\mathbf{q}_{j+1}+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\sum_{i=1}^{j+1} (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\sum_{i=1}^{j+1} h_{ij}\mathbf{q}_i\\&amp;=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \cdots &amp; \mathbf{q}_j \\ &amp;\end{bmatrix}\begin{bmatrix}h_{11} \\ \vdots \\ h_{jj}\end{bmatrix}+\mathbf{q}_{j+1}h_{j+1, j}.\end{aligned}\]</span></p><p>Hence, <span class="math display">\[\begin{aligned}AQ_j&amp;=\begin{bmatrix}&amp; \\ A\mathbf{q}_1 &amp; A\mathbf{q}_2 &amp; \cdots &amp; A\mathbf{q}_{j-1} &amp; A\mathbf{q}_j \\ &amp;\end{bmatrix}\\&amp;=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \mathbf{q}_2 &amp; \cdots &amp; \mathbf{q}_j &amp; \mathbf{q}_{j+1} \\ &amp;\end{bmatrix}\begin{bmatrix}h_{11} &amp; h_{12} &amp; h_{13} &amp; \cdots &amp; h_{1j} \\h_{21} &amp; h_{22} &amp; h_{23} &amp; \cdots &amp; h_{2j} \\ 0 &amp; h_{32} &amp; h_{33} &amp; \cdots &amp; h_{3j} \\ \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; h_{j, j-1} &amp; h_{jj} \\ 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; h_{j+1, j}\end{bmatrix}\\&amp;=Q_{j+1}H_{j+1, j}\end{aligned}.\]</span></p><p>Besides, we have <span class="math display">\[Q^T_jAQ_j=Q_j^TQ_{j+1}H_{j+1, j}=\begin{bmatrix}&amp; \\ I_{j \times j} &amp; \mathbf{0}_{j \times 1} \\ &amp;\end{bmatrix}\begin{bmatrix}H_j \\ (k+1)^\text{th}\ \text{row} \end{bmatrix}=H_j,\]</span> where <span class="math inline">\(H_j\)</span> is the <strong>Hessenberg matrix</strong> (an upper triangular matrix with one nonzero sub-diagonal). In addition, <span class="math inline">\(H_j=Q_j^TAQ_j\)</span> is the projection of <span class="math inline">\(A\)</span> onto the Krylov space, using the basis of <span class="math inline">\(\mathbf{q}\)</span>'s.</p>
          </div>
<h2 id="computation-of-eigenvalue-and-singular-value">2.2. Computation of Eigenvalue and Singular Value</h2>
<h3 id="qr-algorithm-for-eigenvalue">2.2.1. QR Algorithm for Eigenvalue</h3>
<ul>
<li>QR algorithm with shifts:</li>
</ul>
<p>         Step 1: Reduce <span class="math inline">\(A\)</span> to a similar Hessenberg form <span class="math inline">\(A_0\)</span>;</p>
<p>         Step 2: QR with shift:</p>
<p>             (1) Choose a shift <span class="math inline">\(s_k\)</span> at step <span class="math inline">\(k\)</span>, where <span class="math inline">\(k=0, \ldots\)</span>;</p>
<p>             (2) Factor <span class="math inline">\(A_k-s_kI=Q_kR_k\)</span>;</p>
<p>             (3) Reverse factors and shift back: <span class="math inline">\(A_{k+1}=R_kQ_k+s_kI\)</span>.</p>
<ul>
<li><p>Well-chosen shifts <span class="math inline">\(s_k\)</span> will greatly speed up the approach of the <span class="math inline">\(A\)</span>'s to a diagonal matrix <span class="math inline">\(\Lambda\)</span>.</p></li>
<li><p>If the matrix <span class="math inline">\(S\)</span> is symmetric, then we can reduce <span class="math inline">\(S\)</span> to a symmetric Hessenberg matrix, i.e., <strong>tridiagonal matrix</strong>.</p></li>
</ul>
<h3 id="golub-kahan-algorithm-for-singular-value">2.2.2. Golub-Kahan Algorithm for Singular Value</h3>
<ul>
<li><p>Eigenvalues are the same for <span class="math inline">\(S\)</span> and <span class="math inline">\(Q^{-1}SQ=Q^TSQ\)</span>, so we have <em>limited freedom</em> to create zeros in <span class="math inline">\(Q^{-1}SQ\)</span>. The good <span class="math inline">\(Q^{-1}SQ\)</span> will be tridiagonal matrix.</p></li>
<li><p>Singular values are the same for <span class="math inline">\(A\)</span> and <span class="math inline">\(Q_1AQ_2^T\)</span> even if <span class="math inline">\(Q_1\)</span> is different from <span class="math inline">\(Q_2\)</span>. We have <em>more freedom</em> to create zeros in <span class="math inline">\(Q_1AQ_2^T\)</span>. With the right <span class="math inline">\(Q\)</span>'s, the good <span class="math inline">\(Q_1AQ_2^T\)</span> will be <strong>bidiagonal matrix</strong>.</p></li>
<li><p>Golub-Kahan algorithm:</p></li>
</ul>
<p>         Step 1: Find <span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_2\)</span> so that <span class="math inline">\(Q_1AQ_2^T\)</span> is bidiagonal;</p>
<p>         Step 2: Apply the shifted QR algorithm to find the eigenvalues of <span class="math display">\[(Q_1AQ_2^T)^T(Q_1AQ_2^T)=Q_2A^TAQ_2^T,\]</span> which is tridiagonal.</p>
<h1 id="random-matrix-multiplication">3. Random Matrix Multiplication</h1>
<ul>
<li><p>Let <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{C}\)</span>, where <span class="math inline">\(\mathbf{a}_j\)</span> is the column <span class="math inline">\(j\)</span> of <span class="math inline">\(A\)</span>, <span class="math inline">\(\mathbf{b}_j^T\)</span> is the row <span class="math inline">\(j\)</span> of <span class="math inline">\(B\)</span>, and <span class="math inline">\(\displaystyle C=\sum_{j=1}^n \|\mathbf{a}_j\|\|\mathbf{b}_j^T\|\)</span>.</p></li>
<li><p>Take <span class="math inline">\(\mathbf{a}_j\)</span> and <span class="math inline">\(\mathbf{b}_j^T\)</span> w.p. <span class="math inline">\(p_j\)</span>, and each of the <span class="math inline">\(s\)</span> trials produces a matrix <span class="math inline">\(\displaystyle X_j=\frac{\mathbf{a}_j\mathbf{b}_j^T}{sp_j}\)</span>. The mean of each trial is <span class="math display">\[\mathbb{E}[X]=\sum_{j=1}^n p_jX_j=\frac{1}{s}\sum_{j=1}^n \mathbf{a}_j\mathbf{b}_j^T=\frac{1}{s}AB.\]</span> The approximation of <span class="math inline">\(AB\)</span> is the sum of <span class="math inline">\(s\)</span> trials, and we have <span class="math inline">\(s\mathbb{E}[X]=AB\)</span>.</p></li>
<li><p>The variance of each trial is <span class="math display">\[\begin{aligned}
\text{Var}[X]&amp;=\mathbb{E}[X^2]-(\mathbb{E}[X])^2
\\&amp;=\sum_{j=1}^n p_j\frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{s^2p_j^2}-\frac{1}{s^2}\|AB\|_F^2
\\&amp;=\frac{1}{s^2}(C^2-\|AB\|_F^2).
\end{aligned}\]</span> The variance of samples is <span class="math inline">\(\displaystyle s\text{Var}[X]=\frac{1}{s}(C^2-\|AB\|_F^2)\)</span>.</p></li>
<li><p>Norm-squared sampling uses the optimal probabilities <span class="math inline">\(p_j\)</span> for minimum variance.</p></li>
</ul>
<p><strong><em>Proof.</em></strong> We want to solve <span class="math display">\[\min \sum_{j=1}^n \frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j}-\frac{1}{s}\|AB\|_F^2\ \ \text{s.t.}\ \ \sum_{j=1}^n p_j=1.\]</span> By Lagrange, <span class="math display">\[L(p_1, \ldots, p_n, \lambda)=\sum_{j=1}^n \frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j}+\lambda\left(\sum_{j=1}^n p_j-1\right).\]</span></p>
<p>Take the partial derivatives w.r.t. <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\lambda\)</span>, let <span class="math display">\[\frac{\partial L}{\partial p_j}=-\frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j^2}+\lambda=0\]</span> and <span class="math display">\[\frac{\partial L}{\partial \lambda}=\sum_{j=1}^n p_j-1=0,\]</span> then <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{\sqrt{s\lambda}}\)</span> and <span class="math inline">\(\displaystyle \sum_{j=1}^n p_j=1\)</span>.</p>
Therefore, <span class="math inline">\(\displaystyle \sum_{j=1}^n \frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{\sqrt{s\lambda}}=1\)</span> gives <span class="math inline">\(\sqrt{s\lambda}=C\)</span>. Hence, <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{C}\)</span> solves the optimization problem, i.e., norm-squared sampling uses the optimal probabilities <span class="math inline">\(p_j\)</span> for minimum variance.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Matrix method</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>Linear Algebra</title>
    <url>/Mathematics/Matrix-method/Linear_Algebra.html</url>
    <content><![CDATA[<h1 id="column-space">1. Column Space</h1>
<ul>
<li><span class="math inline">\(A\mathbf{x}\)</span> is a linear combination of the columns of <span class="math inline">\(A\)</span>, and the combinations of the columns fill out the <strong>column space</strong> of <span class="math inline">\(A\)</span>.</li>
</ul>
<p><strong>Example 1.1</strong>    Consider <span class="math inline">\(\begin{bmatrix} 2 &amp; 3 \\ 2 &amp; 4 \\ 3 &amp; 7 \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}=x_1\begin{bmatrix} 2 \\ 2 \\ 3 \end{bmatrix}+x_2\begin{bmatrix} 3 \\ 4 \\ 7 \end{bmatrix}\)</span>. The column space is a plane.</p>
<ul>
<li><p>A <strong>basis</strong> for a subspace is a full set of independent vectors, and all vectors in the space are combinations of the basis vectors.</p></li>
<li><p>The <strong>rank</strong> of a matrix <span class="math inline">\(r\)</span> is the dimension of its column space.</p></li>
</ul>
<p><strong>Example 1.2</strong>    If <span class="math inline">\(A=\begin{bmatrix} 1 &amp; 3 &amp; 8 \\ 1 &amp; 2 &amp; 6 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}\)</span>, then the <em>matrix of all independent columns</em> is <span class="math inline">\(C=\begin{bmatrix} 1 &amp; 3 \\ 1 &amp; 2 \\ 0 &amp; 1 \end{bmatrix}\)</span>.</p>
<ul>
<li>Suppose <span class="math inline">\(A \in \mathbb{R}^{m \times n}\)</span>, one of the <em>factorization</em>s of <span class="math inline">\(A\)</span> is <span class="math inline">\(A=CR\)</span>, whose shape is <span class="math display">\[(m \times n)=(m \times r)(r \times n).\]</span> <span class="math inline">\(R\)</span> is the <strong>row-reduced echelon form</strong> of <span class="math inline">\(A\)</span> (without zero rows), denoted <span class="math inline">\(\text{rref}(A)\)</span>.</li>
</ul>
<p><strong>Example 1.3</strong>    <span class="math inline">\(A=\begin{bmatrix} 1 &amp; 3 &amp; 8 \\ 1 &amp; 2 &amp; 6 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}=\begin{bmatrix} 1 &amp; 3 \\ 1 &amp; 2 \\ 0 &amp; 1 \end{bmatrix}\begin{bmatrix} 1 &amp; 0 &amp; 2 \\ 0 &amp; 1 &amp; 2 \end{bmatrix}=CR\)</span>.</p>
<ul>
<li>The number of independent columns equals the number of independent rows. The column space and row space of <span class="math inline">\(A\)</span> both have dimension <span class="math inline">\(r\)</span>, with <span class="math inline">\(r\)</span> basis vectors - columns of <span class="math inline">\(C\)</span> and rows of <span class="math inline">\(R\)</span>.</li>
</ul>
<h1 id="factorization-of-matrix">2. Factorization of Matrix</h1>
<ul>
<li><p><span class="math inline">\(A=LU\)</span> comes from <strong>elimination</strong>, where <span class="math inline">\(L\)</span> is lower triangular, and <span class="math inline">\(U\)</span> is upper triangular.</p></li>
<li><p><span class="math inline">\(A=QR\)</span> comes from <strong>orthogonalizing</strong> the columns <span class="math inline">\(\mathbf{a}_1\)</span> to <span class="math inline">\(\mathbf{a}_n\)</span> as in <strong>Gram-Schmidt</strong>, where <span class="math inline">\(Q\)</span> has orthonormal columns (i.e., <span class="math inline">\(Q^TQ=I\)</span>), and <span class="math inline">\(R\)</span> is upper triangular.</p></li>
<li><p><span class="math inline">\(S=Q \Lambda Q^T\)</span> comes from the <strong>eigenvalues</strong> <span class="math inline">\(\lambda_1, \ldots, \lambda_n\)</span> of a symmetric matrix, where eigenvalues are on the diagonal of <span class="math inline">\(\Lambda\)</span>, and orthonormal eigenvectors are in the columns of <span class="math inline">\(Q\)</span>.</p></li>
<li><p><span class="math inline">\(A=X \Lambda X^{-1}\)</span> is <strong>diagonalization</strong> when <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times n\)</span> with <span class="math inline">\(n\)</span> independent eigenvectors, where eigenvalues of <span class="math inline">\(A\)</span> are on the diagonal of <span class="math inline">\(\Lambda\)</span>, and eigenvectors of <span class="math inline">\(A\)</span> are in the columns of <span class="math inline">\(X\)</span>.</p></li>
<li><p><span class="math inline">\(A=U \Sigma V^T\)</span> is the <strong>singular value decomposition (SVD)</strong> of any matrix <span class="math inline">\(A\)</span>, where singular values <span class="math inline">\(\sigma_1, \ldots, \sigma_r\)</span> are in <span class="math inline">\(\Sigma\)</span>, and orthonormal singular vectors are in <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span>.</p></li>
</ul>
<h1 id="fundamental-subspace">3. Fundamental Subspace</h1>
<p>Suppose <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix with rank <span class="math inline">\(r\)</span>.</p>
<ul>
<li><p>The <strong>column space</strong> <span class="math inline">\(C(A)\)</span> contains all combinations of the columns of <span class="math inline">\(A\)</span>.</p></li>
<li><p>The <strong>row space</strong> <span class="math inline">\(C(A^T)\)</span> contains all combinations of the columns of <span class="math inline">\(A^T\)</span>.</p></li>
<li><p>The <strong>null space</strong> <span class="math inline">\(N(A)\)</span> contains all solutions <span class="math inline">\(\mathbf{x}\)</span> to <span class="math inline">\(A\mathbf{x}=\mathbf{0}\)</span>.</p></li>
<li><p>The <strong>left null space</strong> <span class="math inline">\(N(A^T)\)</span> contains all solutions <span class="math inline">\(\mathbf{y}\)</span> to <span class="math inline">\(A^T\mathbf{y}=\mathbf{0}\)</span>.</p></li>
</ul>
<h1 id="orthonormal-matrix">4. Orthonormal Matrix</h1>
<ul>
<li><p>If <span class="math inline">\(Q\)</span> is a matrix with <strong>orthonormal columns</strong>, then <span class="math inline">\(Q^TQ=I\)</span>.</p></li>
<li><p><strong>Orthogonal matrix</strong> is square with orthonormal columns: <span class="math inline">\(Q^T=Q^{-1}\)</span>, and <span class="math inline">\(Q^TQ=QQ^T=I\)</span>.</p></li>
</ul>
<p><strong>Example 4.1</strong>    Rotation through an angle <span class="math inline">\(\theta\)</span>: <span class="math inline">\(Q=\begin{bmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{bmatrix}\)</span>.</p>
<p><strong>Example 4.2</strong>    Reflection across the <span class="math inline">\(\displaystyle \frac{\theta}{2}\)</span> line: <span class="math inline">\(Q=\begin{bmatrix} \cos\theta &amp; \sin\theta \\ \sin\theta &amp; -\cos\theta \end{bmatrix}\)</span>.</p>
<ul>
<li><span class="math inline">\(\|Q\mathbf{x}\|^2=\|\mathbf{x}\|^2\)</span>.</li>
</ul>
<strong><em>Proof.</em></strong> We have <span class="math display">\[(Q\mathbf{x})^T(Q\mathbf{x})=\mathbf{x}^TQ^TQ\mathbf{x}=\mathbf{x}^TI\mathbf{x}=\mathbf{x}^T\mathbf{x}.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="householder-reflection">4.1. Householder Reflection</h2>
<ul>
<li><p>Householder matrix: <span class="math inline">\(H=I-2\mathbf{u}\mathbf{u}^T\)</span>, where <span class="math inline">\(\mathbf{u}^T\mathbf{u}=1\)</span>.</p></li>
<li><p><span class="math inline">\(H\)</span> is orthogonal matrix.</p></li>
</ul>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(H\)</span> is symmetric, then <span class="math display">\[H^TH=H^2=(I-2\mathbf{u}\mathbf{u}^T)(I-2\mathbf{u}\mathbf{u}^T)=I-4\mathbf{u}\mathbf{u}^T+4\mathbf{u}\mathbf{u}^T\mathbf{u}\mathbf{u}^T=I.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="eigenvalue-and-eigenvector">5. Eigenvalue and Eigenvector</h1>
<ul>
<li>The <strong>eigenvector</strong> <span class="math inline">\(\mathbf{x}\)</span> of <span class="math inline">\(A\)</span> does not change direction when multiplying <span class="math inline">\(\mathbf{x}\)</span> by <span class="math inline">\(A\)</span>, i.e., <span class="math inline">\(A\mathbf{x}=\lambda\mathbf{x}\)</span>.</li>
</ul>
<div class="note warning">
            <p><span class="math inline">\(\lambda(A+B) \not\equiv \lambda(A)+\lambda(B)\)</span> and <span class="math inline">\(\lambda(AB) \not\equiv \lambda(A)\lambda(B)\)</span>.</p>
          </div>
<ul>
<li><p>The sum of eigenvalues of a square <span class="math inline">\(A\)</span> equals the <strong>trace</strong> (the diagonal sum) of <span class="math inline">\(A\)</span>.</p></li>
<li><p>The product of eigenvalues of a square <span class="math inline">\(A\)</span> equals the determinant of <span class="math inline">\(A\)</span>.</p></li>
</ul>
<h2 id="similar-matrix">5.1. Similar Matrix</h2>
<ul>
<li><p>Two <span class="math inline">\(n \times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>similar</strong> if there exists an invertible <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(B=P^{-1}AP\)</span>. <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> have same eigenvalues: <span class="math display">\[B\mathbf{y}=P^{-1}AP\mathbf{y}=\lambda\mathbf{y} \Rightarrow A(P\mathbf{y})=\lambda (P\mathbf{y}).\]</span></p></li>
<li><p>Suppose <span class="math inline">\(A\)</span> has eigenvalues <span class="math inline">\(\lambda_1, \ldots, \lambda_n\)</span> with corresponding eigenvectors <span class="math inline">\(\mathbf{x}_1, \ldots, \mathbf{x}_n\)</span>. <span class="math inline">\(A=X\Lambda X^{-1}\)</span>, where eigenvalues of <span class="math inline">\(A\)</span> are on the diagonal of <span class="math inline">\(\Lambda\)</span>, and eigenvectors of <span class="math inline">\(A\)</span> are in the columns of <span class="math inline">\(X\)</span>, i.e., <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(\Lambda\)</span>.</p></li>
</ul>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(X=\begin{bmatrix} &amp; \\ \mathbf{x}_1 &amp; \cdots &amp; \mathbf{x}_n \\ &amp; \end{bmatrix}\)</span>. We have <span class="math display">\[AX=\begin{bmatrix}
&amp; \\ A\mathbf{x}_1 &amp; \cdots &amp; A\mathbf{x}_n \\ &amp;
\end{bmatrix}=\begin{bmatrix}
&amp; \\ \lambda_1\mathbf{x}_1 &amp; \cdots &amp; \lambda_n\mathbf{x}_n \\ &amp;
\end{bmatrix}\]</span> and <span class="math display">\[X\Lambda=\begin{bmatrix}
&amp; \\ \lambda_1\mathbf{x}_1 &amp; \cdots &amp; \lambda_n\mathbf{x}_n \\ &amp;
\end{bmatrix},\]</span> i.e., <span class="math inline">\(AX=X\Lambda\)</span>, and thus <span class="math inline">\(A=X\Lambda X^{-1}\)</span>. Hence, <span class="math inline">\(X^{-1}AX=\Lambda\)</span>, i.e., <span class="math inline">\(A\)</span> is similar to <span class="math inline">\(\Lambda\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="symmetric-postive-definite-matrix">6. Symmetric Postive Definite Matrix</h1>
<ul>
<li><p>All eigenvalues of a symmetric matrix <span class="math inline">\(S\)</span> are real numbers.</p></li>
<li><p>The eigenvectors of <span class="math inline">\(S\)</span> can be chosen orthogonal.</p></li>
<li><p><strong>(Spectral Theorem)</strong> Every real symmetric matrix has the form <span class="math inline">\(S=Q\Lambda Q^T\)</span>, where eigenvalues are on the diagonal of <span class="math inline">\(\Lambda\)</span>, and orthonormal eigenvectors are in the columns of <span class="math inline">\(Q\)</span>.</p></li>
<li><p>Suppose <span class="math inline">\(S\)</span> is a real symmetric matrix. <span class="math inline">\(S\)</span> is a positive definite matrix if and only if one of the following statements holds:</p></li>
</ul>
<p>         1. All eigenvalues of <span class="math inline">\(S\)</span> are positive;</p>
<p>         2. The <strong>energy</strong> <span class="math inline">\(\mathbf{x}^TS\mathbf{x}\)</span> is positive for all <span class="math inline">\(\mathbf{x} \neq \mathbf{0}\)</span>;</p>
<p>         3. <span class="math inline">\(S=A^TA\)</span> for a matrix <span class="math inline">\(A\)</span> with independent columns;</p>
<p>         4. All the leading determinants of <span class="math inline">\(S\)</span> are positive;</p>
<p>         5. All the pivots of <span class="math inline">\(S\)</span> are positive.</p>
<div class="note info">
            <p>The <span class="math inline">\(k\)</span>th pivot is the ratio <span class="math inline">\(\displaystyle \frac{D_k}{D_{k-1}}\)</span> of the leading determinants (sizes <span class="math inline">\(k\)</span> and <span class="math inline">\(k-1\)</span>).</p>
          </div>
<p><span id="7"></span></p>
<h1 id="singular-value-decomposition-svd">7. Singular Value Decomposition (SVD)</h1>
<ul>
<li>The <strong>singular value decomposition</strong> of <span class="math inline">\(A\)</span> is <span class="math inline">\(A=U\Sigma V^T\)</span>, where <span class="math inline">\(U\)</span> contains orthonormal eigenvectors of <span class="math inline">\(AA^T\)</span>, <span class="math inline">\(\Sigma\)</span> contains singular values <span class="math inline">\(\sigma_1, \ldots, \sigma_r\)</span>, and <span class="math inline">\(V\)</span> contains orthonormal eigenvectors of <span class="math inline">\(A^TA\)</span>. Note that <span class="math inline">\(\sigma_1^2 \geq \ldots \geq \sigma_r^2\)</span> are the nonzero eigenvalues of both <span class="math inline">\(AA^T\)</span> and <span class="math inline">\(A^TA\)</span> for <span class="math inline">\(r\)</span> is the rank of <span class="math inline">\(A\)</span>.</li>
</ul>
<p><strong>Example 7.1</strong>    Find an SVD of <span class="math inline">\(A=\begin{bmatrix} 3 &amp; 0 \\ 4 &amp; 5 \end{bmatrix}\)</span>.</p>
<p><strong><em>Solution.</em></strong> We have <span class="math inline">\(AA^T=\begin{bmatrix} 9 &amp; 12 \\ 12 &amp; 41 \end{bmatrix}\)</span> and <span class="math inline">\(A^TA=\begin{bmatrix} 25 &amp; 20 \\ 20 &amp; 25 \end{bmatrix}\)</span>. The nonzero eigenvalues of both <span class="math inline">\(AA^T\)</span> and <span class="math inline">\(A^TA\)</span> are <span class="math inline">\(\lambda_1=45\)</span> and <span class="math inline">\(\lambda_2=5\)</span>. The eigenvectors of <span class="math inline">\(AA^T\)</span> is <span class="math inline">\(\begin{bmatrix} 1 \\ 3 \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} -3 \\ 1 \end{bmatrix}\)</span>. The eigenvectors of <span class="math inline">\(A^TA\)</span> is <span class="math inline">\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span>. Therefore, <span class="math inline">\(\displaystyle U=\frac{1}{\sqrt{10}}\begin{bmatrix} 1 &amp; -3 \\ 3 &amp; 1 \end{bmatrix}\)</span> and <span class="math inline">\(\displaystyle V=\frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix}\)</span>, and <span class="math inline">\(\Sigma=\begin{bmatrix} \sqrt{45} &amp; 0 \\ 0 &amp; \sqrt{5} \end{bmatrix}\)</span>.</p>
<p><strong>Example 7.2</strong>    Find an SVD of <span class="math inline">\(A=\begin{bmatrix} 1 &amp; 1 \\ 1 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}\)</span>.</p>
<p><strong><em>Solution.</em></strong> We have <span class="math inline">\(AA^T=\begin{bmatrix} 2 &amp; 2 &amp; 0 \\ 2 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span> and <span class="math inline">\(A^TA=\begin{bmatrix} 2 &amp; 2 \\ 2 &amp; 2 \end{bmatrix}\)</span>. The nonzero eigenvalue of both <span class="math inline">\(AA^T\)</span> and <span class="math inline">\(A^TA\)</span> is <span class="math inline">\(\lambda_1=4\)</span>. The eigenvectors of <span class="math inline">\(AA^T\)</span> is <span class="math inline">\(\begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}\)</span>, <span class="math inline">\(\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}\)</span>, and <span class="math inline">\(\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}\)</span>. The eigenvectors of <span class="math inline">\(A^TA\)</span> is <span class="math inline">\(\begin{bmatrix} 1 \\ 1 \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span>. Therefore, <span class="math inline">\(U=\begin{bmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} &amp; 0 \\ 1/\sqrt{2} &amp; 1/\sqrt{2} &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}\)</span> and <span class="math inline">\(V=\displaystyle \frac{1}{\sqrt{2}}\begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 1 \end{bmatrix}\)</span>, and <span class="math inline">\(\Sigma=\begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\)</span>.</p>
<ul>
<li>The pieces of the SVD: <span class="math inline">\(A=U\Sigma V^T=\sigma_1\mathbf{u}_1\mathbf{v}_1^T+\cdots+\sigma_r\mathbf{u}_r\mathbf{v}_r^T\)</span>.</li>
</ul>
<h1 id="norm-of-vector-and-matrix">8. Norm of Vector and Matrix</h1>
<h2 id="vector-norm">8.1. Vector Norm</h2>
<p>Suppose <span class="math inline">\(\mathbf{v} \in \mathbb{C}^n\)</span>.</p>
<ul>
<li><p><strong><span class="math inline">\(\mathscr{l}^2\)</span> norm</strong>: <span class="math inline">\(\|\mathbf{v}\|_2=\sqrt{|v_1|^2+\cdots+|v_n|^2}\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\mathscr{l}^1\)</span> norm</strong>: <span class="math inline">\(\|\mathbf{v}\|_1=|v_1|+\cdots+|v_n|\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\mathscr{l}^\infty\)</span> norm</strong>: <span class="math inline">\(\max |v_i|\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\mathscr{l}^p\)</span> norm</strong>: <span class="math inline">\(\|\mathbf{v}\|_p=(|v_1|^p+\cdots+|v_n|^p)^{1/p}\)</span>.</p></li>
<li><p><strong><span class="math inline">\(S\)</span> norm</strong>: <span class="math inline">\(\|\mathbf{v}\|_S=\sqrt{\mathbf{v}^TS\mathbf{v}}\)</span> for any symmetric positive definite matrix <span class="math inline">\(S\)</span>.</p></li>
</ul>
<h2 id="matrix-norm">8.2. Matrix Norm</h2>
<ul>
<li><p><strong><span class="math inline">\(\mathscr{l}^2\)</span> norm</strong> or <strong>Spectral norm</strong>: <span class="math display">\[\begin{aligned}
\|A\|_2&amp;=\max \frac{\|A\mathbf{x}\|_2}{\|\mathbf{x}\|_2}=\max\frac{\sqrt{\mathbf{x}^T(A^TA)\mathbf{x}}}{\sqrt{\mathbf{x}^T\mathbf{x}}}=\max\sqrt{\frac{\lambda_1c_1^2+\cdots+\lambda_rc_r^2}{c_1^2+\cdots+c_r^2}}
\\&amp;=\sqrt{\lambda_1}=\sigma_1,
\end{aligned}\]</span> where <span class="math inline">\(\lambda_1 \geq \cdots \geq \lambda_r\)</span>, i.e., the largest singular value of <span class="math inline">\(A\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\mathscr{l}^1\)</span> norm</strong>: <span class="math inline">\(\|A\|_1\)</span> is the largest <span class="math inline">\(\mathscr{l}^1\)</span> norm of the columns of <span class="math inline">\(A\)</span>.</p></li>
<li><p><strong><span class="math inline">\(\mathscr{l}^\infty\)</span> norm</strong>: <span class="math inline">\(\|A\|_1\)</span> is the largest <span class="math inline">\(\mathscr{l}^1\)</span> norm of the rows of <span class="math inline">\(A\)</span>.</p></li>
<li><p><strong>Frobenius norm</strong>: <span class="math inline">\(\|A\|_F=\sqrt{|a_{11}|^2+\cdots+|a_{1n}|^2+\cdots+|a_{mn}|^2}=\sqrt{\sigma_1^2+\cdots+\sigma_r^2}\)</span>.</p></li>
<li><p><strong>Nuclear norm</strong>: <span class="math inline">\(\|A\|_N=\sigma_1+\cdots+\sigma_r\)</span>.</p></li>
<li><p>For any orthogonal matrix <span class="math inline">\(Q\)</span>, we have <span class="math inline">\(\|Q\|_2=1\)</span>, <span class="math inline">\(\|Q\|_F=\sqrt{n}\)</span>, and <span class="math inline">\(\|Q\|_N=n\)</span>. The spectral, Frobenius and nuclear norms of any matrix stay the same when <span class="math inline">\(A\)</span> is multiplies (on either side) by an orthogonal matrix.</p></li>
</ul>
<h1 id="principal-component-analysis-pca">9. Principal Component Analysis (PCA)</h1>
<ul>
<li><strong>(Eckart-Young Theorem)</strong> If <span class="math inline">\(B\)</span> has rank <span class="math inline">\(k\)</span>, then <span class="math display">\[\|A-B\| \geq \|A-A_k\|,\]</span> where <span class="math inline">\(A_k=U_k\Sigma_kV_k^T=\sigma_1\mathbf{u}_1\mathbf{v}_1^T+\cdots+\sigma_k\mathbf{u}_k\mathbf{v}_k^T\)</span> with <span class="math inline">\(\text{rank}(A_k)=k\)</span>.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Matrix method</category>
      </categories>
      <tags>
        <tag>Linear algebra</tag>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title>Loss Landscape</title>
    <url>/Computer-science/Deep-learning/Loss_Landscape.html</url>
    <content><![CDATA[<h1 id="topology-of-loss-landscape">1. Topology of Loss Landscape</h1>
<p>Consider the <strong>loss function</strong> <span class="math display">\[\mathcal{L}(\theta; X, Y)=n^{-1}\sum_{\mu=1}^n l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu)\]</span> and its associated <strong>level set</strong> <span class="math display">\[\Omega_\mathcal{L}(\lambda)=\{\theta: \mathcal{L}(\theta; X, Y) \leq \lambda\}.\]</span></p>
<p>Define the <strong>number of connected components</strong>, say <span class="math inline">\(N_\lambda\)</span> in <span class="math inline">\(\Omega_\mathcal{L}(\lambda)\)</span>. If <span class="math inline">\(N_\lambda=1\)</span> for all <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span> has no isolated local minima and any descent method can obtain a global minima. If <span class="math inline">\(N_\lambda&gt;1\)</span>, there may be spurious valleys in which the minima in the connected component does not achieve the global minima.</p>
<h2 id="linear-network-single-component">1.1. Linear Network: Single Component</h2>
<p><strong>Theorem (Freeman et al., 2016)</strong>     Let <span class="math inline">\(H(\mathbf{x}; \theta)\)</span> be an <span class="math inline">\(L\)</span> layer net given by <span class="math inline">\(\mathbf{h}^{(\mathscr{l})}=W^{(\mathscr{l})}\mathbf{h}^{(\mathscr{l}-1)}\)</span> with <span class="math inline">\(W^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l} \times n_{\mathscr{l}-1}}\)</span>, then if <span class="math inline">\(n_\mathscr{l}&gt;\min(n_0, n_L)\)</span> for <span class="math inline">\(0&lt;\mathscr{l}&lt;L\)</span>, the sum of squares loss function has a single connected component.</p>
<h2 id="relu-network-multiple-components">1.2. ReLU Network: Multiple Components</h2>
<p><strong>Theorem (Freeman et al., 2016)</strong>     Let <span class="math inline">\(H(\mathbf{x}; \theta)\)</span> be an <span class="math inline">\(L\)</span> layer net given by <span class="math inline">\(\mathbf{h}^{(\mathscr{l})}=\phi(W^{(\mathscr{l})}\mathbf{h}^{(\mathscr{l}-1)})\)</span> with <span class="math inline">\(W^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l} \times n_{\mathscr{l}-1}}\)</span> and <span class="math inline">\(\phi(\cdot)=\max(0, \cdot)\)</span>, then for any choice of <span class="math inline">\(n_\mathscr{l}\)</span>, there is a distribution of data <span class="math inline">\((X, Y)\)</span> s.t. there are more than one single connected component.</p>
<h2 id="relu-activation-network-nearly-connected">1.3. ReLu Activation Network: Nearly Connected</h2>
<p><strong>Theorem (Venturi et al., 2016)</strong>     Consider a two layer ReLu network <span class="math inline">\(H(\mathbf{x}, \theta)=W^{(2)}\phi(W^{(1)}\mathbf{x})\)</span> with <span class="math inline">\(W^{(1)} \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(W^{(2)} \in \mathbb{R}^m\)</span>, then for any two parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> with <span class="math inline">\(\mathcal{L}(\theta_i) \leq \lambda\)</span> for <span class="math inline">\(i=1, 2\)</span>, then there is a path <span class="math inline">\(\gamma(t)\)</span> between <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> s.t. <span class="math inline">\(\mathcal{L}(\theta_{\gamma(t)}) \leq \max(\lambda, m^{-1/n})\)</span>.</p>
<h2 id="quadratic-activation-network-single-component">1.4. Quadratic Activation Network: Single Component</h2>
<p><strong>Theorem (Venturi et al., 2016)</strong>     Let <span class="math inline">\(H(\mathbf{x}; \theta)\)</span> be an <span class="math inline">\(L\)</span> layer net given by <span class="math inline">\(\mathbf{h}^{(\mathscr{l})}=\phi(W^{(\mathscr{l})}\mathbf{h}^{(\mathscr{l}-1)})\)</span> with <span class="math inline">\(W^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l} \times n_{\mathscr{l}-1}}\)</span> and quadratic activation <span class="math inline">\(\phi(z)=z^2\)</span>, then once the number of parameters <span class="math inline">\(n_\mathscr{l} \geq 3N^{2^\mathscr{l}}\)</span> where <span class="math inline">\(N\)</span> is the number of data entries, then the sum of squares loss function has a single connected component. For the two layer case with a single quadratic activation this simplifies to <span class="math inline">\(n&gt;2N\)</span>.</p>
<h1 id="manifold-of-global-minimizer">2. Manifold of Global Minimizer</h1>
<p><strong>Theorem (Cooper, 2021)</strong>     Let <span class="math inline">\(H(\mathbf{x}; \theta)\)</span> be a DNN from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^r\)</span> with smooth nonlinear activation <span class="math inline">\(\phi(\cdot)\)</span>, let the loss function over <span class="math inline">\(d\)</span> distinct data elements be defined as <span class="math display">\[\mathcal{L}=(2m)^{-1}\sum_{\mu=1}^d \|H(\mathbf{x}_\mu; \theta)-\mathbf{y}_\mu\|^2_2,\]</span> and let <span class="math inline">\(\Omega_{\mathcal{L}}^*(0)=\{\theta: \mathcal{L}(\theta; X, Y)=0\}\)</span> be the set of weight and bias trainable parameters for which the DNN exactly fits the <span class="math inline">\(d\)</span> data elements. Then, subject to possibly arbitrarily small perturbation, the set <span class="math inline">\(\Omega_\mathcal{L}^*(0)\)</span> is a smooth <span class="math inline">\((d-rn)\)</span> dimensional sub-manifold (possibly empty) of <span class="math inline">\(\mathbb{R}^d\)</span>.</p>
<h1 id="summary">3. Summary</h1>
<ul>
<li><p>Loss landscape for DNN can be non-convex and hence difficult to optimize.</p></li>
<li><p>The number of components of a loss landscape level curve can be analyzed, and in some settings has a single component greatly aiding its optimization.</p></li>
<li><p>Increasing width of a DNN can improve the loss landscape.</p></li>
<li><p>The local shape of random net can be analyzed, showing that when near a minima the Hessian has only non-negative eigenvalues.</p></li>
<li><p>When the amount of data exceeds the product of the input and output dimensions, DNN with smooth non-linear activations which exactly fit the data, have smooth manifold of a known dimension.</p></li>
</ul>
<h1 id="improvement">4. Improvement</h1>
<ul>
<li><p>Larger training batch size narrows the loss function while weight decay (adding <span class="math inline">\(\|\theta\|\)</span> to the loss, broadens the loss function).</p></li>
<li><p>Adding skip connections through residual networks can greatly smooth the loss landscape.</p></li>
<li><p>Batch normalization can help train parameters in bulk and in so doing improve the training rate (though superfluous for expressivity).</p></li>
<li><p>CNN can even be convexified which may limit overall accuracy, but ensures ease of training regardless of initialization.</p></li>
</ul>
]]></content>
      <categories>
        <category>Computer science</category>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>Loss landscape</tag>
      </tags>
  </entry>
  <entry>
    <title>Low Rank and Compressed Sensing</title>
    <url>/Mathematics/Matrix-method/Low_Rank_and_Compressed_Sensing.html</url>
    <content><![CDATA[<h1 id="sherman-morrison-woodbury-formula">1. Sherman-Morrison-Woodbury Formula</h1>
<ul>
<li><span class="math inline">\(\displaystyle (I-\mathbf{u}\mathbf{v}^T)^{-1}=I+\frac{\mathbf{u}\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}\)</span>.</li>
</ul>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\begin{aligned}
(I-\mathbf{u}\mathbf{v}^T)(I-\mathbf{u}\mathbf{v}^T)^{-1}&amp;=(I-\mathbf{u}\mathbf{v}^T)\left(I+\frac{\mathbf{u}\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}\right)
\\&amp;=I-\mathbf{u}\mathbf{v}^T+\frac{(I-\mathbf{u}\mathbf{v}^T)\mathbf{u}\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}
\\&amp;=I-\mathbf{u}\mathbf{v}^T+\frac{\mathbf{u}\mathbf{v}^T-\mathbf{u}(\mathbf{v}^T\mathbf{u})\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}
\\&amp;=I-\mathbf{u}\mathbf{v}^T+\frac{(1-\mathbf{v}^T\mathbf{u})\mathbf{u}\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}
\\&amp;=I
\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
(I-\mathbf{u}\mathbf{v}^T)^{-1}(I-\mathbf{u}\mathbf{v}^T)&amp;=\left(I+\frac{\mathbf{u}\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}\right)(I-\mathbf{u}\mathbf{v}^T)
\\&amp;=I-\mathbf{u}\mathbf{v}^T+\frac{\mathbf{u}\mathbf{v}^T(I-\mathbf{u}\mathbf{v}^T)}{1-\mathbf{v}^T\mathbf{u}}
\\&amp;=I-\mathbf{u}\mathbf{v}^T+\frac{\mathbf{u}\mathbf{v}^T-\mathbf{u}(\mathbf{v}^T\mathbf{u})\mathbf{v}^T}{1-\mathbf{v}^T\mathbf{u}}
\\&amp;=I.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<ul>
<li><span class="math inline">\((I_n-UV^T)^{-1}=I_n+U(I_k-V^TU)^{-1}V^T\)</span>, where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are <span class="math inline">\(n \times k\)</span> matrices.</li>
</ul>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\begin{aligned}
(I_n-UV^T)(I_n-UV^T)^{-1}&amp;=I_n-UV^T+(I_n-UV^T)U(I_k-V^TU)^{-1}V^T
\\&amp;=I_n-UV^T+(U-UV^TU)(I_k-V^TU)^{-1}V^T
\\&amp;=I_n-UV^T+U(I_k-V^TU)(I_k-V^TU)^{-1}V^T
\\&amp;=I_n-UV^T+UV^T
\\&amp;=I_n
\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
(I_n-UV^T)^{-1}(I_n-UV^T)&amp;=I_n+U(I_k-V^TU)^{-1}V^T-(I_n+U(I_k-V^TU)^{-1}V^T)UV^T
\\&amp;=I_n+U(I_k-V^TU)^{-1}V^T-UV^T-U(I_k-V^TU)^{-1}V^TUV^T
\\&amp;=I_n-UV^T+U(I_k-V^TU)^{-1}V^T(I_n-UV^T)
\\&amp;=I_n-UV^T+U(I_k-V^TU)^{-1}(V^T-V^TUV^T)
\\&amp;=I_n-UV^T+U(I_k-V^TU)^{-1}(I_k-V^TU)V^T
\\&amp;=I_n-UV^T+UV^T
\\&amp;=I_n.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<ul>
<li><span class="math inline">\((A-UV^T)^{-1}=A^{-1}+A^{-1}U(I-V^TA^{-1}U)^{-1}V^TA^{-1}\)</span>.</li>
</ul>
<h2 id="application-solving-a-perturbation-problem">1.1. Application: Solving a Perturbation Problem</h2>
<ul>
<li>Suppose <span class="math inline">\(A\mathbf{w}=\mathbf{b}\)</span> is solved for <span class="math inline">\(\mathbf{w}\)</span>, and we want to solve <span class="math inline">\((A-\mathbf{u}\mathbf{v}^T)\mathbf{x}=\mathbf{b}\)</span>. We first solve <span class="math inline">\(A\mathbf{z}=\mathbf{u}\)</span>, then <span class="math display">\[\mathbf{x}=\mathbf{w}+\frac{\mathbf{v}^T\mathbf{w}\mathbf{z}}{1-\mathbf{v}^T\mathbf{z}}.\]</span></li>
</ul>
<h2 id="application-recursive-least-squares">1.2. Application: Recursive Least Squares</h2>
<ul>
<li>Suppose there is a new measurement in least squares: <span class="math inline">\(\begin{bmatrix} A \\ \mathbf{v} \end{bmatrix}\mathbf{x}=\begin{bmatrix} \mathbf{b} \\ b_\text{new} \end{bmatrix}\)</span>. The new normal equation is <span class="math display">\[\begin{bmatrix}
A^T &amp; \mathbf{v}^T
\end{bmatrix}\begin{bmatrix}
A \\ \mathbf{v}
\end{bmatrix}\widehat{\mathbf{x}}_\text{new}=\begin{bmatrix}
A^T &amp; \mathbf{v}^T
\end{bmatrix}\begin{bmatrix}
\mathbf{b} \\ b_\text{new}
\end{bmatrix},\]</span> i.e., <span class="math inline">\((A^TA+\mathbf{v}^T\mathbf{v})\widehat{\mathbf{x}}_\text{new}=A^T\mathbf{b}+\mathbf{v}^Tb_\text{new}\)</span>. The update formula is <span class="math display">\[(A^TA+\mathbf{v}^T\mathbf{v})^{-1}=(A^TA)^{-1}-c(A^TA)^{-1}\mathbf{v}^T\mathbf{v}(A^TA)^{-1}\]</span> with <span class="math inline">\(\displaystyle c=\frac{1}{1+\mathbf{v}(A^TA)^{-1}\mathbf{v}^T}\)</span>. To find <span class="math inline">\(c\)</span>, we only need to solve the old equation <span class="math inline">\((A^TA)\mathbf{y}=\mathbf{v}^T\)</span>. The same idea applies when <span class="math inline">\(A\)</span> has more new rows.</li>
</ul>
<h2 id="application-kalman-filter">1.3. Application: Kalman Filter</h2>
<ul>
<li>The update ideal also applies to <strong>dynamic least squares</strong>.</li>
</ul>
<h1 id="derivative-of-a-1">2. Derivative of \(A^{-1}\)</h1>
<ul>
<li>Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are invertible. <span class="math inline">\(B^{-1}-A^{-1}=B^{-1}(A-B)A^{-1}\)</span>.</li>
</ul>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are invertible, then <span class="math inline">\(B^{-1}(A-B)A^{-1}=B^{-1}AA^{-1}-B^{-1}BA^{-1}=B^{-1}-A^{-1}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<ul>
<li>Let <span class="math inline">\(\Delta A=B-A\)</span>, then <span class="math inline">\(\displaystyle \frac{\Delta A^{-1}}{\Delta t}=(A+\Delta A)^{-1}\left(-\frac{\Delta A}{\Delta t}\right)A^{-1}\)</span>. Therefore, as <span class="math inline">\(\Delta t \to 0\)</span>, <span class="math display">\[\frac{\text{d}A^{-1}}{\text{d}t}=-A^{-1}\frac{\text{d}A}{\text{d}t}A^{-1}.\]</span></li>
</ul>
<h1 id="change-in-eigenvalues-and-singular-values">3. Change in Eigenvalues and Singular Values</h1>
<ul>
<li><p><span class="math inline">\(A(t)\mathbf{x}(t)=\lambda(t)\mathbf{x}(t)\)</span>, <span class="math inline">\(\mathbf{y}^T(t)A(t)=\lambda(t)\mathbf{y}^T(t)\)</span>, and <span class="math inline">\(\mathbf{y}^T(t)\mathbf{x}(t)=1\)</span>. In matrix notation, <span class="math inline">\(AX=X\Lambda\)</span>, <span class="math inline">\(Y^TA=\Lambda Y^T\)</span>, and <span class="math inline">\(Y^TX=I\)</span>.</p></li>
<li><p><span class="math inline">\(\lambda(t)=\mathbf{y}^T(t)A(t)\mathbf{x}(t)\)</span>.</p></li>
</ul>
<strong><em>Proof.</em></strong> <span class="math inline">\(\mathbf{y}^T(t)A(t)\mathbf{x}(t)=\lambda(t)\mathbf{y}^T(t)\mathbf{x}(t)=\lambda(t)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<ul>
<li><span class="math inline">\(\displaystyle \frac{\text{d}\lambda}{\text{d}t}=\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t)\)</span>.</li>
</ul>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(\lambda(t)=\mathbf{y}^T(t)A(t)\mathbf{x}(t)\)</span>, then <span class="math display">\[\begin{aligned}
\frac{\text{d}\lambda}{\text{d}t}&amp;=\frac{\text{d}\mathbf{y}^T}{\text{d}t}A(t)\mathbf{x}(t)+\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t)+\mathbf{y}^T(t)A(x)\frac{\text{d}\mathbf{x}}{\text{d}t}
\\&amp;=\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t)+\lambda(t)\left(\frac{\text{d}\mathbf{y}^T}{\text{d}t}\mathbf{x}(t)+\mathbf{y}^T(t)\frac{\text{d}\mathbf{x}}{\text{d}t}\right)
\\&amp;=\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t)+\lambda(t)\frac{\text{d}(\mathbf{y}^T\mathbf{x})}{\text{d}t}
\\&amp;=\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t)+\lambda(t)\frac{\text{d}(1)}{\text{d}t}
\\&amp;=\mathbf{y}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{x}(t).
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<ul>
<li><span class="math inline">\(\displaystyle \frac{\text{d}\sigma}{\text{d}t}=\mathbf{u}^T(t)\frac{\text{d}A}{\text{d}t}\mathbf{v}(t)\)</span>.</li>
</ul>
<h1 id="interlacing">4. Interlacing</h1>
<ul>
<li>Suppose the eigenvalues of a symmetric matrix <span class="math inline">\(S\)</span> is <span class="math inline">\(\mu_1 \geq \cdots \geq \mu_n\)</span>, and the eigenvalues of <span class="math inline">\(S+\mathbf{u}\mathbf{u}^T\)</span> is <span class="math inline">\(\lambda_1 \geq \cdots \geq \lambda_n\)</span>, then <span class="math inline">\(\lambda_1 \geq \mu_1 \geq \lambda_2 \geq \mu_2 \geq \cdots \geq \lambda_n \geq \mu_n\)</span>.</li>
</ul>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Matrix method</category>
      </categories>
      <tags>
        <tag>Matrix</tag>
      </tags>
  </entry>
  <entry>
    <title>Measurable Function</title>
    <url>/Mathematics/Real-analysis/Measurable_Function.html</url>
    <content><![CDATA[<h1 id="preliminary">1. Preliminary</h1>
<p>We now expand <span class="math inline">\(\mathbb{R}\)</span> to <span class="math inline">\(\mathbb{R} \cup \{-\infty, \infty\}\)</span>. Note that we do not define <span class="math inline">\((\infty)-(\infty)\)</span>, <span class="math inline">\((-\infty)-(-\infty)\)</span>, <span class="math inline">\((\infty)+(-\infty)\)</span>, <span class="math inline">\((-\infty)+(\infty)\)</span>, <span class="math inline">\(\displaystyle \frac{a}{0}\)</span>, and <span class="math inline">\(\displaystyle \frac{\pm\infty}{0}\)</span>.</p>
<p>We define <span class="math inline">\(f: E \to \mathbb{R}\)</span> as a <strong>finite function</strong>, which is different from bounded function. For example, <span class="math inline">\(f(x)=x^{-1}\)</span>, where <span class="math inline">\(x \in (0, \infty)\)</span>, is finite but not bounded.</p>
<h1 id="definition">2. Definition</h1>
<p><strong>Definition 2.1</strong>    Suppose <span class="math inline">\(E\)</span> is a measurable set. <span class="math inline">\(f: E \to \mathbb{R} \cup \{-\infty, \infty\}\)</span> is a measurable function if <span class="math display">\[E[f&gt;a]:=\{x \in E: f(x)&gt;a\}\]</span> is measurable for all <span class="math inline">\(a \in \mathbb{R}\)</span>.</p>
<div class="note warning">
            <p><span class="math inline">\(f(x)&gt;a \Leftrightarrow f(x) \in (a, \infty]\)</span>.</p>
          </div>
<p><strong>Theorem 2.1</strong>    <span class="math inline">\(f\)</span> is a measurable function if and only if <span class="math inline">\(E[f \leq a]\)</span> is measurable for all <span class="math inline">\(a \in \mathbb{R}\)</span>.</p>
<p><strong>Theorem 2.2</strong>    <span class="math inline">\(f\)</span> is a measurable function if and only if <span class="math inline">\(E[f \geq a]\)</span> is measurable for all <span class="math inline">\(a \in \mathbb{R}\)</span>.</p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Rightarrow)\)</span> We have <span class="math display">\[\begin{aligned}
E[f \geq a]&amp;=f^{-1}([a, \infty])=f^{-1}\left(\bigcap_{n=1}^\infty \left(a-\frac{1}{n}, \infty\right]\right)
\\&amp;=\bigcap_{n=1}^\infty f^{-1}\left(\left(a-\frac{1}{n}, \infty\right]\right)=\bigcap_{n=1}^\infty E\left[f&gt;a-\frac{1}{n}\right].
\end{aligned}\]</span> Since <span class="math inline">\(f\)</span> is measurable, then <span class="math inline">\(\displaystyle E\left[f&gt;a-\frac{1}{n}\right]\)</span> is measurable and thus <span class="math inline">\(E[f \geq a]\)</span> is measurable.</p>
<span class="math inline">\((\Leftarrow)\)</span> We have <span class="math inline">\(\displaystyle E[f&gt;a]=\bigcup_{n=1}^\infty E\left[f \geq a+\frac{1}{n}\right]\)</span>, and thus <span class="math inline">\(f\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.3</strong>    <span class="math inline">\(f\)</span> is a measurable function if and only if <span class="math inline">\(E[f&lt;a]\)</span> is measurable for all <span class="math inline">\(a \in \mathbb{R}\)</span>.</p>
<p><strong>Theorem 2.4</strong>    <span class="math inline">\(f\)</span> is a measurable function if and only if when <span class="math inline">\(|f(x)|&lt;\infty\)</span>, <span class="math inline">\(E[a \leq f&lt;b]\)</span> is measurable for all <span class="math inline">\(a, b \in \mathbb{R}\)</span> (<span class="math inline">\(a&lt;b\)</span>).</p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Rightarrow)\)</span> Since <span class="math inline">\(f\)</span> is measurable function and <span class="math display">\[E[a \leq f&lt;b]=E[f&lt;b]-E[f&lt;a],\]</span> then <span class="math inline">\(E[a \leq f&lt;b]\)</span>is measurable. Note that we do not need <span class="math inline">\(|f(x)|&lt;\infty\)</span> here.</p>
<p><span class="math inline">\((\Leftarrow)\)</span> Suppose <span class="math inline">\(|f(x)|&lt;\infty\)</span> and <span class="math inline">\(E[a \leq f&lt;b]\)</span> is measurable. Since <span class="math display">\[E[f \geq a]=E[f \in [a, \infty)]=\bigcup_{n=1}^\infty E[a \leq f&lt;a+n]\]</span> and <span class="math inline">\(E[a \leq f&lt;a+n]\)</span> is measurable, then <span class="math inline">\(E[f \geq a]\)</span> is measurable, and thus <span class="math inline">\(f\)</span> is measurable.</p>
<p><strong>Corollary 2.5</strong>    If <span class="math inline">\(f: E \to \mathbb{R} \cup \{-\infty, \infty\}\)</span> is measurable function, then <span class="math inline">\(E[f=a]\)</span> is measurable.</p>
<p><strong><em>Proof.</em></strong> When <span class="math inline">\(a \in \mathbb{R}\)</span>, we have <span class="math display">\[E[f=a]=E[f \geq a]-E[f&gt;a].\]</span> Since <span class="math inline">\(E[f \geq a]\)</span> and <span class="math inline">\(E[f&gt;a]\)</span> are measurable, then <span class="math inline">\(E[f=a]\)</span> is measurable.</p>
<p>When <span class="math inline">\(a=\infty\)</span>, we have <span class="math display">\[E[f=\infty]=\bigcap_{n=1}^\infty E[n&lt;f \leq \infty].\]</span> Since <span class="math inline">\(E[n&lt;f \leq \infty]\)</span> is measurable, then <span class="math inline">\(E[f=\infty]\)</span> is measurable.</p>
When <span class="math inline">\(a=-\infty\)</span>, we have <span class="math display">\[E[f=-\infty]=\bigcap_{n=1}^\infty E[-\infty \leq f&lt;-n].\]</span> Since <span class="math inline">\(E[-\infty \leq f&lt;-n]\)</span> is measurable, then <span class="math inline">\(E[f=-\infty]\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="operation">3. Operation</h1>
<p><strong>Theorem 3.1</strong>    Suppose <span class="math inline">\(E\)</span> is measurable, and <span class="math inline">\(g: E \to \mathbb{R} \cup \{-\infty, \infty\}\)</span> is a constant function, then <span class="math inline">\(g\)</span> is a measurable function.</p>
<strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, we have <span class="math display">\[E[f&gt;a]=\begin{cases}
E, &amp;a&lt;c \\
\varnothing, &amp;a \geq c
\end{cases},\]</span> which is measurable, then <span class="math inline">\(g\)</span> is a measurable function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.2</strong>    Suppose <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are constant functions, then <span class="math inline">\(f \pm g\)</span>, <span class="math inline">\(f \cdot g\)</span>, and <span class="math inline">\(\displaystyle \frac{f}{g}\)</span> are measurable functions.</p>
<p><strong>Theorem 3.3</strong>    Suppose <span class="math inline">\(f\)</span> is a measurable function, and <span class="math inline">\(g=c\)</span> is a constant function, then <span class="math inline">\(f \pm g\)</span>, <span class="math inline">\(f \cdot g\)</span>, and <span class="math inline">\(\displaystyle \frac{f}{g}\)</span> are measurable functions.</p>
<p><strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, we have <span class="math display">\[E[f+g&gt;a]=E[f+c&gt;a]=E[f&gt;a-c],\]</span> which is measurable, then <span class="math inline">\(f+c\)</span> is a measurable function. Besides, <span class="math display">\[E[f \cdot c&gt;a]=\begin{cases}
\displaystyle E\left[f&gt;\frac{a}{c}\right], &amp;c&gt;0 \\
\displaystyle E\left[f&lt;\frac{a}{c}\right], &amp;c&lt;0 \\
\end{cases},\]</span> which is measurable, and when <span class="math inline">\(c=0\)</span>, <span class="math inline">\(f \cdot c=0\)</span> is measurable. Therefore, <span class="math inline">\(f \cdot c\)</span> is a measurable function.</p>
As a consequence, <span class="math inline">\(f-g\)</span> and <span class="math inline">\(\displaystyle \frac{f}{g}\)</span> are measurable functions.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.4</strong>    If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are measurable functions on <span class="math inline">\(E\)</span>, then <span class="math inline">\(E[f&gt;g]\)</span> and <span class="math inline">\(E[f \geq g]\)</span> are measurable.</p>
<p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(x \in E[f&gt;g]\)</span>. There exists an <span class="math inline">\(r \in \mathbb{Q}\)</span> such that <span class="math inline">\(f(x)&gt;r&gt;g(x)\)</span>, i.e., <span class="math display">\[x \in E[f&gt;r] \cap E[g&lt;r].\]</span> Hence, <span class="math display">\[E[f&gt;g] \subset \bigcup_{r \in \mathbb{Q}} (E[f&gt;r] \cap E[g&lt;r]).\]</span> For any <span class="math inline">\(r \in \mathbb{Q}\)</span>, take an arbitrary <span class="math inline">\(x \in E[f&gt;r] \cap E[g&lt;r]\)</span>, <span class="math inline">\(f(x)&gt;g(x)\)</span>, i.e., <span class="math inline">\(x \in E[f&gt;g]\)</span>. Hence, <span class="math display">\[E[f&gt;g] \supset \bigcup_{r \in \mathbb{Q}} (E[f&gt;r] \cap E[g&lt;r]).\]</span> Therefore, <span class="math display">\[E[f&gt;g]=\bigcup_{r \in \mathbb{Q}} (E[f&gt;r] \cap E[g&lt;r]).\]</span> Since <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are measurable functions, then <span class="math inline">\(E[f&gt;r]\)</span> and <span class="math inline">\(E[g&lt;r]\)</span> are measurable, and thus <span class="math display">\[\bigcup_{r \in \mathbb{Q}} (E[f&gt;r] \cap E[g&lt;r])\]</span> is measurable. Therefore, <span class="math inline">\(E[f&gt;g]\)</span> is measurable.</p>
In addition, since <span class="math inline">\(E[f \geq g]=(E[g&gt;f])^c\)</span>, then <span class="math inline">\(E[f \geq g]\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.5</strong>    If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are measurable functions, then <span class="math inline">\(f \pm g\)</span> are measurable functions.</p>
<strong><em>Proof.</em></strong> We have <span class="math inline">\(E[f+g&gt;a]=E[f&gt;a-g]\)</span>. Since <span class="math inline">\(a-g\)</span> is a measurable function, then <span class="math inline">\(E[f&gt;a-g]\)</span> is measurable, i.e., <span class="math inline">\(E[f+g&gt;a]\)</span> is measurable. Similarly, <span class="math inline">\(E[f-g&gt;a]\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.6</strong>    If <span class="math inline">\(f\)</span> is a measurable function, then <span class="math inline">\(f^2\)</span> is a measurable function.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[E[f^2&gt;a]=\begin{cases}
E, &amp;a&lt;0 \\
E[f&gt;\sqrt{a}] \cup E[f&lt;-\sqrt{a}], &amp; a \geq 0
\end{cases}.\]</span> Therefore, <span class="math inline">\(f^2\)</span> is a measurable function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.7</strong>    If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are measurable functions, then <span class="math inline">\(f \cdot g\)</span> is a measurable function.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math display">\[f \cdot g=\frac{1}{4}((f+g)^2-(f-g)^2),\]</span> then it is obvious that <span class="math inline">\(f \cdot g\)</span> is a measurable function.</p>
<p><strong>Theorem 3.8</strong>    If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are measurable functions, then <span class="math inline">\(\displaystyle \frac{1}{g}\)</span> and <span class="math inline">\(\displaystyle \frac{f}{g}\)</span> are measurable functions.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[E\left[\frac{1}{g}&gt;a\right]=\begin{cases}
\displaystyle E[g&gt;0] \cap E\left[g&lt;\frac{1}{a}\right], &amp;a&gt;0 \\
E[g&gt;0] \cap E[g \neq \infty], &amp;a=0 \\
\displaystyle E[g&gt;0] \cup E\left[g&lt;\frac{1}{a}\right], &amp;a&lt;0
\end{cases},\]</span> i.e., <span class="math inline">\(\displaystyle E\left[\frac{1}{g}&gt;a\right]\)</span> is measurable, and thus <span class="math inline">\(\displaystyle \frac{1}{g}\)</span> and <span class="math inline">\(\displaystyle \frac{f}{g}\)</span> are measurable functions.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.9</strong>    If <span class="math inline">\(f\)</span> is a measurable function, then <span class="math inline">\(|f|\)</span> is a measurable function.</p>
<p><span id="thm3.10"><strong>Theorem 3.10</strong></span>    Suppose <span class="math inline">\(\{f_n\}\)</span> includes at most countable measurable functions on <span class="math inline">\(E\)</span>. Let <span class="math display">\[\mu=\inf_n f_n, \lambda=\sup_n f_n.\]</span> Then <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lambda\)</span> are measurable on <span class="math inline">\(E\)</span>.</p>
<strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, we have <span class="math display">\[E[\mu \geq a]=\bigcap_n E[f_n \geq a]\]</span> and <span class="math display">\[E[\lambda \leq a]=\bigcap_n E[f_n \leq a].\]</span> Therefore, <span class="math inline">\(\mu, \lambda\)</span> are measurable functions.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><font color="#696969" font size="2">关于<span class="math inline">\(\displaystyle E[\mu \geq a]=\bigcap_n E[f_n \geq a]\)</span>的证明如下.</font></p><p>任取<span class="math inline">\(x \in E[\mu \geq a]\)</span>，我们知道<span class="math inline">\(E[\mu \geq a]=\{x \in E: \inf_n f_n(x) \geq a\}\)</span>. 因此对于所有的<span class="math inline">\(n \in \mathbb{N}\)</span>，我们有<span class="math inline">\(f_n(x) \geq a\)</span>. 若不然，存在<span class="math inline">\(n \in \mathbb{N}\)</span>使得<span class="math inline">\(f_n(x)&lt;a\)</span>，与下确界定义矛盾. 因此<span class="math inline">\(\displaystyle x \in \bigcap_n E[f_n \geq a]\)</span>，也即<span class="math inline">\(\displaystyle E[\mu \geq a] \subset \bigcap_n E[f_n \geq a]\)</span>.</p><p>反之，任取<span class="math inline">\(\displaystyle x \in \bigcap_n E[f_n \geq a]\)</span>，则对于所有的<span class="math inline">\(n \in \mathbb{N}\)</span>，我们有<span class="math inline">\(f_n(x) \geq a\)</span>. 因此<span class="math inline">\(\inf_n f_n(x) \geq a\)</span>，即<span class="math inline">\(x \in E[\mu \geq a]\)</span>. 所以，<span class="math inline">\(\displaystyle E[\mu \geq a] \supset \bigcap_n E[f_n \geq a]\)</span>.</p><p>所以，<span class="math inline">\(\displaystyle E[\mu \geq a]=\bigcap_n E[f_n \geq a]\)</span>. 同理我们可证<span class="math inline">\(\displaystyle E[\lambda \leq a]=\bigcap_n E[f_n \leq a]\)</span>.</p>
          </div>
<div class="note warning">
            <p><font color="#696969" font size="2">连续函数不具备<a href="#thm3.10">定理3.10</a>的性质. 假设<span class="math display">\[f_n(x)=\begin{cases}0, &amp;x \leq 0 \\nx, &amp;\displaystyle 0&lt;x \leq \frac{1}{n} \\1, &amp;\displaystyle x&gt;\frac{1}{n}\end{cases}.\]</span> 显然，<span class="math inline">\(f_n\)</span>在<span class="math inline">\(\mathbb{R}\)</span>上是连续的. 不难发现，<span class="math display">\[\lambda(x)=\sup_n f_n(x)=\begin{cases}0, &amp;x \leq 0 \\1, &amp;x&gt;0\end{cases}\]</span>并非连续函数. 因此至多可数个连续函数的上下确界函数未必连续.</font></p>
          </div>
<p><strong>Theorem 3.11</strong>    Suppose <span class="math inline">\(\{f_n\}\)</span> includes at most countable measurable functions on <span class="math inline">\(E\)</span>. Then <span class="math inline">\(\displaystyle \varliminf_{n \to \infty} f_n\)</span> and <span class="math inline">\(\displaystyle \varlimsup_{n \to \infty} f_n\)</span> are measurable. Specifically, if <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n\)</span> exists, then <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n\)</span> is measurable.</p>
<p><strong>Theorem 3.12</strong>    We define <span class="math inline">\(f^+=\max\{f, 0\}\)</span> and <span class="math inline">\(f^-=-\min\{f, 0\}\)</span>. If <span class="math inline">\(f\)</span> is measurable, then <span class="math inline">\(f^+\)</span> and <span class="math inline">\(f^-\)</span> are measurable. Besides, <span class="math inline">\(f=f^+-f^-\)</span> and <span class="math inline">\(|f|=f^++f^-\)</span>.</p>
<h1 id="example">4. Example</h1>
<p><strong>Example 4.1</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span> is measurable, and <span class="math inline">\(f: E \to \mathbb{R}\)</span> is a constant function, then <span class="math inline">\(f\)</span> is a measurable function.</p>
<p><strong>Example 4.2</strong>    Suppose <span class="math inline">\(f: E \to \mathbb{R}\)</span> is a continuous function, then <span class="math inline">\(f\)</span> is a measurable function.</p>
<strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, we have <span class="math display">\[E[f&gt;a]=\{x \in E: f(x)&gt;a\}=\{x \in E: f^{-1}((a, \infty])=U \cap E,\]</span> where <span class="math inline">\(U \subset \mathbb{R}^n\)</span> is an open set. Therefore, <span class="math inline">\(E[f&gt;a]\)</span> is measurable, and thus <span class="math inline">\(f\)</span> is a measurable function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 4.3</strong>    Suppose <span class="math inline">\(f: [a, b] \to \mathbb{R}\)</span> is a monotonic function, then <span class="math inline">\(f\)</span> is a measurable function.</p>
<p><strong><em>Proof.</em></strong> Assume without loss of generality that <span class="math inline">\(f\)</span> is an increasing function.</p>
<p>For all <span class="math inline">\(c \in \mathbb{R}\)</span>, assume <span class="math inline">\(E[f \geq c] \neq \varnothing\)</span>. Let <span class="math inline">\(x_0=\inf E[f \geq c]\)</span>. Take an arbitrary <span class="math inline">\(x&gt;x_0\)</span>, if <span class="math inline">\(f(x)&lt;c\)</span>, then for all <span class="math inline">\(z \in E[f \geq c]\)</span>, <span class="math inline">\(z&gt;x\)</span>, i.e., <span class="math inline">\(x\)</span> is a lower bound, then <span class="math inline">\(x_0 \geq x\)</span>, which is a contradiction. Hence, <span class="math inline">\(f(x) \geq c\)</span>. Take an arbitrary <span class="math inline">\(y&lt;x_0\)</span>. Since <span class="math inline">\(x_0\)</span> is an infimum, then <span class="math inline">\(y \notin E[f \geq c]\)</span>.</p>
Therefore, when <span class="math inline">\(x_0 \in E[f \geq c]\)</span>, <span class="math inline">\(E[f \geq c]=[x_0, b]\)</span>; when <span class="math inline">\(x_0 \notin E[f \geq c]\)</span>, <span class="math inline">\(E[f \geq c]=(x_0, b]\)</span>. Therefore, <span class="math inline">\(E[f \geq c]\)</span> is measurable, and thus <span class="math inline">\(f\)</span> is a measurable function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 4.4</strong>    We divide <span class="math inline">\(E\)</span> into a finite number of disjoint measurable subsets, <span class="math inline">\(E_1, \ldots, E_s\)</span>. A <strong>simple function</strong> <span class="math inline">\(f: E \to \mathbb{R} \cup \{-\infty, \infty\}\)</span> is a constant function on <span class="math inline">\(E_i\)</span>. Then <span class="math inline">\(f\)</span> is a measurable function.</p>
<div class="note ">
            <p><strong>Lemma 4.4.1</strong>    Suppose <span class="math inline">\(f\)</span> is a measurable function on a measurable set <span class="math inline">\(E\)</span>, and <span class="math inline">\(E&#39;\)</span> is a measurable subset of <span class="math inline">\(E\)</span>, then <span class="math inline">\(f|_{E&#39;}\)</span> is measurable on <span class="math inline">\(E&#39;\)</span>.</p><strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, <span class="math inline">\(E&#39;[f&gt;a]=E&#39; \cap E[f&gt;a]\)</span>, which is measurable. Hence, <span class="math inline">\(f|_{E&#39;}\)</span> is measurable on <span class="math inline">\(E&#39;\)</span>.<p align="right"><span class="math inline">\(\square\)</span></p><p><strong>Lemma 4.4.2</strong>    Suppose <span class="math inline">\(f\)</span> is a function on <span class="math inline">\(\displaystyle E=\bigcup_{i=1}^s E_i\)</span>, and <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E_i\)</span>, then <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E\)</span>.</p><strong><em>Proof.</em></strong> For all <span class="math inline">\(a \in \mathbb{R}\)</span>, <span class="math display">\[E[f&gt;a]=\bigcup_{i=1}^s E_i[f&gt;a],\]</span> which is measurable, then <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E\)</span>.<p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<strong><em>Proof.</em></strong> By lemmas, it is easy to show that <span class="math inline">\(f\)</span> is a measurable function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 4.5</strong>    Dirichlet function on <span class="math inline">\([0, 1]\)</span> is a simple function.</p>
<h1 id="approximating-measurable-function-with-simple-functions">5. Approximating Measurable Function with Simple Functions</h1>
<p><span id="thm5.1"><strong>Theorem 5.1</strong></span>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span> is measurable. If <span class="math inline">\(f\)</span> is non-negative and measurable on <span class="math inline">\(E\)</span>, then there exists a sequence of non-negative and simple functions <span class="math inline">\(\{\varphi_k\}\)</span> such that for all <span class="math inline">\(x \in E\)</span>, <span class="math inline">\(\varphi_k(x) \leq \varphi_{k+1}(x)\)</span>, and <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k(x)=f(x)\)</span>.</p>
<p><strong><em>Proof.</em></strong> Let <span class="math display">\[E_{k, j}=E\left[\frac{j-1}{2^k} \leq f&lt;\frac{j}{2^k}\right], j=1, \ldots, k \cdot 2^k,\]</span> and <span class="math inline">\(E_k=E[f \geq k]\)</span>. Note that <span class="math inline">\(E_{k, j}\)</span> and <span class="math inline">\(E_k\)</span> are measurable.</p>
<p>We define <span class="math display">\[\varphi_k(x)=\begin{cases}
\displaystyle \frac{j-1}{2^k}, &amp;x \in E_{k, j} \\
k, &amp;x \in E_k
\end{cases},\]</span> which is a simple function and satisfies <span class="math inline">\(\varphi_k(x) \leq \varphi_{k+1}(x) \leq f(x)\)</span>.</p>
<p>When <span class="math inline">\(f(x) \neq \infty\)</span>, take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists a <span class="math inline">\(k\)</span> such that <span class="math inline">\(f(x)&lt;k\)</span> and <span class="math inline">\(\displaystyle \frac{1}{2^k}&lt;\varepsilon\)</span>. Hence, <span class="math inline">\(x\)</span> must lie in one of <span class="math inline">\(E_{k, j}\)</span>, and thus <span class="math display">\[0 \leq f(x)-\varphi_k(x) \leq \frac{1}{2^k}.\]</span> For all <span class="math inline">\(n \geq k\)</span>, <span class="math display">\[0 \leq f(x)-\varphi_n(x) \leq f(x)-\varphi_k(x) \leq \frac{1}{2^k}&lt;\varepsilon.\]</span> Therefore, <span class="math inline">\(|f(x)-\varphi_n(x)|&lt;\varepsilon\)</span>, i.e., <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k(x)=f(x)\)</span>.</p>
When <span class="math inline">\(f(x)=\infty\)</span>, for all <span class="math inline">\(k \in \mathbb{Z}^+\)</span>, <span class="math inline">\(\varphi_k(x)=k\)</span>. Therefore, <span class="math inline">\(\displaystyle \infty=f(x)=\lim_{k \to \infty} \varphi_k(x)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 5.2</strong>    If <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E\)</span>, then there exists a sequence of simple functions <span class="math inline">\(\{\varphi_k\}\)</span> such that for all <span class="math inline">\(x \in E\)</span>, <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k(x)=f(x)\)</span>. Moreover, if <span class="math inline">\(f\)</span> is bounded, then <span class="math inline">\(\{\varphi_k\}\)</span> is uniformly convergent.</p>
<p><strong><em>Proof.</em></strong> We can write <span class="math inline">\(f=f^+-f^-\)</span>. Because of <a href="#thm5.1">theorem 5.1</a>, there exist sequences of simple functions <span class="math inline">\(\{\varphi_k^+\}\)</span> and <span class="math inline">\(\{\varphi_k^-\}\)</span> such that <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k^+(x)=f^+(x)\)</span> and <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k^-(x)=f^-(x)\)</span>.</p>
<p>Since <span class="math inline">\(\varphi_k=\varphi_k^+-\varphi_k^-\)</span> is a simple function, then <span class="math inline">\(\displaystyle \lim_{k \to \infty} \varphi_k(x)=f(x)\)</span>.</p>
<p>When <span class="math inline">\(f\)</span> is bounded, there exists an <span class="math inline">\(M&gt;0\)</span> such that <span class="math inline">\(|f(x)| \leq M\)</span>. Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists a <span class="math inline">\(k&gt;M\)</span> such that <span class="math inline">\(\displaystyle \frac{1}{2^{k-1}}&lt;\varepsilon\)</span> and <span class="math inline">\(E_k=\varnothing\)</span>. For all <span class="math inline">\(x \in E\)</span>, <span class="math inline">\(x\)</span> must lie in one of <span class="math inline">\(E_{k, j}\)</span>.</p>
We have <span class="math display">\[0 \leq f^+(x)-\varphi_k^+(x) \leq \frac{1}{2^k}\]</span> and <span class="math display">\[0 \leq f^-(x)-\varphi_k^-(x) \leq \frac{1}{2^k}.\]</span> For all <span class="math inline">\(n \geq k\)</span>, <span class="math display">\[0 \leq f^+(x)-\varphi_n^+(x) \leq \frac{1}{2^k}\]</span> and <span class="math display">\[0 \leq f^-(x)-\varphi_n^-(x) \leq \frac{1}{2^k}.\]</span> Therefore, <span class="math display">\[|f(x)-\varphi_n(x)| \leq |f^+(x)-\varphi_n^+(x)|+|f^-(x)-\varphi_n^-(x)| \leq \frac{1}{2^{k-1}}&lt;\varepsilon.\]</span> Therefore, <span class="math inline">\(\{\varphi_k\}\)</span> is uniformly convergent.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><strong>Recall 5.1</strong>    For each <span class="math inline">\(k \in \mathbb{N}\)</span>, let <span class="math inline">\(f_k\)</span> be a function defined on <span class="math inline">\(E\)</span>. The sequence of functions <span class="math inline">\(\{f_k\}\)</span> <strong>converges pointwise</strong> on <span class="math inline">\(E\)</span> to a function <span class="math inline">\(f\)</span> defined on <span class="math inline">\(E\)</span> if <span class="math display">\[\forall x \in E, \forall \varepsilon&gt;0, \exists N \in \mathbb{N}\ \text{s.t.}\ \forall k \geq N, |f_k(x)-f(x)|&lt;\varepsilon.\]</span> Note that <span class="math inline">\(N(x, \varepsilon)\)</span> is related to <span class="math inline">\(X\)</span> and <span class="math inline">\(\varepsilon\)</span>, i.e., for different <span class="math inline">\(x\)</span>, the sequence of functions converges in different speeds.</p><p><strong>Recall 5.2</strong>    For each <span class="math inline">\(k \in \mathbb{N}\)</span>, let <span class="math inline">\(f_k\)</span> be a function defined on <span class="math inline">\(E\)</span>. The sequence of functions <span class="math inline">\(\{f_k\}\)</span> <strong>converges uniformly</strong> on <span class="math inline">\(E\)</span> to a function <span class="math inline">\(f\)</span> defined on <span class="math inline">\(E\)</span> if <span class="math display">\[\forall \varepsilon&gt;0, \exists N \in \mathbb{N}\ \text{s.t.}\ \forall x \in E, \forall k \geq N, |f_k(x)-f(x)|&lt;\varepsilon.\]</span> Note that <span class="math inline">\(N(\varepsilon)\)</span> is only related to <span class="math inline">\(\varepsilon\)</span>. An equivalent definition is <span class="math display">\[\lim_{k \to \infty} \sup_{x \in E} |f_k(x)-f(x)|=0.\]</span></p>
          </div>
<h1 id="convergence">6. Convergence</h1>
<h2 id="egorovs-theorem">6.1. Egorov's Theorem</h2>
<p><strong>Definition 6.1</strong>    If a property holds after removing a zero measure set, we say that the property holds <strong>almost everywhere (a.e.)</strong>.</p>
<p><strong>Example 6.1</strong>    Dirichlet function <span class="math inline">\(D(x)=0\ \text{a.e.}\ x \in [0, 1]\)</span>.</p>
<p><strong>Example 6.2</strong>    <span class="math inline">\(|\tan x|&lt;\infty\ \text{a.e.}\ x \in \mathbb{R}\)</span>.</p>
<p><strong>Example 6.3</strong>    If <span class="math inline">\(f(x)=g(x)\ \text{a.e.}\ x \in E\)</span>, <span class="math inline">\(g(x)=h(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math inline">\(f(x)=h(x)\ \text{a.e.}\ x \in E\)</span>.</p>
<p><strong>Theorem 6.1 (Egorov's Theorem)</strong>    Suppose <span class="math inline">\(f_k\)</span> and <span class="math inline">\(f\)</span> are measurable functions defined on <span class="math inline">\(E\)</span>, where <span class="math inline">\(f_k\)</span> and <span class="math inline">\(f\)</span> are finite almost everywhere, and <span class="math inline">\(m(E)&lt;\infty\)</span>. If <span class="math inline">\(f_k(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>, when <span class="math inline">\(k \to \infty\)</span>, then for all <span class="math inline">\(\delta&gt;0\)</span>, there exists measurable subset <span class="math inline">\(E_\delta \subset E\)</span>, where <span class="math inline">\(m(E_\delta) \leq \delta\)</span>, such that <span class="math inline">\(\{f_k\}\)</span> converges uniformly on <span class="math inline">\(E-E_\delta\)</span> to <span class="math inline">\(f\)</span>.</p>
<div class="note ">
            <p><span id="thm6.1.1"><strong>Lemma 6.1.1</strong></span>    Suppose <span class="math inline">\(f_k\)</span> and <span class="math inline">\(f\)</span> are measurable functions defined on <span class="math inline">\(E\)</span>, where <span class="math inline">\(f_k\)</span> and <span class="math inline">\(f\)</span> are finite almost everywhere, and <span class="math inline">\(m(E)&lt;\infty\)</span>. If <span class="math inline">\(f_k(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>, when <span class="math inline">\(k \to \infty\)</span>, then for all <span class="math inline">\(\varepsilon&gt;0\)</span>, we have <span class="math display">\[\lim_{N \to \infty} m\left(\bigcup_{k=N}^\infty E_k(\varepsilon)\right)=0,\]</span> where <span class="math inline">\(E_k(\varepsilon)=\{x \in E: |f_k(x)-f(x)| \geq \varepsilon\}\)</span>.</p><strong><em>Proof.</em></strong> Since <span class="math inline">\(f_k(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>, then the set of all non-convergent points has measure zero. We can write the set as <span class="math display">\[\begin{aligned}&amp;\ \ \ \ \ \{x \in E: \exists \varepsilon&gt;0\ \text{s.t.}\ \forall N \in \mathbb{N}, \exists k \geq N\ \text{s.t.}\ |f_k(x)-f(x)| \geq \varepsilon\}\\&amp;=\bigcup_{\varepsilon&gt;0}\bigcap_{N \geq 1}\bigcup_{k \geq N} \{x \in E: |f_k(x)-f(x)| \geq \varepsilon\}\\&amp;=\bigcup_{\varepsilon&gt;0}\bigcap_{N \geq 1}\bigcup_{k \geq N} E_k(\varepsilon)\\&amp;=\bigcup_{\varepsilon&gt;0}\lim_{N \to \infty}\bigcup_{k \geq N} E_k(\varepsilon).\end{aligned}\]</span> Therefore, all points in <span class="math inline">\(\displaystyle \bigcap_{N \geq 1}\bigcup_{k \geq N} E_k(\varepsilon)\)</span> are not convergent, for any <span class="math inline">\(\varepsilon&gt;0\)</span>, and thus <span class="math display">\[m\left(\lim_{N \to \infty}\bigcup_{k \geq N} E_k(\varepsilon)\right)=0.\]</span><p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(\displaystyle \bigcup_{k \geq N} E_k(\varepsilon)\)</span> is decreasing and <span class="math inline">\(m(E)&lt;\infty\)</span>, then we have <span class="math display">\[\lim_{N \to \infty} m\left(\bigcup_{k=N}^\infty E_k(\varepsilon)\right)=m\left(\lim_{N \to \infty}\bigcup_{k=N}^\infty E_k(\varepsilon)\right)=0.\]</span></p>
<p>Assume without loss of generality that <span class="math inline">\(f_k\)</span> and <span class="math inline">\(f\)</span> are finite on <span class="math inline">\(E\)</span>.</p>
<p>By <a href="#thm6.1.1">lemma 6.1.1</a>, we know <span class="math display">\[\forall \varepsilon&gt;0, \forall \widetilde{\varepsilon}&gt;0, \exists N(\varepsilon, \widetilde{\varepsilon}) \in \mathbb{N}\ \text{s.t.}\ \forall N \geq N(\varepsilon, \widetilde{\varepsilon}), m\left(\bigcup_{k=N}^\infty E_k(\varepsilon)\right)&lt;\widetilde{\varepsilon}.\]</span></p>
<p>Let <span class="math inline">\(\displaystyle \varepsilon=\frac{1}{i}\)</span> and <span class="math inline">\(\displaystyle \widetilde{\varepsilon}=\frac{\delta}{2^i}\)</span>, <span class="math inline">\(i \in \mathbb{N}\)</span>. Therefore, <span class="math display">\[\forall \delta&gt;0, \forall i \in \mathbb{N}, \exists N(\delta, i) \in \mathbb{N}\ \text{s.t.}\ m\left(\bigcup_{k \geq N(\delta, i)} E_k\left(\frac{1}{i}\right)\right)&lt;\frac{\delta}{2^i}.\]</span> Take <span class="math inline">\(\displaystyle E_\delta=\bigcup_{i \geq 1}\bigcup_{k \geq N(\delta, i)}E_k\left(\frac{1}{i}\right)\)</span>, then we have <span class="math display">\[m(E_\delta) \leq \sum_{i \geq 1} m\left(\bigcup_{k \geq N(\delta, i)} E_k\left(\frac{1}{i}\right)\right)=\delta.\]</span> We also have <span class="math display">\[E-E_\delta=\bigcap_{i \geq 1}\bigcap_{k \geq N(\delta, i)} E_k\left(\frac{1}{i}\right)^c.\]</span></p>
Note that for all <span class="math inline">\(\varepsilon&gt;0\)</span>, we can find an <span class="math inline">\(i\)</span> such that <span class="math inline">\(\displaystyle \frac{1}{i}&lt;\varepsilon\)</span>. Therefore, <span class="math display">\[\forall \delta&gt;0, \forall \varepsilon&gt;0, \exists N(\delta, i) \in \mathbb{N}\ \text{s.t.}\ \forall x \in E-E_\delta, \forall k \geq N(\delta, i), |f_k(x)-f(x)|&lt;\frac{1}{i}&lt;\varepsilon.\]</span> Hence, <span class="math inline">\(\{f_k\}\)</span> converges uniformly on <span class="math inline">\(E-E_\delta\)</span> to <span class="math inline">\(f\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><font color="#696969" font size="2">假设<span class="math inline">\(f_k\)</span>在<span class="math inline">\(E-N_k\)</span>上是有限函数，其中<span class="math inline">\(N_k\)</span>为零测集；<span class="math inline">\(f\)</span>在<span class="math inline">\(E-N\)</span>上是有限函数，其中<span class="math inline">\(N\)</span>为零测集. 因此<span class="math inline">\(\{f_k\}\)</span>和<span class="math inline">\(f\)</span>在<span class="math inline">\(\displaystyle E-\left(\bigcup_{k=1}^\infty N_k \cup N\right)\)</span>上是有限的，其中<span class="math inline">\(\displaystyle \left(\bigcup_{k=1}^\infty N_k \cup N\right)\)</span>是零测集.</font></p><p>如果我们能在<span class="math inline">\(\displaystyle E-\left(\bigcup_{k=1}^\infty N_k \cup N\right)\)</span>上找到<span class="math inline">\(E_\delta\)</span>使命题成立，那么自然在<span class="math inline">\(\displaystyle E_\delta \cup \left(\bigcup_{k=1}^\infty N_k \cup N\right)\)</span>上我们可以找到<span class="math inline">\(E_\delta\)</span>使命题成立. 因此，在证明过程中，我们不妨假设<span class="math inline">\(f_k\)</span>和<span class="math inline">\(f\)</span>在<span class="math inline">\(E\)</span>上为有限函数.</p>
          </div>
<div class="note warning">
            <p>If <span class="math inline">\(m(E)=\infty\)</span>, then the Egorov's theorem does not hold.</p>
          </div>
<h2 id="convergence-in-measure">6.2. Convergence in Measure</h2>
<p><strong>Definition 6.2</strong>    Suppose <span class="math inline">\(f_n\)</span> and <span class="math inline">\(f\)</span> are measurable functions defined on <span class="math inline">\(E\)</span>, where <span class="math inline">\(f_n\)</span> and <span class="math inline">\(f\)</span> are finite almost everywhere. If <span class="math display">\[\forall \sigma&gt;0, \lim_{n \to \infty} m(E[|f-f_n| \geq \sigma])=0,\]</span> then <span class="math inline">\(\{f_n\}\)</span> <strong>converges in measure</strong> to <span class="math inline">\(f\)</span>, denoted as <span class="math inline">\(f_n \Rightarrow f\)</span>.</p>
<p><strong>Theorem 6.2</strong>    If <span class="math inline">\(f_n \Rightarrow f\)</span>, <span class="math inline">\(f_n \Rightarrow g\)</span>, then <span class="math inline">\(f(x)=g(x)\ \text{a.e.}\ x \in E\)</span>.</p>
<p><strong><em>Proof.</em></strong> We have <span class="math inline">\(|f(x)-g(x)| \leq |f(x)-f_k(x)|+|g(x)-f_k(x)|\)</span>. If <span class="math inline">\(\displaystyle |f-f_k|&lt;\frac{1}{2n}\)</span> and <span class="math inline">\(\displaystyle |g-f_k|&lt;\frac{1}{2n}\)</span>, then <span class="math inline">\(\displaystyle |f-g|&lt;\frac{1}{n}\)</span>, i.e., <span class="math display">\[E\left[|f-g|&lt;\frac{1}{n}\right] \supset E\left[|f-f_k|&lt;\frac{1}{2n}\right] \cap E\left[|g-f_k|&lt;\frac{1}{2n}\right],\]</span> or equivalently, <span class="math display">\[E\left[|f-g| \geq \frac{1}{n}\right] \subset E\left[|f-f_k| \geq \frac{1}{2n}\right] \cup E\left[|g-f_k| \geq \frac{1}{2n}\right].\]</span> Therefore, <span class="math display">\[m\left(E\left[|f-g| \geq \frac{1}{n}\right]\right) \leq m\left(E\left[|f-f_k| \geq \frac{1}{2n}\right]\right)+m\left(E\left[|g-f_k| \geq \frac{1}{2n}\right]\right).\]</span> Let <span class="math inline">\(k \to \infty\)</span>, then <span class="math inline">\(\displaystyle m\left(E\left[|f-g| \geq \frac{1}{n}\right]\right)=0\)</span>.</p>
Since <span class="math inline">\(\displaystyle E[f \neq g]=\bigcup_{n=1}^\infty E\left[|f-g| \geq \frac{1}{n}\right]\)</span>, then <span class="math inline">\(m(E[f \neq g])=0\)</span>, i.e., <span class="math inline">\(f(x)=g(x)\ \text{a.e.}\ x \in E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>Convergence in measure cannot derive almost everywhere convergence; almost everywhere convergence cannot derive convergence in measure.</p>
          </div>
<p><strong>Theorem 6.3 (Lebesgue's Theorem)</strong>    Suppose <span class="math inline">\(f_n\)</span> and <span class="math inline">\(f\)</span> are measurable functions defined on <span class="math inline">\(E\)</span>, where <span class="math inline">\(f_n\)</span> and <span class="math inline">\(f\)</span> are finite almost everywhere, and <span class="math inline">\(m(E)&lt;\infty\)</span>. If <span class="math inline">\(f_n(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>, when <span class="math inline">\(n \to \infty\)</span>, then <span class="math inline">\(f_n \Rightarrow f\)</span>.</p>
<strong><em>Proof.</em></strong> By <a href="#thm6.1.1">lemma 6.1.1</a>, we know <span class="math display">\[\lim_{n \to \infty} m\left(\bigcup_{k \geq n} E[|f_k-f| \geq \varepsilon]\right)=0.\]</span> Since <span class="math inline">\(\displaystyle E[|f_n-f| \geq \varepsilon] \subset \bigcup_{k \geq n} E[|f_k-f| \geq \varepsilon]\)</span>, then <span class="math display">\[m(E[|f_n-f| \geq \varepsilon]) \leq m\left(\bigcup_{k \geq n} E[|f_k-f| \geq \varepsilon]\right).\]</span> Take <span class="math inline">\(n \to \infty\)</span>, then <span class="math display">\[\lim_{n \to \infty} m(E[|f_n-f] \geq \varepsilon])=0,\]</span> i.e., <span class="math inline">\(f_n \Rightarrow f\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 6.4 (Riesz's Theorem)</strong>    Suppose <span class="math inline">\(\{f_n\}\)</span> converges in measure to <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span>, then there exists a subsequence <span class="math inline">\(\{f_{n_s}\}\)</span> such that <span class="math inline">\(f_{n_s}(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(f_n \Rightarrow f\)</span>, then <span class="math display">\[\exists N \geq 1\ \text{s.t.}\ \forall n_s \geq n \geq N, m\left(E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\right)&lt;\frac{1}{2^s}.\]</span></p>
<p>Take an arbitrary <span class="math inline">\(\displaystyle x \in \bigcup_{N \geq 1}\bigcap_{s \geq N} E\left[|f_{n_s}-f|&lt;\frac{1}{s}\right]\)</span>, then <span class="math display">\[\exists N \geq 1\ \text{s.t.}\ \forall s \geq N, |f_{n_s}(x)-f(x)|&lt;\frac{1}{s}.\]</span> Since for any <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an <span class="math inline">\(N \geq 1\)</span> such that <span class="math inline">\(\displaystyle \varepsilon&gt;\frac{1}{N}\)</span>. Hence, <span class="math display">\[\forall \varepsilon&gt;0, \exists N \geq 1\ \text{s.t.}\ \forall s \geq N, |f_{n_s}(x)-f(x)|&lt;\frac{1}{s} \leq \frac{1}{N}&lt;\varepsilon,\]</span> i.e., <span class="math inline">\(f_{n_s}(x) \to f(x)\)</span> as <span class="math inline">\(s \to \infty\)</span> when <span class="math inline">\(\displaystyle x \in \bigcup_{N \geq 1}\bigcap_{s \geq N} E\left[|f_{n_s}-f|&lt;\frac{1}{s}\right]\)</span>.</p>
Since for all <span class="math inline">\(n \geq 1\)</span>, <span class="math inline">\(\displaystyle \bigcap_{N \geq 1}\bigcup_{s \geq N} E\left[|f_{n_s}-f| \geq \frac{1}{s}\right] \subset \bigcup_{s \geq n} E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\)</span>, then <span class="math display">\[\begin{aligned}
m\left(\bigcap_{N \geq 1}\bigcup_{s \geq N} E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\right)&amp;\leq m\left(\bigcup_{s \geq n} E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\right)
\\&amp;\leq \sum_{s \geq n} m\left(E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\right)
\\&amp;&lt; \sum_{s \geq n} \frac{1}{2^s}=\frac{1}{2^{n-1}}.
\end{aligned}\]</span> Hence, <span class="math display">\[m\left(\bigcap_{N \geq 1}\bigcup_{s \geq N} E\left[|f_{n_s}-f| \geq \frac{1}{s}\right]\right)=0.\]</span> Therefore, <span class="math inline">\(f_{n_s}(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="measurable-function-and-continuous-function">7. Measurable Function and Continuous Function</h1>
<div class="note info">
            <p><font color="#696969" font size="2">从拓扑的角度看，一个函数是可测的如果这一函数能使Borel集的原像是可测集，一个函数是连续的如果这一函数能使开集的原像是开集. 因此不难看到，对于一个定义在可测集上的连续函数一定是可测的. 然而，一个可测函数不一定是连续函数. 因此，我们想探究这两类函数之间的区别.</font></p>
          </div>
<p><strong>Theorem 7.1 (Lusin's Theorem)</strong>    Suppose <span class="math inline">\(f\)</span> is a measurable function defined on <span class="math inline">\(E\)</span>, where <span class="math inline">\(f\)</span> is finite almost everywhere, then for all <span class="math inline">\(\delta&gt;0\)</span>, there exists a closed set <span class="math inline">\(F_\delta \subset E\)</span> such that <span class="math inline">\(m(E-F_\delta)&lt;\delta\)</span>, and <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>.</p>
<p><strong><em>Proof.</em></strong> First, we consider simple function. Suppose <span class="math inline">\(\displaystyle E=\bigcup_{i=1}^s E_i\)</span>, where <span class="math inline">\(E_i\)</span> is measurable, and <span class="math inline">\(f(x)=c_i\)</span>, for all <span class="math inline">\(x \in E_i\)</span>. For all <span class="math inline">\(\delta&gt;0\)</span>, there exists a closed set <span class="math inline">\(F_i \subset E_i\)</span> such that <span class="math inline">\(\displaystyle m(E-F_i)&lt;\frac{\delta}{s}\)</span>. Let <span class="math inline">\(\displaystyle F_\delta=\bigcup_{i=1}^s F_i\)</span>. We know <span class="math inline">\(F_\delta\)</span> is closed and <span class="math inline">\(\displaystyle m(E-F_\delta)&lt;\delta\)</span>. Consider <span class="math inline">\(f\)</span> defined on <span class="math inline">\(F_\delta\)</span>. For any closed set <span class="math inline">\(F\)</span> on image, <span class="math display">\[f^{-1}(F)=f^{-1}(F) \cap F_\delta=f^{-1}(F) \cap \bigcup_{i=1}^s F_i=\bigcup_{i=1}^s (f^{-1}(F) \cap F_i),\]</span> which is closed. Therefore, <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>.</p>
<p>Then we consider bounded and measurable function. There exists a sequence of simple functions <span class="math inline">\(\{\varphi_k\}\)</span> such that <span class="math inline">\(\{\varphi_k\}\)</span> is uniformly convergent. For all <span class="math inline">\(\delta&gt;0\)</span>, there exists a closed set <span class="math inline">\(F_k \subset E\)</span> such that <span class="math inline">\(\displaystyle m(E-F_k)&lt;\frac{\delta}{2^k}\)</span> and <span class="math inline">\(\varphi_k\)</span> is continuous on <span class="math inline">\(F_k\)</span>. Let <span class="math inline">\(\displaystyle F_\delta=\bigcap_{k=1}^\infty F_k\)</span>, then <span class="math inline">\(\varphi_k\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>. We know <span class="math display">\[\begin{aligned}
m(E-F_\delta)&amp;=m\left(E-\bigcap_{k=1}^\infty F_k\right)
\\&amp;=m\left(\bigcup_{k=1}^\infty (E-F_k)\right)
\\&amp;\leq \sum_{k=1}^\infty m(E-F_k)&lt;\delta.
\end{aligned}\]</span> Since <span class="math inline">\(\varphi_k\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>, and <span class="math inline">\(\{\varphi_k\}\)</span> is uniformly convergent to <span class="math inline">\(f\)</span>, then <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>.</p>
Finally, we consider measurable function. Since <span class="math inline">\(m(\{x: |f(x)|=\infty\})=0\)</span>, then we can assume without loss of generality that <span class="math inline">\(f\)</span> is finite on <span class="math inline">\(E\)</span>. Let <span class="math display">\[g(x)=\frac{f(x)}{1+|f(x)|}, x \in E.\]</span> We know <span class="math inline">\(g\)</span> is bounded and measurable on <span class="math inline">\(E\)</span>. Therefore, there exists a closed set <span class="math inline">\(F_\delta \subset E\)</span> such that <span class="math inline">\(m(E-F_\delta)&lt;\delta\)</span> and <span class="math inline">\(g\)</span> is continuous on <span class="math inline">\(F_\delta\)</span>. Hence, <span class="math display">\[f(x)=\frac{g(x)}{1-|g(x)|}, |g(x)|&lt;1\]</span> is continuous on <span class="math inline">\(E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Function</tag>
        <tag>Measure</tag>
      </tags>
  </entry>
  <entry>
    <title>Differentiation and Indefinite Integral</title>
    <url>/Mathematics/Real-analysis/Differentiation_and_Indefinite_Integral.html</url>
    <content><![CDATA[<h1 id="differentiability-of-monotonic-function">1. Differentiability of Monotonic Function</h1>
<p><strong>Definition 1.1</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}\)</span>, <span class="math inline">\(\mathcal{V}=\{I_\alpha\}\)</span> is a family of intervals with all positive lengths. If <span class="math display">\[\forall x \in E, \forall \varepsilon&gt;0, \exists I_\alpha \in \mathcal{V}\ \text{s.t.}\ x \in I_\alpha, m(I_\alpha)&lt;\varepsilon,\]</span> then <span class="math inline">\(\mathcal{V}\)</span> is a <strong>Vitali cover</strong> of <span class="math inline">\(E\)</span>.</p>
<p><strong>Theorem 1.1</strong>    <span class="math inline">\(\mathcal{V}\)</span> is a Vitali cover if and only if <span class="math display">\[\forall x \in E, \exists \{I_n\} \subset \mathcal{V}\ \text{s.t.}\ x \in I_n, m(I_n) \to 0.\]</span></p>
<p><span id="thm1.2"><strong>Theorem 1.2 (Vitali Covering Theorem)</strong></span>    Suppose <span class="math inline">\(E \subset \mathbb{R}\)</span> and <span class="math inline">\(m^*(E)&lt;\infty\)</span>. If <span class="math inline">\(\mathcal{V}\)</span> is a Vitali cover of <span class="math inline">\(E\)</span>, then there exists a sequence of intervals <span class="math inline">\(\{I_n\} \subset \mathcal{V}\)</span> such that <span class="math inline">\(I_i \cap I_j=\varnothing\)</span> for all <span class="math inline">\(i \neq j\)</span>, and <span class="math display">\[m^*\left(E-\bigcup_{i=1}^\infty I_i\right)=0.\]</span></p>
<div class="note ">
            <p><span id="thm1.2.1"><strong>Lemma 1.2.1</strong></span>    For <span class="math inline">\(x \in E \cap U\)</span>, where <span class="math inline">\(U\)</span> is open, and a Vitali cover <span class="math inline">\(\mathcal{V}\)</span> of <span class="math inline">\(E\)</span>, there exists an <span class="math inline">\(I_x \in \mathcal{V}\)</span> such that <span class="math inline">\(x \in I_x\)</span> and <span class="math inline">\(I_x \subset U\)</span>.</p><strong><em>Proof.</em></strong> Since <span class="math inline">\(x \in U\)</span>, where <span class="math inline">\(U\)</span> is open, then <span class="math display">\[\exists \varepsilon&gt;0\ \text{s.t.}\ (x-\varepsilon, x+\varepsilon) \subset U.\]</span> Since <span class="math inline">\(\mathcal{V}\)</span> is a Vitali cover of <span class="math inline">\(E\)</span>, then <span class="math display">\[\exists I_x \in \mathcal{V}\ \text{s.t.}\ x \in I_x, m(I_x)&lt;\frac{\varepsilon}{4}.\]</span> Hence, <span class="math inline">\(I_x \subset (x-\varepsilon, x+\varepsilon) \subset U\)</span>.<p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong><em>Proof.</em></strong> Since <span class="math display">\[m^*\left(E-\bigcup_{i=1}^\infty I_i\right)=m^*\left(E-\bigcup_{i=1}^\infty \overline{I_i}\right),\]</span> then we can assume without loss of generality that <span class="math inline">\(I_i\)</span> is closed for all <span class="math inline">\(i\)</span>.</p>
<p>In addition, since <span class="math inline">\(m^*(E)&lt;\infty\)</span>, then there exists an open set <span class="math inline">\(U \supset E\)</span> such that <span class="math inline">\(m(U)&lt;\infty\)</span>. Let <span class="math display">\[\mathcal{W}=\{I \in \mathcal{V}: I \subset U\}.\]</span> By <a href="#thm1.2.1">lemma 1.2.1</a>, for all <span class="math inline">\(x \in E\)</span>, there exists an <span class="math inline">\(I_x\)</span> such that <span class="math inline">\(x \in I_x \subset U\)</span>, and thus <span class="math inline">\(I_x \in \mathcal{W}\)</span>, i.e., <span class="math inline">\(\mathcal{W} \neq \varnothing\)</span>. Take an arbitrary <span class="math inline">\(\delta&gt;0\)</span>, <span class="math display">\[\exists \widetilde{I}_x \in \mathcal{V}\ \text{s.t.}\ x \in \widetilde{I}_x, m(\widetilde{I}_x)&lt;\min\left\{\frac{\varepsilon}{4}, \delta\right\}.\]</span> Hence, <span class="math inline">\(\widetilde{I}_x \in \mathcal{W}\)</span>, and thus <span class="math inline">\(\mathcal{W}\)</span> is a Vitali cover of <span class="math inline">\(E\)</span>. Therefore, we can assume without loss of generality that the intervals in <span class="math inline">\(\mathcal{V}\)</span> are all contained in an open set <span class="math inline">\(U\)</span> with a finite measure.</p>
<p>Take an arbitrary <span class="math inline">\(I_1 \in \mathcal{V}\)</span>. If <span class="math inline">\(E-I_1=\varnothing\)</span>, then the theorem holds. If <span class="math inline">\(E-I_1 \neq \varnothing\)</span>, then for <span class="math inline">\(x \in E-I_1\)</span>, <span class="math inline">\(x \in I_1^c\)</span>, where <span class="math inline">\(I_1^c\)</span> is open. By <a href="#thm1.2.1">lemma 1.2.1</a>, there exists an <span class="math inline">\(I_x \in \mathcal{V}\)</span> such that <span class="math inline">\(x \in I_x\)</span> and <span class="math inline">\(I_x \subset I_1^c\)</span>, i.e., <span class="math inline">\(I_x \cap I_1=\varnothing\)</span>. Hence, <span class="math inline">\(\{I \in \mathcal{V}: I \cap I_1=\varnothing\} \neq \varnothing\)</span>. Let <span class="math inline">\(r_1=\sup\{m(I): I \in \mathcal{V}, I \cap I_1=\varnothing\} \leq m(U)&lt;\infty\)</span>. We now can find <span class="math inline">\(I_2 \in \mathcal{V}\)</span>, <span class="math inline">\(I_2 \cap I_1=\varnothing\)</span> such that <span class="math inline">\(\displaystyle m(I_2)&gt;\frac{r_1}{2}\)</span>. Similarly, we can find <span class="math inline">\(r_3, r_4, \ldots\)</span>. Therefore, we can find a sequence of intervals <span class="math inline">\(\{I_n\} \subset \mathcal{V}\)</span> such that <span class="math inline">\(I_i \cap I_j=\varnothing\)</span>, where <span class="math display">\[r_n=\sup\{m(I): I \in \mathcal{V}, I \cap I_1=\varnothing, \ldots, I \cap I_n=\varnothing\}\]</span> and <span class="math display">\[m(I_{n+1})&gt;\frac{r_n}{2}.\]</span></p>
<p>Since <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty I_i \subset U\)</span>, then <span class="math display">\[\sum_{i=1}^\infty m(I_i)=m\left(\bigcup_{i=1}^\infty I_i\right) \leq m(U)&lt;\infty.\]</span> Hence, <span class="math inline">\(m(I_i) \to 0\)</span> as <span class="math inline">\(i \to \infty\)</span>, and <span class="math inline">\(\displaystyle \sum_{i=k+1}^\infty m(I_i) \to 0\)</span> as <span class="math inline">\(k \to \infty\)</span>.</p>
<p>Take an arbitrary <span class="math inline">\(\displaystyle y \in E-\bigcup_{i=1}^\infty I_i\)</span>, and fix a <span class="math inline">\(k \in \mathbb{N}\)</span> such that <span class="math inline">\(\displaystyle y \notin \bigcap_{i=1}^k I_i\)</span>, i.e., <span class="math inline">\(\displaystyle y \in \left(\bigcup_{i=1}^k I_i\right)^c\)</span>. By <a href="#thm1.2.1">lemma 1.2.1</a>, there exists an <span class="math inline">\(I_y \in \mathcal{V}\)</span> such that <span class="math inline">\(y \in I_y\)</span> and <span class="math inline">\(\displaystyle I_y \subset \left(\bigcup_{i=1}^k I_i\right)^c\)</span>.</p>
<p>We can find a <span class="math inline">\(n&gt;k\)</span> such that <span class="math inline">\(I_y \cap I_n \neq \varnothing\)</span>; otherwise, <span class="math inline">\(m(I_y) \leq r_n \to 0\)</span>, i.e., <span class="math inline">\(m(I_y)=0\)</span>, which is a contradiction. Take an <span class="math inline">\(n(y) \in \mathbb{N}\)</span> such that <span class="math inline">\(I_y \cap I_{n(y)} \neq \varnothing\)</span> and <span class="math inline">\(I_y \cap I_{n(y)-1}=\varnothing, \ldots, I_y \cap I_1=\varnothing\)</span>. Note that <span class="math inline">\(n(y)&gt;k\)</span>. Let <span class="math inline">\(x_{n(y)}\)</span> be the midpoint of <span class="math inline">\(I_{n(y)}\)</span>, then <span class="math display">\[\begin{aligned}
|y-x_{n(y)}| &amp;\leq m(I_y)+\frac{1}{2}m(I_{n(y)})
\\&amp;\leq r_{n(y)-1}+\frac{1}{2}m(I_{n(y)})
\\&amp;&lt;2m(I_{n(y)})+\frac{1}{2}m(I_{n(y)})
\\&amp;=\frac{5}{2}m(I_{n(y)}).
\end{aligned}\]</span></p>
<p>Let <span class="math inline">\(J_n\)</span> be a closed interval with the midpoint of <span class="math inline">\(I_n\)</span> as the midpoint, and a length that becomes five times that of <span class="math inline">\(I_n\)</span>, then <span class="math inline">\(y \in J_{n(y)}\)</span>. Since <span class="math inline">\(n(y)&gt;k\)</span>, then <span class="math inline">\(\displaystyle y \in \bigcup_{i=k+1}^\infty J_i\)</span>. Hence, <span class="math inline">\(\displaystyle E-\bigcup_{i=1}^\infty I_i \subset \bigcup_{i=k+1}^\infty J_i\)</span>.</p>
Therefore, <span class="math display">\[m^*\left(E-\bigcup_{i=1}^\infty I_i\right) \leq m\left(\bigcup_{i=k+1}^\infty J_i\right) \leq \sum_{i=k+1}^\infty m(J_i)=5\sum_{i=k+1}^\infty m(I_i).\]</span> Since <span class="math inline">\(\displaystyle \sum_{i=k+1}^\infty m(I_i) \to 0\)</span> as <span class="math inline">\(k \to \infty\)</span>, then <span class="math display">\[m^*\left(E-\bigcup_{i=1}^\infty I_i\right)=0.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 1.3</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}\)</span> and <span class="math inline">\(m^*(E)&lt;\infty\)</span>. If <span class="math inline">\(\mathcal{V}\)</span> is a Vitali cover of <span class="math inline">\(E\)</span>, then for all <span class="math inline">\(\varepsilon&gt;0\)</span>, there exist finite disjoint intervals <span class="math inline">\(\{I_i\} \subset \mathcal{V}\)</span> such that <span class="math display">\[m^*\left(E-\bigcup_{i=1}^n I_i\right)&lt;\varepsilon.\]</span></p>
<p><strong>Definition 1.2</strong>    The <strong>upper right-hand derivative</strong>, <strong>lower right-hand derivative</strong>, <strong>upper left-hand derivative</strong>, and <strong>lower left-hand derivative</strong> (collectively called <strong>Dini derivative</strong>) are defined as <span class="math display">\[\begin{aligned}
D^+f(x_0)&amp;=\varlimsup_{h \to 0^+} \frac{f(x_0+h)-f(x_0)}{h},
\\D_+f(x_0)&amp;=\varliminf_{h \to 0^+} \frac{f(x_0+h)-f(x_0)}{h},
\\D^-f(x_0)&amp;=\varlimsup_{h \to 0^-} \frac{f(x_0+h)-f(x_0)}{h},
\end{aligned}\]</span> and <span class="math display">\[D_-f(x_0)=\varliminf_{h \to 0^-} \frac{f(x_0+h)-f(x_0)}{h}.\]</span> Note that <span class="math inline">\(\displaystyle \lim_{h \to 0} \frac{f(x_0+h)-f(x_0)}{h}\)</span> exists if and only if <span class="math inline">\(D^+f(x_0)=D_+f(x_0)=D^-f(x_0)=D_-f(x_0)\)</span>. Moreover, <span class="math inline">\(f\)</span> is <strong>differentiable</strong> at <span class="math inline">\(x_0\)</span> if and only if <span class="math display">\[-\infty&lt;f&#39;(x_0)=\lim_{h \to 0} \frac{f(x_0+h)-f(x_0)}{h}=D^+f(x_0)=D_+f(x_0)=D^-f(x_0)=D_-f(x_0)&lt;\infty.\]</span></p>
<p><strong>Definition 1.3</strong>    Let <span class="math display">\[Df(x_0)=\lim_{n \to \infty} \frac{f(x_0+h_n)-f(x_0)}{h_n},\]</span> where <span class="math inline">\(h_n \to 0\)</span> and <span class="math inline">\(Df(x_0)\)</span> exists.</p>
<p><span id="thm1.4"><strong>Theorem 1.4</strong></span>    Suppose <span class="math inline">\(f\)</span> is a strictly increasing function on <span class="math inline">\([a, b]\)</span>, and <span class="math inline">\(E \subset [a, b]\)</span>. If for all <span class="math inline">\(x \in E\)</span>, there exists a sequence <span class="math inline">\(\{h_n\}\)</span>, where <span class="math inline">\(h_n \to 0\)</span> such that <span class="math inline">\(p \geq Df(x) \geq q\)</span>, where <span class="math inline">\(p, q \geq 0\)</span>, then <span class="math display">\[p \cdot m^*(E) \geq m^*(f(E)) \geq q \cdot m^*(E).\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an open set <span class="math inline">\(G \supset E\)</span> such that <span class="math inline">\(m^*(E)&gt;m^*(G)-\varepsilon\)</span>. Let <span class="math inline">\(p_0&gt;p\)</span>, for <span class="math inline">\(x_0 \in E\)</span>, there exists a sequence <span class="math inline">\(\{h_n\}\)</span>, where <span class="math inline">\(h_n \to 0\)</span>, such that <span class="math display">\[\lim_{n \to \infty} \frac{f(x_0+h_n)-f(x_0)}{h_n}&lt;p_0.\]</span></p>
<p>Let <span class="math display">\[I_n(x_0)=\begin{cases}
[x_0, x_0+h_n], &amp;h_n&gt;0 \\
[x_0+h_n, x_0], &amp;h_n&lt;0
\end{cases},\]</span> and <span class="math display">\[\Delta_n(x_0)=\begin{cases}
[f(x_0), f(x_0+h_n)], &amp;h_n&gt;0 \\
[f(x_0+h_n), f(x_0)], &amp;h_n&lt;0
\end{cases}.\]</span> Since <span class="math inline">\(f\)</span> is a strictly increasing function, then <span class="math inline">\(f(I_n(x_0)) \subset \Delta_n(x_0)\)</span>.</p>
<p>When <span class="math inline">\(n\)</span> is large enough, <span class="math inline">\(I_n(x_0) \subset G\)</span>, and <span class="math display">\[\frac{f(x_0+h_n)-f(x_0)}{h_n}&lt;p_0.\]</span> By inequality above, <span class="math display">\[m^*(f(I_n(x_0))) \leq m^*(\Delta_n(x_0))&lt;p_0 \cdot m(I_n(x_0)).\]</span></p>
<p>Since <span class="math inline">\(m^*(I_n(x_0)) \to 0\)</span>, then <span class="math inline">\(m^*(\Delta_n(x_0)) \to 0\)</span>, i.e., <span class="math inline">\(\{\Delta_n(x_0)\}\)</span> is a Vitali cover of <span class="math inline">\(f(E)\)</span>. By <a href="#thm1.2">Vitali covering theorem</a>, there exist countable disjoint intervals <span class="math inline">\(\{\Delta_{n_j}(x_j)\}\)</span> such that <span class="math display">\[m\left(f(E)-\bigcup_{j=1}^\infty \Delta_{n_j}(x_j)\right)=0.\]</span> Besides, <span class="math inline">\(\{I_{n_j}(x_j)\}\)</span> are disjoint. Hence, <span class="math display">\[m^*(f(E)) \leq \sum_{j=1}^\infty m^*(\Delta_{n_j}(x_j))&lt;p_0\sum_{j=1}^\infty m^*(I_{n_j}(x_j))=p_0 \cdot m^*\left(\bigcup_{j=1}^\infty I_{n_j}(x_j)\right).\]</span> Since <span class="math inline">\(\displaystyle \bigcup_{j=1}^\infty I_{n_j}(x_j) \subset G\)</span>, then <span class="math display">\[m^*(f(E))&lt;p_0 \cdot m^*(G)&lt;p_0(m^*(E)+\varepsilon).\]</span> Let <span class="math inline">\(\varepsilon \to 0\)</span> and <span class="math inline">\(p_0 \to p\)</span>, we have <span class="math inline">\(m^*(f(E)) \leq p \cdot m^*(E)\)</span>.</p>
Assume without loss of generality that <span class="math inline">\(f\)</span> is continuous and <span class="math inline">\(q&gt;0\)</span>. Since <span class="math inline">\(f\)</span> is a strictly increasing function on <span class="math inline">\([a, b]\)</span>, then there is a strictly increasing function <span class="math inline">\(f^{-1}\)</span> on <span class="math inline">\(f([a, b])\)</span>. Take <span class="math inline">\(x_0 \in E\)</span> and <span class="math inline">\(y_0=f(x_0)\)</span>, we have <span class="math display">\[\lim_{n \to \infty} \frac{f^{-1}(y_0+k_n)-f^{-1}(y_0)}{k_n}=\lim_{n \to \infty} \frac{(x_0+h_n)-x_0}{f(x_0+h_n)-f(x_0)}=\frac{1}{Df(x_0)} \leq \frac{1}{q},\]</span> where <span class="math inline">\(k_n=f(x_0+h_n)-f(x_0) \to 0\)</span>. Hence, <span class="math display">\[m^*(f^{-1}(f(E))) \leq \frac{1}{q} \cdot m^*(f(E)),\]</span> i.e., <span class="math display">\[q \cdot m^*(E) \leq m^*(f(E)).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm1.5"><strong>Theorem 1.5 (Lebesgue's Theorem)</strong></span>    Suppose <span class="math inline">\(f\)</span> is an increasing function on <span class="math inline">\([a, b]\)</span>. Then <span class="math inline">\(f\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>, <span class="math inline">\(f&#39;\)</span> is Lebesgue integrable on <span class="math inline">\([a, b]\)</span>, and <span class="math display">\[\int_a^b f&#39;(x)\text{d}x \leq f(b)-f(a).\]</span></p>
<p><strong><em>Proof.</em></strong> Let <span class="math inline">\(g(x)=f(x)+x\)</span>, which is strictly increasing. Let <span class="math display">\[E=\{x \in [a, b]: \text{$g&#39;(x)$ does not exist}\}.\]</span> For all <span class="math inline">\(x \in E\)</span>, there exist <span class="math inline">\(D_1g(x)\)</span> and <span class="math inline">\(D_2g(x)\)</span> such that <span class="math inline">\(D_1g(x)&lt;D_2g(x)\)</span>. There exist <span class="math inline">\(p, q \in \mathbb{Q}\)</span> such that <span class="math inline">\(D_1g(x)&lt;p&lt;q&lt;D_2g(x)\)</span>. Let <span class="math inline">\(E_{pq}=\{x \in [a, b]: 0&lt;D_1g(x)&lt;p&lt;q&lt;D_2g(x)\}\)</span>, then <span class="math display">\[E=\bigcup_{p, q \in \mathbb{Q}} E_{pq}.\]</span></p>
<p>By <a href="#thm1.4">theorem 1.4</a>, <span class="math display">\[q \cdot m^*(E_{pq}) \leq m^*(g(E_{pq})) \leq p \cdot m^*(E_{pq}).\]</span> Since <span class="math inline">\(q&gt;p\)</span>, then <span class="math inline">\(m^*(E_{pq})=0\)</span>, and thus <span class="math inline">\(m(E)=0\)</span>, i.e., <span class="math inline">\(g\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>, and <span class="math inline">\(f\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>.</p>
Let <span class="math inline">\(\displaystyle g_n(x)=n\left(f\left(x+\frac{1}{n}\right)-f(x)\right)\)</span>, and <span class="math inline">\(f(x)=f(b)\)</span> when <span class="math inline">\(x \geq b\)</span>, then <span class="math inline">\(g_n(x) \to f&#39;(x)\ \text{a.e.}\ x \in [a, b]\)</span>, where <span class="math inline">\(g_n(x) \geq 0\)</span>. Let <span class="math inline">\(f&#39;(x)=0\)</span> if the derivative does not exist at <span class="math inline">\(x\)</span>. We have <span class="math display">\[\begin{aligned}
\int_a^b f&#39;(x)\text{d}x &amp;\leq \varliminf_{n \to \infty} \int_a^b g_n(x)\text{d}x
\\&amp;=\varliminf_{n \to \infty} \int_a^b n\left[f\left(x+\frac{1}{n}\right)-f(x)\right]\text{d}x
\\&amp;=\varliminf_{n \to \infty} \left(n\int_a^b f\left(x+\frac{1}{n}\right)\text{d}x-n\int_a^b f(x)\text{d}x\right)
\\&amp;=\varliminf_{n \to \infty} \left(n\int_{a+1/n}^{b+1/n} f(x)\text{d}x-n\int_a^b f(x)\text{d}x\right)
\\&amp;=\varliminf_{n \to \infty} \left(n\int_b^{b+1/n} f(x)\text{d}x-n\int_a^{a+1/n} f(x)\text{d}x\right)
\\&amp;=\varliminf_{n \to \infty} \left(f(b)-n\int_a^{a+1/n} f(x)\text{d}x\right)
\\&amp;\leq \varliminf_{n \to \infty} (f(b)-f(a))
\\&amp;=f(b)-f(a)&lt;\infty.
\end{aligned}\]</span> Therefore, <span class="math inline">\(f&#39;\)</span> is Lebesgue integrable on <span class="math inline">\([a, b]\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 1.1</strong>    Suppose <span class="math inline">\(E \subset [a, b]\)</span> and <span class="math inline">\(m(E)=0\)</span>. Then there exists an increasing function <span class="math inline">\(f\)</span> on <span class="math inline">\([a, b]\)</span> such that for all <span class="math inline">\(x \in E\)</span>, <span class="math inline">\(f&#39;(x)=\infty\)</span>.</p>
<p><strong><em>Proof.</em></strong> There exist open sets <span class="math inline">\(G_1 \supset G_2 \supset \cdots\)</span> such that <span class="math inline">\(E \subset G_n\)</span> and <span class="math inline">\(\displaystyle m(G_n)&lt;\frac{1}{2^n}\)</span>. Let <span class="math display">\[f_n(x)=m([a, x] \cap G_n),\]</span> where <span class="math inline">\(f_n\)</span> is increasing on <span class="math inline">\([a, b]\)</span>, and <span class="math inline">\(\displaystyle 0 \leq f_n(x) \leq m(G_n)&lt;\frac{1}{2^n}\)</span>. Besides, since <span class="math inline">\(f_n(x+h)-f_n(x) \leq |h|\)</span>, it is easy to show that <span class="math inline">\(f_n\)</span> is continuous on <span class="math inline">\([a, b]\)</span>. Let <span class="math display">\[f(x)=\sum_{n=1}^\infty f_n(x),\]</span> where <span class="math inline">\(f\)</span> is increasing and continuous on <span class="math inline">\([a, b]\)</span>.</p>
For all <span class="math inline">\(x \in E\)</span>, when <span class="math inline">\(x \neq b\)</span>, take a <span class="math inline">\(k \in \mathbb{N}\)</span>, then <span class="math display">\[\exists \varepsilon&gt;0\ \text{s.t.}\ 0&lt;h&lt;\varepsilon, [x, x+h] \subset G_1, \ldots, G_k.\]</span> We have <span class="math display">\[\begin{aligned}
\frac{f(x+h)-f(x)}{h}&amp;=\frac{1}{h}\left(\sum_{i=1}^\infty f_i(x+h)-\sum_{i=1}^\infty f_i(x)\right)
\\&amp;=\sum_{i=1}^\infty \frac{f_i(x+h)-f_i(x)}{h}
\\&amp;\geq \sum_{i=1}^k \frac{f_i(x+h)-f_i(x)}{h}
\\&amp;=\sum_{i=1}^k \frac{m([a, x+h] \cap G_i)-m([a, x] \cap G_i)}{h}
\\&amp;=\sum_{i=1}^k \frac{h}{h}=k.
\end{aligned}\]</span> Hence, <span class="math inline">\(f&#39;_+(x)=\infty.\)</span> Similarly, for all <span class="math inline">\(x \in E\)</span>, when <span class="math inline">\(x \neq a\)</span>, we can show <span class="math inline">\(f&#39;_-(x)=\infty\)</span>. Therefore, <span class="math inline">\(f&#39;(x)=\infty\)</span> for all <span class="math inline">\(x \in E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 1.2</strong>    In <a href="#thm1.5">Lebesgue's theorem</a>, we can find a function <span class="math inline">\(\theta\)</span> such that <span class="math display">\[\int_a^b \theta&#39;(x)\text{d}x&lt;\theta(b)-\theta(a).\]</span></p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(P_0\)</span> is a Cantor ternary set. Let <span class="math inline">\(\displaystyle G_0=P_0^c\)</span>. Define a function <span class="math inline">\(\theta\)</span> on <span class="math inline">\(G_0\)</span>: when <span class="math inline">\(\displaystyle x \in \left(\frac{1}{3}, \frac{2}{3}\right)\)</span>, <span class="math inline">\(\displaystyle \theta(x)=\frac{1}{2}\)</span>; when <span class="math inline">\(\displaystyle x \in \left(\frac{1}{9}, \frac{2}{9}\right)\)</span>, <span class="math inline">\(\displaystyle \theta(x)=\frac{1}{4}\)</span>; when <span class="math inline">\(\displaystyle x \in \left(\frac{7}{9}, \frac{8}{9}\right)\)</span>, <span class="math inline">\(\displaystyle \theta(x)=\frac{3}{4}\)</span>; <span class="math inline">\(\ldots\)</span>. For <span class="math inline">\(x_0 \in P_0\)</span>, let <span class="math display">\[\theta(x_0)=\sup_{\substack{x \in G_0 \\ x&lt;x_0}}\{\theta(x)\}.\]</span> Hence, <span class="math inline">\(\theta\)</span> is increasing. In addition, since <span class="math display">\[\lim_{x \to x_0^-} \theta(x)=\lim_{x \to x_0^+} \theta(x)=\theta(x_0),\]</span> then <span class="math inline">\(\theta\)</span> is continuous.</p>
For <span class="math inline">\(x \in G_0\)</span>, <span class="math inline">\(\theta&#39;(x)=0\)</span>, i.e., <span class="math inline">\(\theta&#39;(x)=0\ \text{a.e.}\ x \in [0, 1]\)</span>. Hence, <span class="math display">\[0=\int_0^1 \theta&#39;(x)\text{d}x&lt;1=\theta(1)-\theta(0).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="bounded-variation-function">2. Bounded Variation Function</h1>
<p><strong>Definition 2.1</strong>    Suppose <span class="math inline">\(f\)</span> is a real function on <span class="math inline">\([a, b]\)</span>. Take a partition <span class="math inline">\(\Delta: a=x_0&lt;x_1&lt;\cdots&lt;x_n=b\)</span>. The <strong>variation</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\([a, b]\)</span> is <span class="math display">\[v_\Delta=\sum_{i=1}^n |f(x_i)-f(x_{i-1})|.\]</span> The <strong>total variation</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\([a, b]\)</span> is <span class="math display">\[\bigvee_a^b (f)=\sup\{v_\Delta: \text{$\Delta$ is an arbitrary partition of $[a, b]$}\}.\]</span> If <span class="math display">\[\bigvee_a^b (f)&lt;\infty,\]</span> then <span class="math inline">\(f\)</span> is a <strong>bounded variation function</strong> on <span class="math inline">\([a, b]\)</span>. The collection of all bounded variation functions on <span class="math inline">\([a, b]\)</span> is denoted <span class="math inline">\(\text{BV}([a, b])\)</span>.</p>
<p><strong>Theorem 2.1</strong>    If <span class="math inline">\(f \in \text{BV}([a, b])\)</span>, then <span class="math inline">\(f\)</span> is bounded on <span class="math inline">\([a, b]\)</span>.</p>
<strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(x \in [a, b]\)</span>, and take a partition <span class="math inline">\(\Delta: a=x_0&lt;x_1=x&lt;x_2=b\)</span>. We have <span class="math display">\[\begin{aligned}
|f(x)| &amp;\leq |f(x)-f(a)|+|f(a)| 
\\&amp;\leq |f(x)-f(a)|+|f(b)-f(x)|+|f(a)|
\\&amp;=v_\Delta+|f(a)| 
\\&amp;\leq \bigvee_a^b (f)+|f(a)|.
\end{aligned}\]</span> Since <span class="math inline">\(f \in \text{BV}([a, b])\)</span>, then <span class="math display">\[\bigvee_a^b (f)+|f(a)|&lt;\infty,\]</span> i.e., <span class="math inline">\(f\)</span> is bounded on <span class="math inline">\([a, b]\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.2</strong>    <span class="math inline">\(\text{BV}([a, b])\)</span> is a linear space.</p>
<strong><em>Proof.</em></strong> Take an arbitrary partition <span class="math inline">\(\Delta: a=x_0&lt;x_1&lt;\cdots&lt;x_n=b\)</span>. Suppose <span class="math inline">\(c \in \mathbb{R}\)</span>, <span class="math inline">\(f, g \in \text{BV}([a, b])\)</span>, <span class="math display">\[\sum_{i=1}^n |f(x_i)-f(x_{i-1})| \leq M_1,\]</span> and <span class="math display">\[\sum_{i=1}^n |g(x_i)-g(x_{i-1})| \leq M_2.\]</span> Hence, <span class="math display">\[\begin{aligned}
\sum_{i=1}^n |(cf+g)(x_i)-(cf+g)(x_{i-1})|&amp;=\sum_{i=1}^n |(cf(x_i)+g(x_i))-(cf(x_{i-1})+g(x_{i-1}))|
\\&amp;=\sum_{i=1}^n |c(f(x_i)-f(x_{i-1}))+(g(x_i)-g(x_{i-1}))|
\\&amp;\leq |c|\sum_{i=1}^n |f(x_i)-f(x_{i-1})|+\sum_{i=1}^n |g(x_i)-g(x_{i-1})|
\\&amp;\leq |c|M_1+M_2&lt;\infty,
\end{aligned}\]</span> i.e., <span class="math inline">\(cf+g \in \text{BV}([a, b])\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.3</strong>    If <span class="math inline">\(f, g \in \text{BV}([a, b])\)</span>, then <span class="math inline">\(fg \in \text{BV}([a, b])\)</span>.</p>
<p><strong><em>Proof.</em></strong> Take an arbitrary partition <span class="math inline">\(\Delta: a=x_0&lt;x_1&lt;\cdots&lt;x_n=b\)</span>. Since <span class="math inline">\(f, g \in \text{BV}([a, b])\)</span>, then there is an <span class="math inline">\(M\)</span> such that <span class="math inline">\(|f(x)| \leq M\)</span>, <span class="math inline">\(|g(x)| \leq M\)</span>, <span class="math inline">\(\displaystyle \sum_{i=1}^n |f(x_i)-f(x_{i-1})| \leq M\)</span>, and <span class="math inline">\(\displaystyle \sum_{i=1}^n |g(x_i)-g(x_{i-1})| \leq M\)</span>. Hence,</p>
<span class="math display">\[\begin{aligned}
\sum_{i=1}^n |(fg)(x_i)-(fg)(x_{i-1})|&amp;=\sum_{i=1}^n |f(x_i)g(x_i)-f(x_{i-1})g(x_{i-1})|
\\&amp;=\sum_{i=1}^n |f(x_i)g(x_i)-f(x_i)g(x_{i-1})+f(x_i)g(x_{i-1})-f(x_{i-1})g(x_{i-1})|
\\&amp;=\sum_{i=1}^n |f(x_i)(g(x_i)-g(x_{i-1}))+g(x_{i-1})(f(x_i)-f(x_{i-1}))|
\\&amp;\leq \sum_{i=1}^n |f(x_i)| \cdot |g(x_i)-g(x_{i-1})|+\sum_{i=1}^n |g(x_{i-1})| \cdot |f(x_i)-f(x_{i-1})|
\\&amp;\leq 2nM^2&lt;\infty,
\end{aligned}\]</span> i.e., <span class="math inline">\(fg \in \text{BV}([a, b])\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.4</strong>    If <span class="math inline">\(f \in \text{BV}([a, b])\)</span> and <span class="math inline">\(a&lt;c&lt;b\)</span>, then <span class="math display">\[\bigvee_a^b (f)=\bigvee_a^c (f)+\bigvee_c^b (f).\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary partition <span class="math inline">\(\Delta: a=x_0&lt;x_1&lt;\cdots&lt;x_n=b\)</span>. If <span class="math inline">\(c=x_k\)</span> for some <span class="math inline">\(k\)</span>, then <span class="math display">\[\sum_{i=1}^n |f(x_i)-f(x_{i-1})|=\sum_{i=1}^k |f(x_i)-f(x_{i-1})|+\sum_{i=k+1}^n |f(x_i)-f(x_{i-1})| \leq \bigvee_a^c (f)+\bigvee_c^b (f).\]</span> If <span class="math inline">\(x_k&lt;c&lt;x_{k+1}\)</span> for some <span class="math inline">\(k\)</span>, then <span class="math display">\[\begin{aligned}
\sum_{i=1}^n |f(x_i)-f(x_{i-1})|&amp;=\sum_{i=1}^k |f(x_i)-f(x_{i-1})|+|f(x_{k+1})-f(x_k)|+\sum_{i=k+2}^n |f(x_i)-f(x_{i-1})|
\\&amp;\leq \sum_{i=1}^k |f(x_i)-f(x_{i-1})|+|f(c)-f(x_k)|
\\&amp;\ \ \ \ +|f(x_{k+1})-f(c)|+\sum_{i=k+2}^n |f(x_i)-f(x_{i-1})|
\\&amp;=\bigvee_a^c (f)+\bigvee_c^b (f).
\end{aligned}\]</span> Hence, <span class="math display">\[\bigvee_a^b (f) \leq \bigvee_a^c (f)+\bigvee_c^b (f).\]</span></p>
For all <span class="math inline">\(\varepsilon&gt;0\)</span>, we can find two partitions <span class="math display">\[\Delta_1: a=y_0&lt;y_1&lt;\cdots&lt;y_m=c\]</span> and <span class="math display">\[\Delta_2: c=z_0&lt;z_1&lt;\cdots&lt;z_n=b\]</span> such that <span class="math display">\[\bigvee_a^c (f)&lt;\sum_{i=1}^m |f(y_i)-f(y_{i-1})|+\frac{\varepsilon}{2}\]</span> and <span class="math display">\[\bigvee_c^b (f)&lt;\sum_{i=1}^n |f(z_i)-f(z_{i-1})|+\frac{\varepsilon}{2}.\]</span> Let <span class="math inline">\(\Delta&#39;=\Delta_1 \cup \Delta_2\)</span>, then <span class="math inline">\(\Delta&#39;\)</span> is a partition of <span class="math inline">\([a, b]\)</span> with <span class="math display">\[v_{\Delta&#39;}=\sum_{i=1}^m |f(y_i)-f(y_{i-1})|+\sum_{i=1}^n |f(z_i)-f(z_{i-1})|&gt;\bigvee_a^c (f)+\bigvee_c^b (f)-\varepsilon,\]</span> i.e., <span class="math display">\[\bigvee_a^b (f) \geq \bigvee_a^c (f)+\bigvee_c^b (f).\]</span> Therefore, <span class="math display">\[\bigvee_a^b (f)=\bigvee_a^c (f)+\bigvee_c^b (f).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.5 (Jordan Decomposition Theorem)</strong>    <span class="math inline">\(f \in \text{BV}([a, b])\)</span> if and only if <span class="math inline">\(f=g-h\)</span>, where <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are increasing on <span class="math inline">\([a, b]\)</span>.</p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Leftarrow)\)</span> For any increasing function <span class="math inline">\(\theta\)</span> on <span class="math inline">\([a, b]\)</span> with an arbitrary partition <span class="math inline">\(\Delta\)</span>, we have <span class="math inline">\(v_\Delta=|\theta(b)-\theta(a)|\)</span>, and thus <span class="math display">\[\bigvee_a^b (\theta)=|\theta(b)-\theta(a)|&lt;\infty,\]</span> i.e., <span class="math inline">\(\theta \in \text{BV}([a, b])\)</span>.</p>
<p>Suppose <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are increasing on <span class="math inline">\([a, b]\)</span>, then <span class="math inline">\(g, h \in \text{BV}([a, b])\)</span>, and thus <span class="math inline">\(f=g-h \in \text{BV}([a, b])\)</span>.</p>
<p><span class="math inline">\((\Rightarrow)\)</span> Suppose <span class="math inline">\(f \in \text{BV}([a, b])\)</span>. Let <span class="math display">\[g(x)=\bigvee_a^x (f),\]</span> where <span class="math inline">\(g\)</span> is increasing.</p>
Let <span class="math inline">\(h(x)=g(x)-f(x)\)</span>. Take arbitrary <span class="math inline">\(x_1&lt;x_2\)</span> such that <span class="math inline">\(a \leq x_1&lt;x_2 \leq b\)</span>. We have <span class="math display">\[\begin{aligned}
h(x_2)-h(x_1)&amp;=\bigvee_a^{x_2} (f)-f(x_2)-\left(\bigvee_a^{x_1} (f)-f(x_1)\right)
\\&amp;=\bigvee_{x_1}^{x_2} (f)-(f(x_2)-f(x_1))
\\&amp;\geq |f(x_2)-f(x_1)|-(f(x_2)-f(x_1)) \geq 0.
\end{aligned}\]</span> Hence, <span class="math inline">\(h\)</span> is increasing, and <span class="math inline">\(f=g-h\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>If <span class="math inline">\(f \in \text{BV}([a, b])\)</span>, then <span class="math inline">\(f=g-h\)</span>, where <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are increasing on <span class="math inline">\([a, b]\)</span>. Take an arbitrary increasing function <span class="math inline">\(a\)</span>, we know <span class="math inline">\(g+a\)</span> and <span class="math inline">\(h+a\)</span> are increasing, and <span class="math inline">\(f=(g+a)-(h+a)\)</span>. Hence, the increasing decomposition functions <strong>are not</strong> unique.</p><p>We define <strong>positive variation function</strong> as <span class="math display">\[p(x)=\frac{1}{2}\left(\bigvee_a^x (f)+f(x)-f(a)\right),\]</span> and <strong>negative variation function</strong> as <span class="math display">\[n(x)=\frac{1}{2}\left(\bigvee_a^x (f)-f(x)+f(a)\right).\]</span> Hence, <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span> are increasing, <span class="math inline">\(\displaystyle \bigvee_a^x (f)=p(x)+n(x)\)</span>, and <span class="math inline">\(f(x)-f(a)=p(x)-n(x)\)</span>, which is called <strong>normal decomposition</strong>.</p>
          </div>
<p><strong>Theorem 2.6</strong>    Suppose <span class="math inline">\(f \in \text{BV}([a, b])\)</span>. Then <span class="math inline">\(f\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>, and <span class="math inline">\(f&#39; \in L([a, b])\)</span>.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(f \in \text{BV}([a, b])\)</span>, then <span class="math inline">\(f=g-h\)</span>, where <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are increasing on <span class="math inline">\([a, b]\)</span>.</p>
<p>By <a href="#thm1.5">Lebesgue's theorem</a>, <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>; <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span> are Lebesgue integrable on <span class="math inline">\([a, b]\)</span>.</p>
Hence, <span class="math inline">\(f=g-h\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>, and is Lebesgue integrable on <span class="math inline">\([a, b]\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>The bounded variation function <strong>is not necessarily</strong> a continuous function. For example, <span class="math display">\[f(x)=\begin{cases}0, &amp;x \leq 0 \\1, &amp;x&gt;0\end{cases}.\]</span></p><p>The continuous function <strong>is not necessarily</strong> a bounded variation function. For example, <span class="math display">\[f(x)=\begin{cases}\displaystyle x\sin\frac{1}{x}, &amp;0&lt;x \leq 1 \\0, &amp;x=0\end{cases}.\]</span> Take a partition: <span class="math display">\[x_0=0, x_1=\frac{1}{(n-2)\pi+\pi/2}, x_2=\frac{1}{(n-3)\pi+\pi/2}, \ldots, x_{n-1}=\frac{1}{\pi+\pi/2}, x_n=1.\]</span> We have <span class="math display">\[\begin{aligned}\sum_{i=1}^n |f(x_i)-f(x_{i-1})|&amp;=\left|x_1\sin\frac{1}{x_1}-0\right|+\cdots+\left|1\sin1-x_{n-1}\sin\frac{1}{x_{n-1}}\right|\\&amp;\geq \frac{4}{\pi}\sum_{k=2}^n \frac{1}{2k-1}+\sin1.\end{aligned}\]</span> As <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\displaystyle \sum_{i=1}^n |f(x_i)-f(x_{i-1})=\infty\)</span>, and thus <span class="math inline">\(f \notin \text{BV}([a, b])\)</span>.</p>
          </div>
<p><span id="thm2.7"><strong>Theorem 2.7</strong></span>    If <span class="math inline">\(f \in \text{BV}([a, b])\)</span>, then <span class="math inline">\(f=g+h\)</span>, for some step function <span class="math inline">\(g\)</span> and continuous function <span class="math inline">\(h\)</span>.</p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(f\)</span> be an increasing function. Let <span class="math display">\[\theta_l(x)=\begin{cases}
0, &amp;x&lt;0 \\
1, &amp;x \geq 0
\end{cases}\]</span> and <span class="math display">\[\theta_r(x)=\begin{cases}
0, &amp;x \leq 0 \\
1, &amp;x&gt;0
\end{cases}.\]</span> Let <span class="math inline">\(d_n\)</span> be the discontinuous point of <span class="math inline">\(f\)</span>, and <span class="math display">\[g(x)=\sum_{n=1}^\infty \mu_n\theta_l(x)+\sum_{n=1}^\infty \lambda_n\theta_r(x),\]</span> where <span class="math inline">\(\mu_n=f(d_n)-f(d_n^-)\)</span> and <span class="math inline">\(\lambda_n=f(d_n^+)-f(d_n)\)</span>. Note that <span class="math inline">\(g\)</span> is a step function, with discontinuous points <span class="math inline">\(\{d_n\}\)</span>. Hence, <span class="math inline">\(f-g\)</span> is continuous, i.e., <span class="math display">\[\text{Increasing function}=\text{Step function}+\text{Continuous function}.\]</span> Since a bounded variation function is difference of two increasing functions, then <span class="math display">\[\text{Bounded variation function}=\text{Step function}+\text{Continuous function}.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="differentiation-of-indefinite-integral">3. Differentiation of Indefinite Integral</h1>
<p><strong>Theorem 3.1*</strong>    Suppose <span class="math inline">\(f \in L([a, b])\)</span>, and <span class="math inline">\(f(x)=0\)</span> if <span class="math inline">\(x \notin [a, b]\)</span>. Then <span class="math display">\[\lim_{h \to 0} \int_\mathbb{R} |f(x+h)-f(x)|\text{d}x=0.\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, and let <span class="math inline">\(f=f_1+f_2\)</span>, where <span class="math inline">\(f_1\)</span> is a continuous function with compact support set on <span class="math inline">\(\mathbb{R}\)</span>, and <span class="math inline">\(f_2\)</span> satisfies <span class="math display">\[\int_\mathbb{R} |f_2(x)|\text{d}x&lt;\frac{\varepsilon}{4}.\]</span> Since <span class="math inline">\(f_1\)</span> has compact support set and is uniformly continuous, then there exists a <span class="math inline">\(\delta&gt;0\)</span> such that when <span class="math inline">\(|h|&lt;\delta\)</span>, <span class="math display">\[\int_\mathbb{R} |f_1(x+h)-f_1(x)|\text{d}x&lt;\frac{\varepsilon}{2}.\]</span></p>
Hence, <span class="math display">\[\begin{aligned}
\int_\mathbb{R} |f(x+h)-f(x)|\text{d}x &amp;\leq \int_\mathbb{R} |f_1(x+h)-f_1(x)|\text{d}x+\int_\mathbb{R} |f_2(x+h)-f_2(x)|\text{d}x
\\&amp;&lt;\frac{\varepsilon}{2}+\int_\mathbb{R} |f_2(x+h)|\text{d}x+\int_\mathbb{R} |f_2(x)|\text{d}x
\\&amp;=\frac{\varepsilon}{2}+2\int_\mathbb{R} |f_2(x)|\text{d}x&lt;\varepsilon.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.2</strong>    Suppose <span class="math inline">\(f \in L([a, b])\)</span>. Then <span class="math display">\[\frac{\text{d}}{\text{d}x}\left(\int_a^x f(t)\text{d}t\right)=f(x)\ \text{a.e.}\ x \in [a, b].\]</span></p>
<div class="note ">
            <p><strong>Lemma 3.2.1</strong>    Suppose <span class="math inline">\(f \in L([a, b])\)</span>. Then <span class="math display">\[\lim_{h \to 0}\frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t=0\ \text{a.e.}\ x \in [a, b].\]</span></p><p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(h&gt;0\)</span>, and assume <span class="math inline">\(f(x)=0\)</span> if <span class="math inline">\(x \notin [a, b]\)</span>. We have <span class="math display">\[\begin{aligned}\int_a^b \left(\lim_{h \to 0} \frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t\right)\text{d}x &amp;\leq \int_\mathbb{R} \left(\lim_{h \to 0} \frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t\right)\text{d}x\\&amp;\leq \varliminf_{h \to 0} \int_\mathbb{R} \left(\frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t\right)\text{d}x\\&amp;=\varliminf_{h \to 0} \int_0^h \frac{1}{h}\left(\int_\mathbb{R} |f(x+t)-f(x)|\text{d}x\right)\text{d}t.\end{aligned}\]</span> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists a <span class="math inline">\(\delta&gt;0\)</span> such that when <span class="math inline">\(|t|&lt;\delta\)</span>, <span class="math display">\[\int_\mathbb{R} |f(x+t)-f(x)|\text{d}x&lt;\varepsilon.\]</span> When <span class="math inline">\(h&lt;\delta\)</span>, we have <span class="math display">\[\int_0^h \frac{1}{h}\left(\int_\mathbb{R} |f(x+t)-f(x)|\text{d}x\right)\text{d}t&lt;\frac{\varepsilon}{h}\int_0^h \text{d}t=\varepsilon.\]</span> Therefore, <span class="math display">\[\lim_{h \to 0}\frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t=0\ \text{a.e.}\ x \in [a, b].\]</span> We define <span class="math inline">\(x\)</span> as a <strong>Lebesgue point</strong> if <span class="math inline">\(x\)</span> satisfies <span class="math display">\[\lim_{h \to 0}\frac{1}{h}\int_0^h |f(x+t)-f(x)|\text{d}t=0.\]</span></p>
          </div>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(h&gt;0\)</span>, and assume <span class="math inline">\(f(x)=0\)</span> if <span class="math inline">\(x \notin [a, b]\)</span>. We have <span class="math display">\[\begin{aligned}
\int_a^b \left|\lim_{h \to 0} \frac{1}{h}\int_0^h (f(x+t)-f(x))\text{d}t\right|\text{d}x&amp;=\int_a^b \left(\lim_{h \to 0} \frac{1}{h}\left|\int_0^h (f(x+t)-f(x))\text{d}t\right|\right)\text{d}x
\\&amp;\leq \int_a^b \left(\lim_{h \to 0} \frac{1}{h} \int_0^h |f(x+t)-f(x)|\text{d}t\right)\text{d}x
\\&amp;=0.
\end{aligned}\]</span> Hence, <span class="math display">\[\int_a^b \left|\lim_{h \to 0} \frac{1}{h}\int_0^h (f(x+t)-f(x))\text{d}t\right|\text{d}x=0,\]</span> i.e., <span class="math display">\[\lim_{h \to 0} \frac{1}{h}\int_0^h (f(x+t)-f(x))\text{d}t=0\ \text{a.e.}\ x \in [a, b].\]</span></p>
<p>Similarly, if <span class="math inline">\(h&lt;0\)</span>, we have <span class="math display">\[\lim_{h \to 0} \frac{1}{h}\int_0^h (f(x+t)-f(x))\text{d}t=0\ \text{a.e.}\ x \in [a, b].\]</span></p>
Therefore, <span class="math display">\[\lim_{h \to 0} \frac{1}{h}\int_0^h (f(x+t)-f(x))\text{d}t=\lim_{h \to 0} \frac{1}{h}\int_0^h f(x+t)\text{d}t-f(x)=0\ \text{a.e.}\ x \in [a, b].\]</span> Since <span class="math display">\[\int_0^h f(x+t)\text{d}t=\int_x^{x+h} f(t)\text{d}t=\int_a^{x+h} f(t)\text{d}t-\int_a^x f(t)\text{d}t,\]</span> then <span class="math display">\[\lim_{h \to 0}\frac{1}{h}\left(\int_a^{x+h} f(t)\text{d}t-\int_a^x f(t)\text{d}t\right)=f(x)\ \text{a.e.}\ x \in [a, b],\]</span> i.e., <span class="math display">\[\frac{\text{d}}{\text{d}x}\left(\int_a^x f(t)\text{d}t\right)=f(x)\ \text{a.e.}\ x \in [a, b].\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="fundamental-theorem-of-calculus">4. Fundamental Theorem of Calculus</h1>
<p><span id="thm4.1"><strong>Theorem 4.1</strong></span>    Suppose <span class="math inline">\(f\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span> and <span class="math inline">\(f&#39;(x)=0\ \text{a.e.}\ x \in [a, b]\)</span>. If <span class="math inline">\(f\)</span> is not a constant function on <span class="math inline">\([a, b]\)</span>, then there exists an <span class="math inline">\(\varepsilon&gt;0\)</span> such that for all <span class="math inline">\(\delta&gt;0\)</span>, there exists a finite sequence of pairwise disjoint subintervals <span class="math inline">\((x_i, y_i)\)</span> of <span class="math inline">\([a, b]\)</span> with <span class="math inline">\(x_i&lt;y_i \in [a, b]\)</span> such that <span class="math display">\[\sum_{i=1}^n |y_i-x_i|&lt;\delta,\]</span> and <span class="math display">\[\sum_{i=1}^n |f(y_i)-f(x_i)|&gt;\varepsilon.\]</span> <span class="math inline">\(f\)</span> is defined as a <strong>singularity function</strong>.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(f\)</span> is not a constant function, then there exists a <span class="math inline">\(c \in [a, b]\)</span> such that <span class="math inline">\(f(a) \neq f(c)\)</span>. Since <span class="math inline">\(f&#39;(x)=0\ \text{a.e.}\ x \in [a, b]\)</span>, then there exists an <span class="math inline">\(E \subset [a, c]\)</span> such that <span class="math inline">\(m([a, c]-E)=0\)</span>, where <span class="math inline">\(f&#39;(x)=0\)</span> for <span class="math inline">\(x \in E\)</span>.</p>
<p>Let <span class="math inline">\(2\varepsilon=|f(c)-f(a)|&gt;0\)</span> and <span class="math inline">\(\displaystyle r=\frac{\varepsilon}{c-a}\)</span>. There exits a <span class="math inline">\(\delta_1&gt;0\)</span> such that when <span class="math inline">\(0&lt;h&lt;\delta_1\)</span>, <span class="math display">\[|f(x+h)-f(x)|&lt;rh.\]</span> It is obvious that the collection of <span class="math inline">\([x, x+h]\)</span> is a Vitali cover of <span class="math inline">\(E\)</span>. By <a href="#thm1.2">Vitali covering theorem</a>, for all <span class="math inline">\(\delta&gt;0\)</span>, there exist finite disjoint intervals <span class="math inline">\([x_1, x_1+h_1], \ldots, [x_n, x_n+h_n]\)</span> such that <span class="math display">\[m\left(E-\bigcup_{i=1}^n [x_i, x_i+h_i]\right)&lt;\delta.\]</span> Assume <span class="math inline">\(x_1&lt;x_1+h_1&lt;\cdots&lt;x_n&lt;x_n+h_n\)</span>. Hence, <span class="math display">\[m\left((a, x_1) \cup \bigcup_{i=1}^n (x_i+h_i, x_{i+1}) \cup (x_n+h_n, c)\right)&lt;\delta.\]</span></p>
Since <span class="math display">\[\sum_{i=1}^n |f(x_i+h_i)-f(x_i)|&lt;r\sum_{i=1}^n h_i&lt;r(c-a)=\varepsilon,\]</span> and <span class="math display">\[\begin{aligned}
&amp;|f(x_1)-f(a)|+\sum_{i=1}^n |f(x_{i+1})-f(x_i+h_i)|+|f(c)-f(x_n+h_n)|+\sum_{i=1}^n |f(x_i+h_i)-f(x_i)|
\\\geq &amp;\left|f(x_1)-f(a)+\sum_{i=1}^n (f(x_{i+1})-f(x_i+h_i))+f(c)-f(x_n+h_n)+\sum_{i=1}^n (f(x_i+h_i)-f(x_i))\right|
\\=&amp;|f(c)-f(a)|=2\varepsilon,
\end{aligned}\]</span> then <span class="math display">\[|f(x_1)-f(a)|+\sum_{i=1}^n |f(x_{i+1})-f(x_i+h_i)|+|f(c)-f(x_n+h_n)|&gt;\varepsilon.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 4.1</strong> A function <span class="math inline">\(f\)</span> is <strong>absolutely continuous</strong> on <span class="math inline">\([a, b]\)</span> if for all <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists a <span class="math inline">\(\delta&gt;0\)</span> such that whenever a finite sequence of pairwise disjoint subintervals <span class="math inline">\((x_i, y_i)\)</span> of <span class="math inline">\([a, b]\)</span> with <span class="math inline">\(x_i&lt;y_i \in [a, b]\)</span> satisfies <span class="math display">\[\sum_{i=1}^n |y_i-x_i|&lt;\delta,\]</span> then <span class="math display">\[\sum_{i=1}^n |f(y_i)-f(x_i)|&lt;\varepsilon.\]</span> The collection of all absolutely continuous functions on <span class="math inline">\([a, b]\)</span> is denoted <span class="math inline">\(\text{AC}([a, b])\)</span>.</p>
<p><strong>Example 4.1</strong>    If <span class="math inline">\(f\)</span> is Lipschitz continuous on <span class="math inline">\([a, b]\)</span>, i.e., <span class="math inline">\(|f(x)-f(y)| \leq M \cdot |x-y|\)</span>, then <span class="math inline">\(f \in \text{AC}([a, b])\)</span>.</p>
<p><strong>Example 4.2</strong>    If <span class="math inline">\(f \in \text{AC}([a, b])\)</span>, then <span class="math inline">\(f\)</span> is continuous on <span class="math inline">\([a, b]\)</span>.</p>
<p><strong>Example 4.3</strong>    If <span class="math inline">\(f\)</span> is a step function, then <span class="math inline">\(f\)</span> is a singularity function.</p>
<p><strong>Theorem 4.2</strong>    Suppose <span class="math inline">\(f \in L([a, b])\)</span>. Then <span class="math inline">\(\displaystyle \int_a^x f(t)\text{d}t\)</span> is absolutely continuous on <span class="math inline">\([a, b]\)</span>.</p>
<strong><em>Proof.</em></strong> Suppose <span class="math inline">\(f \in L([a, b])\)</span>. We have <span class="math display">\[\forall \varepsilon&gt;0, \exists \delta&gt;0\ \text{s.t.}\ m([a, b])&lt;\delta \Rightarrow \int_{[a, b]} |f(x)|\text{d}x&lt;\varepsilon.\]</span> Take an arbitrary finite sequence of pairwise disjoint subintervals <span class="math inline">\((x_i, y_i)\)</span> of <span class="math inline">\([a, b]\)</span> with <span class="math inline">\(x_i&lt;y_i \in [a, b]\)</span>, where <span class="math display">\[\sum_{i=1}^n |y_i-x_i|&lt;m([a, b])&lt;\delta.\]</span> We have <span class="math display">\[\begin{aligned}
\sum_{i=1}^n \left|\int_a^{y_i} f(t)\text{d}t-\int_a^{x_i} f(t)\text{d}t\right|&amp;=\sum_{i=1}^n \left|\int_{x_i}^{y_i} f(t)\text{d}t\right|
\\&amp;\leq \sum_{i=1}^n \int_{x_i}^{y_i} |f(t)|\text{d}t
\\&amp;=\int_{\bigcup_{i=1}^n (x_i, y_i)} |f(t)|\text{d}t&lt;\varepsilon,
\end{aligned}\]</span> i.e., <span class="math inline">\(\displaystyle \int_a^x f(t)\text{d}t\)</span> is absolutely continuous on <span class="math inline">\([a, b]\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.3</strong>    <span class="math inline">\(\text{AC}([a, b])\)</span> is a linear space.</p>
<p><strong><em>Proof.</em></strong> Take an arbitrary finite sequence of pairwise disjoint subintervals <span class="math inline">\((x_i, y_i)\)</span> of <span class="math inline">\([a, b]\)</span> with <span class="math inline">\(x_i&lt;y_i \in [a, b]\)</span>, where <span class="math display">\[\sum_{i=1}^n |y_i-x_i|&lt;\delta.\]</span> Suppose <span class="math inline">\(c \in \mathbb{R}-\{0\}\)</span>, <span class="math inline">\(f, g \in \text{AC}([a, b])\)</span>, <span class="math display">\[\sum_{i=1}^n |f(y_i)-f(x_i)|&lt;\frac{\varepsilon}{2|c|},\]</span> and <span class="math display">\[\sum_{i=1}^n |g(y_i)-g(x_i)|&lt;\frac{\varepsilon}{2}.\]</span></p>
We have <span class="math display">\[\begin{aligned}
\sum_{i=1}^n |(cf+g)(y_i)-(cf+g)(x_i)|&amp;=\sum_{i=1}^n |(cf(y_i)+g(y_i))-(cf(x_i)+g(x_i))|
\\&amp;=\sum_{i=1}^n |c(f(y_i)-f(x_i))+(g(y_i)-g(x_i))|
\\&amp;\leq |c|\sum_{i=1}^n |f(y_i)-f(x_i)|+\sum_{i=1}^n |g(y_i)-g(x_i)|
\\&amp;&lt;|c|\frac{\varepsilon}{2|c|}+\frac{\varepsilon}{2}=\varepsilon,
\end{aligned}\]</span> i.e., <span class="math inline">\(cf+g \in \text{AC}([a, b])\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.4</strong>    If <span class="math inline">\(f \in \text{AC}([a, b])\)</span>, then <span class="math inline">\(f \in \text{BV}([a, b])\)</span>.</p>
<strong><em>Proof.</em></strong> Take <span class="math inline">\(\varepsilon=1\)</span>, and an arbitrary finite sequence of pairwise disjoint subintervals <span class="math inline">\((x_i, y_i)\)</span> of <span class="math inline">\([a, b]\)</span> with <span class="math inline">\(x_i&lt;y_i \in [a, b]\)</span>, where <span class="math display">\[\sum_{i=1}^n |y_i-x_i|&lt;\delta.\]</span> We have <span class="math display">\[\sum_{i=1}^n |f(y_i)-f(x_i)|&lt;1.\]</span> Take a partition <span class="math inline">\(\Delta: a=c_0&lt;c_1&lt;\cdots&lt;c_n=b\)</span> such that <span class="math inline">\(c_{i+1}-c_i&lt;\delta\)</span>, then <span class="math display">\[\bigvee_{c_i}^{c_{i+1}} (f)&lt;1,\]</span> and thus <span class="math display">\[\bigvee_a^b (f)=\sum_{i=0}^n \bigvee_{c_i}^{c_{i+1}} (f)&lt;n&lt;\infty,\]</span> i.e., <span class="math inline">\(f \in \text{BV}([a, b])\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 4.5</strong>    If <span class="math inline">\(f \in \text{AC}([a, b])\)</span>, then <span class="math inline">\(f\)</span> is differentiable almost everywhere on <span class="math inline">\([a, b]\)</span>, and <span class="math inline">\(f&#39; \in L([a, b])\)</span>.</p>
<p><strong>Theorem 4.6 (Fundamental Theorem of Calculus)</strong>    If <span class="math inline">\(f \in \text{AC}([a, b])\)</span>, then <span class="math display">\[f(x)-f(a)=\int_a^x f&#39;(t)\text{d}t.\]</span></p>
<strong><em>Proof.</em></strong> We know <span class="math inline">\(\displaystyle \int_a^x f&#39;(t)\text{d}t \in \text{AC}([a, b])\)</span> and <span class="math inline">\(\displaystyle \frac{\text{d}}{\text{d}x}\left(\int_a^x f&#39;(t)\text{d}t\right)=f&#39;(x)\ \text{a.e.}\ x \in [a, b]\)</span>. Let <span class="math display">\[h(x)=f(x)-\int_a^x f&#39;(t)\text{d}t,\]</span> then <span class="math inline">\(h \in \text{AC}([a, b])\)</span>, and <span class="math inline">\(h&#39;(x)=f&#39;(x)-f&#39;(x)=0\ \text{a.e.}\ x \in [a, b]\)</span>. By <a href="#thm4.1">theorem 4.1</a>, <span class="math inline">\(h\)</span> is constant, where <span class="math display">\[h(a)=f(a)-\int_a^a f&#39;(t)\text{d}t=f(a)-0=f(a).\]</span> Hence, <span class="math display">\[f(x)-h(a)=f(x)-f(a)=\int_a^x f&#39;(t)\text{d}t.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.7 (Lebesgue Decomposition Theorem)</strong>    Suppose <span class="math inline">\(f \in \text{BV}([a, c])\)</span>. We can find a step function <span class="math inline">\(\varphi\)</span>, an absolutely continuous function <span class="math inline">\(f_1\)</span>, and a continuous singularity function <span class="math inline">\(f_2\)</span> such that <span class="math display">\[f=\varphi+f_1+f_2.\]</span></p>
<strong><em>Proof.</em></strong> By <a href="#thm2.7">theorem 2.7</a>, if <span class="math inline">\(f \in \text{BV}([a, c])\)</span>, we can find a step function <span class="math inline">\(\varphi\)</span>, and a continuous function <span class="math inline">\(g\)</span> such that <span class="math inline">\(f=\varphi+g\)</span>. Let <span class="math display">\[f_1(x)=\int_a^x g&#39;(t)\text{d}t,\]</span> then <span class="math inline">\(f_1\)</span> is an absolutely continuous function. Let <span class="math inline">\(f_2=g-f_1\)</span>, then <span class="math inline">\(f_2\)</span> is a continuous singularity function.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Differential</tag>
        <tag>Integral</tag>
      </tags>
  </entry>
  <entry>
    <title>Introduction to Network</title>
    <url>/Mathematics/Network-science/Introduction_to_Network.html</url>
    <content><![CDATA[<h1 id="definition">1. Definition</h1>
<p><strong>Definition 1.1</strong>    We abbreviate a <strong>graph</strong> <span class="math inline">\(\mathcal{G}\)</span> as <span class="math inline">\(\mathcal{G}=(\mathcal{V}, \mathcal{E})\)</span>, where <span class="math inline">\(\mathcal{V}\)</span> is the set of <strong>vertices (nodes)</strong> and <span class="math inline">\(\mathcal{E}\)</span> is the set of <strong>edges (links)</strong>. Let <span class="math inline">\(|\mathcal{V}|\)</span> be the number of vertices, and <span class="math inline">\(|\mathcal{E}|\)</span> be the number of edges in the graph <span class="math inline">\(\mathcal{G}\)</span>. If <span class="math inline">\(u\)</span> and <span class="math inline">\(b\)</span> are two vertices and there is an edge from <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span>, then we say <span class="math inline">\(v\)</span> is a <strong>neighbor</strong> of <span class="math inline">\(u\)</span>, denoted <span class="math inline">\((u, v) \in \mathcal{E}\)</span>.</p>
<p><strong>Definition 1.2</strong>    A <strong>directed graph</strong> or <strong>digraph</strong> is a graph where <em>all</em> edges are directed. The <strong>underlying</strong> graph of a digraph is the graph that results from turning all directed edges into <strong>undirected</strong> edges.</p>
<p><strong>Definition 1.3</strong>    If both endpoints of an edge are the same, then the edge is a <strong>loop</strong>.</p>
<p><strong>Example 1.1</strong>    The graph below is a loop.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 1
          </pre>
<p><strong>Definition 1.4</strong>    Two vertices are called <strong>adjacent</strong> if they are joined by an edge. A graph can be described by its <span class="math inline">\(|\mathcal{V}| \times |\mathcal{V}|\)</span> <strong>adjacency matrix</strong> <span class="math inline">\(A=(a_{u, v})\)</span>, where <span class="math inline">\(a_{u, v}=1\)</span> iff <span class="math inline">\((u, v) \in \mathcal{E}\)</span>.</p>
<div class="note info">
            <p>If there are no self-loops, all elements on the diagonal of the adjacency matrix are <span class="math inline">\(0\)</span>. If the edges of the graph are undirected, then the adjacency matrix will be symmetric.</p><p>The adjacency matrix entries tell us for every vertex <span class="math inline">\(v\)</span> which vertices are within (graph) distance <span class="math inline">\(1\)</span> of <span class="math inline">\(v\)</span>.</p><p>If we take the matrix product <span class="math inline">\(A^2=AA\)</span>, the entry for <span class="math inline">\((u, v)\)</span> with <span class="math inline">\(u \neq v\)</span> would be <span class="math display">\[a^{(2)}(u, v)=\sum_{w \in \mathcal{V}} a_{u, w}a_{w, v}.\]</span> If <span class="math inline">\(a^{(2)}(u, v) \neq 0\)</span>, then <span class="math inline">\(u\)</span> can be reached from <span class="math inline">\(v\)</span> within two steps, i.e., <span class="math inline">\(u\)</span> is within distance <span class="math inline">\(2\)</span> of <span class="math inline">\(v\)</span>. Higher powers can be interpreted similarly.</p>
          </div>
<p><strong>Definition 1.5</strong>    A <strong>complete</strong> graph is a graph, without self-loops, s.t. every pair of vertices is joined by an edge. The adjacency matrix has entry <span class="math inline">\(0\)</span> on the diagonal, and <span class="math inline">\(1\)</span> everywhere else.</p>
<p><strong>Example 1.2</strong>    The graph below is complete.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2)) --- 3((3))
1 --- 3
          </pre>
<p><strong>Definition 1.6</strong>    A <strong>bipartite</strong> graph is a graph where the vertex set <span class="math inline">\(\mathcal{V}\)</span> is decomposed into two disjoint subsets, <span class="math inline">\(\mathcal{U}\)</span> and <span class="math inline">\(\mathcal{W}\)</span>, s.t. there are no edges between any two vertices in <span class="math inline">\(\mathcal{U}\)</span>, and there are no edges between any two vertices in <span class="math inline">\(\mathcal{W}\)</span>; all edges have one endpoint in <span class="math inline">\(\mathcal{U}\)</span> and the other endpoint in <span class="math inline">\(\mathcal{W}\)</span>. The adjacency matrix <span class="math inline">\(A\)</span> can then be arranged s.t. <span class="math display">\[A=\begin{bmatrix}
0 &amp; A_1 \\
A_2 &amp; 0
\end{bmatrix}.\]</span></p>
<p><strong>Example 1.3</strong>    The graph below is bipartite.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) & 4((4))
2((2)) --- 3((3)) & 4((4))
subgraph U
1((1))
2((2))
end
subgraph W
3((3))
4((4))
end
          </pre>
<h1 id="network-summary">2. Network Summary</h1>
<h2 id="density-degree-and-degree-distribution">2.1. Density, Degree and Degree Distribution</h2>
<p><strong>Definition 2.1</strong>    The <strong>density</strong> of a network with <span class="math inline">\(e\)</span>edges and <span class="math inline">\(n\)</span> vertices is <span class="math display">\[\frac{e}{\displaystyle \binom{n}{2}}=\frac{2e}{n(n-1)}\]</span> where <span class="math inline">\(\displaystyle \binom{n}{2}\)</span> is the potential number of edges.</p>
<p><strong>Definition 2.2</strong>    The <strong>degree</strong> <span class="math inline">\(d(v)\)</span> of a vertex <span class="math inline">\(v\)</span> is the number of edges which involve <span class="math inline">\(v\)</span> as an endpoint. The degree can be calculated from the adjacency matrix <span class="math inline">\(A\)</span> <span class="math display">\[d(v)=\sum_{u \in \mathcal{V}} a_{u, v}.\]</span></p>
<p><strong>Definition 2.3</strong>    The <strong>average degree</strong> of a graph is the average of its vertex degrees <span class="math display">\[\overline{d}=\frac{1}{n}\sum_{v \in \mathcal{V}} d(v).\]</span></p>
<p><strong>Definition 2.4</strong>    The <strong>degree sequence</strong> of a given network on a vertex set <span class="math inline">\(\mathcal{V}\)</span> with <span class="math inline">\(n\)</span> elements is the unordered <span class="math inline">\(n\)</span>-element set of degrees <span class="math inline">\(\{d(v), v \in \mathcal{V}\}\)</span>.</p>
<p><strong>Definition 2.5</strong>    The <strong>degree distribution</strong> <span class="math inline">\((d_0, d_1, \ldots)\)</span> of a graph on <span class="math inline">\(n\)</span> vertices is the vector of fraction of vertices with given degree <span class="math display">\[d_k=\frac{1}{n} \times \text{Number of vertices of degree}\ k.\]</span></p>
<div class="note info">
            <p>For directed graphs, we define the <strong>in-degree</strong> as the number of edges directed at the vertex, and the <strong>out-degree</strong> as the number of edges taht go out from that vertex.</p>
          </div>
<h2 id="local-clustering-coefficient">2.2. Local Clustering Coefficient</h2>
<p><strong>Definition 2.6</strong>    The <strong>local clustering coefficient</strong> of a vertex <span class="math inline">\(v\)</span> is the proportion of neighbors of <span class="math inline">\(v\)</span> which are neighbors themselves. In adjacency matrix notation <span class="math display">\[C(v)=\frac{\displaystyle \sum_{u, w \in \mathcal{V}} a_{u, v}a_{w, v}a_{u,w}}{\displaystyle \sum_{u \neq w \in \mathcal{V}} a_{u, v}a_{w, v}},\]</span> where <span class="math inline">\(\displaystyle \frac{0}{0}:=0\)</span>.</p>
<div class="note info">
            <p>To compute <span class="math inline">\(C(v)\)</span> more efficiently, we should note that <span class="math inline">\(\displaystyle \frac{1}{2}\sum_{\substack{u \neq v \in \mathcal{V} \\ w \neq u, v \in \mathcal{V}}} a_{u, v}a_{w, v}a_{u, w}\)</span> is the number of <strong>triangles</strong> involving <span class="math inline">\(v\)</span> in the graph, and <span class="math inline">\(\displaystyle \frac{1}{2}\sum_{u \neq w \in \mathcal{V}} a_{u, v}a_{w, v}\)</span> is the number of <strong><span class="math inline">\(2\)</span>-stars</strong> centered around <span class="math inline">\(v\)</span> in the graph.</p>
          </div>
<p><strong>Definition 2.7</strong>    The <strong>average clustering coefficient</strong> is defined as <span class="math display">\[\overline{C}=\frac{1}{|\mathcal{V}|}\sum_{v \in \mathcal{V}} C(v).\]</span></p>
<div class="note info">
            <p>The local clustering coefficient describes how <em>locally dense</em> a graph is.</p>
          </div>
<h2 id="global-clustering-coefficient">2.3. Global Clustering Coefficient</h2>
<p><strong>Definition 2.8</strong>    The <strong>global clustering coefficient</strong> or <strong>transitivity</strong> is defined as <span class="math display">\[C=\frac{3 \times \text{Number of triangles}}{\text{Number of connected triples}}=\frac{6 \times \text{Number of triangles}}{\text{Number of paths of length two}}\]</span></p>
<div class="note warning">
            <p><span class="math inline">\(\overline{C} \neq C\)</span> in general. Indeed <span class="math inline">\(\overline{C}\)</span> tends to be dominated by vertices with low degree, since they tend to have small denominators in the local clustering coefficient.</p>
          </div>
<h2 id="expected-clustering-coefficient">2.4. Expected Clustering Coefficient</h2>
<p><strong>Definition 2.9</strong>    For models of random networks, we consider <strong>expected clustering coefficient</strong> <span class="math display">\[E(C)=\frac{3\mathbb{E}[\text{Number of triangles}]}{\mathbb{E}[\text{Number of connected triples}]}\]</span></p>
<h2 id="average-shortest-path">2.5. Average Shortest Path</h2>
<p><strong>Definition 2.10</strong>    In a graph, a <strong>path</strong> from vertex <span class="math inline">\(v_0\)</span> to vertex <span class="math inline">\(v_n\)</span> is an alternating sequence of vertices and edges <span class="math inline">\((v_0, e_1, v_1, e_2, \ldots, v_{n-1}, e_n, v_n)\)</span> s.t. the endpoints of <span class="math inline">\(e_i\)</span> are <span class="math inline">\(v_{i-1}\)</span> and <span class="math inline">\(v_i\)</span> for <span class="math inline">\(i=1, \ldots, n\)</span>. The <strong>distance</strong> <span class="math inline">\(\mathscr{l}(u, v)\)</span> between two vertices <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> is the length of a <strong>shortest</strong> path (not necessarily unique) joining them.</p>
<div class="note info">
            <p>We can calculate <span class="math inline">\(\mathscr{l}(u, v)\)</span> from the adjacency matrix <span class="math inline">\(A\)</span> as the smallest power <span class="math inline">\(p\)</span> of <span class="math inline">\(A\)</span> s.t. <span class="math inline">\(A^{(p)}(u, v) \neq 0\)</span>.</p>
          </div>
<p><strong>Definition 2.11</strong>    A graph is called <strong>connected</strong> if there is a path between any pair of vertices in the graph, otherwise it is <strong>disconnected</strong>. In a connected graph, the <strong>average shortest path length</strong> is defined as <span class="math display">\[\mathscr{l}=\frac{1}{|\mathcal{V}|(|\mathcal{V}|-1)}\sum_{u \neq v \in \mathcal{V}} \mathscr{l}(u, v),\]</span> where <span class="math inline">\(|\mathcal{V}|(|\mathcal{V}|-1)\)</span> means the possible number of shortest path length.</p>
<div class="note info">
            <p>The average shortest path length describes how <em>globally connected</em> a graph is.</p>
          </div>
<p><strong>Example 2.1</strong>    Consider a graph.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 5((5)) --- 3((3)) --- 2((2))
5((5)) --- 2((2))
5((5)) --- 4((4)) --- 2((2))
          </pre>
<p>We know <span class="math inline">\(\mathcal{V}=\{1, 2, 3, 4, 5\}\)</span>, <span class="math inline">\(\mathcal{E}=\{(1, 5), (2, 3), (2, 4), (2, 5), (3, 5), (4, 5)\}\)</span>, <span class="math inline">\(|\mathcal{V}|=5\)</span>, and <span class="math inline">\(|\mathcal{E}|=6\)</span>.</p>
<p>The adjacency matrix is <span class="math display">\[\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 0
\end{bmatrix}.\]</span></p>
<p>The density is <span class="math inline">\(\displaystyle \frac{2e}{n(n-1)}=0.6\)</span>.</p>
<p>The degrees of vertices are <span class="math inline">\(d(1)=1\)</span>, <span class="math inline">\(d(2)=3\)</span>, <span class="math inline">\(d(3)=2\)</span>, <span class="math inline">\(d(4)=2\)</span> and <span class="math inline">\(d(5)=4\)</span>. The average degree is <span class="math inline">\(\overline{d}=2.4\)</span>. The degree sequence is <span class="math inline">\(\{1, 3, 2, 2, 4\}\)</span>. The degree distribution is <span class="math inline">\(d_0=0\)</span>, <span class="math inline">\(d_1=0.2\)</span>, <span class="math inline">\(d_2=0.4\)</span>, <span class="math inline">\(d_3=0.2\)</span> and <span class="math inline">\(d_4=0.2\)</span>.</p>
<p>The local clustering coefficients of vertices are <span class="math inline">\(C(1)=0\)</span>, <span class="math inline">\(\displaystyle C(2)=\frac{2}{3}\)</span>, <span class="math inline">\(C(3)=1\)</span>, <span class="math inline">\(C(4)=1\)</span> and <span class="math inline">\(\displaystyle C(5)=\frac{1}{3}\)</span>. The average clustering coefficient is <span class="math inline">\(\overline{C}=0.6\)</span>.</p>
<p>The global clustering coefficient is <span class="math inline">\(\displaystyle C=\frac{6}{11}\)</span>.</p>
<p>The distance matrix is <span class="math display">\[\begin{bmatrix}
0 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\
2 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
2 &amp; 1 &amp; 0 &amp; 2 &amp; 1 \\
2 &amp; 1 &amp; 2 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 0
\end{bmatrix}.\]</span> The graph is connected, and the average shortest path length is <span class="math inline">\(\mathscr{l}=1.4\)</span>.</p>
<h2 id="small-graph-and-motif">2.6. Small Graph and Motif</h2>
<p><strong>Definition 2.12</strong>    <strong>Small subgraph</strong> can be viewed as building-block pattern of networks. By small we mean graph on a small number (three to five) of vertices. Often a small graph is called a <strong>motif</strong> when it is <strong>over-represented</strong> in the network, where over-representation is judged using a probabilistic model for the network.</p>
<div class="note info">
            <p>We think of a motif as a small graph with a fixed number of vertices and with a given topology, and we use the term interchangeably with small graph.</p>
          </div>
<h2 id="spectral-summary">2.7. Spectral Summary</h2>
<p><strong>Definition 2.13</strong>    We call a graph <strong>simple</strong> if it has no self-loops or multiple edges.</p>
<p><strong>Definition 2.14</strong>     The <strong>eigenvector centrality</strong> <strong><em>(Not done. To be continued...)</em></strong></p>
<p><strong>Definition 2.15</strong>     Let <span class="math inline">\(D\)</span> be the diagonal matrix with diagonal entries <span class="math inline">\(D_{ii}\)</span>, the degree of vertex <span class="math inline">\(i\)</span>. The <strong>graph Laplacian</strong> is <span class="math inline">\(L=D-A\)</span>, where <span class="math display">\[L(i, j)=\begin{cases}
d(i), &amp;i=j \\
-1, &amp;i \neq j\ \text{and}\ (i, j) \in \mathcal{E} \\
0, &amp;\text{otherwise}
\end{cases}.\]</span> The second smallest (smallest non-zero) eigenvalue of <span class="math inline">\(L\)</span> is called <strong>algebraic connectivity</strong> or <strong>Fiedler value</strong> or <strong>spectral gap</strong> of <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="note info">
            <p><span class="math inline">\(L\)</span> is symmetric, with real eigenvalues <span class="math inline">\(\lambda_0 \leq \cdots \leq \lambda_{n-1}\)</span> and eigenvectors <span class="math inline">\(\phi_1, \ldots, \phi_{n-1}\)</span>.</p><p>Since every row sum and column sum of <span class="math inline">\(L\)</span> is zero, then <span class="math inline">\(L\mathbf{1}=0\mathbf{1}\)</span>, and thus <span class="math inline">\(\lambda_0=0\)</span> and <span class="math inline">\(\phi_0=\mathbf{1}\)</span>.</p>
          </div>
<h2 id="other-summary">2.8. Other Summary</h2>
<p>Specific network may require specific summary. For example, when there is a spatial structure in the network, then geometrical consideration may enter.</p>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Network science</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title>Lebesgue Integral</title>
    <url>/Mathematics/Real-analysis/Lebesgue_Integral.html</url>
    <content><![CDATA[<h1 id="lebesgue-integral-of-non-negative-simple-function">1. Lebesgue Integral of Non-negative Simple Function</h1>
<p><strong>Definition 1.1</strong>    Suppose <span class="math inline">\(E\)</span> is measurable, <span class="math inline">\(E_i\)</span> are disjoint measurable sets, and <span class="math display">\[f(x)=\sum_{i=1}^s c_i\chi_{E_i}(x), c_i \geq 0.\]</span> The <strong>Lebesgue integral</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span> is <span class="math display">\[\int_E f(x)\text{d}x=\sum_{i=1}^s c_i \cdot m(E_i).\]</span> Suppose <span class="math inline">\(A \subset E\)</span>, and <span class="math inline">\(f|_A(x)=c_i\)</span>, <span class="math inline">\(x \in A \cap E_i\)</span>, then <span class="math display">\[\int_A f(x)\text{d}x=\sum_{i=1}^s c_i \cdot m(A \cap E_i).\]</span></p>
<p><strong>Theorem 1.1</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint measurable subsets of <span class="math inline">\(E\)</span>, then <span class="math display">\[\int_{A \cup B} f(x)\text{d}x=\int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(A \cap B=\varnothing\)</span>, then <span class="math display">\[\begin{aligned}
\int_{A \cup B} f(x)\text{d}x&amp;=\sum_{i=1}^s c_i \cdot m((A \cup B) \cap E_i)
\\&amp;=\sum_{i=1}^s c_i \cdot m((A \cap E_i) \cup (B \cap E_i))
\\&amp;=\sum_{i=1}^s c_i \cdot (m(A \cap E_i)+m(B \cap E_i))
\\&amp;=\sum_{i=1}^s c_i \cdot m(A \cap E_i)+\sum_{i=1}^s c_i \cdot m(B \cap E_i)
\\&amp;=\int_A f(x)\text{d}x+\int_B f(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 1.2</strong>    Suppose <span class="math inline">\(\{A_n\}_{n=1}^\infty\)</span> is an increasing collection of measurable subsets of <span class="math inline">\(E\)</span>, and <span class="math display">\[E=\bigcup_{n=1}^\infty A_n=\lim_{n \to \infty} A_n.\]</span> Then <span class="math display">\[\int_E f(x)\text{d}x=\lim_{n \to \infty}\int_{A_n} f(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\begin{aligned}
\lim_{n \to \infty} \int_{A_n} f(x)\text{d}x&amp;=\lim_{n \to \infty} \sum_{i=1}^s c_i \cdot m(A_n \cap E_i)
\\&amp;=\sum_{i=1}^s c_i \cdot \lim_{n \to \infty} m(A_n \cap E_i)
\\&amp;=\sum_{i=1}^s c_i \cdot m(E \cap E_i)
\\&amp;=\int_E f(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 1.3</strong>    Suppose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are non-negative real numbers. Then <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(c \in \mathbb{R} \geq 0\)</span>, we have <span class="math inline">\(\displaystyle cf(x)=\sum_{i=1}^s cc_i \cdot \chi_{E_i}(x)\)</span>, where <span class="math inline">\(cc_i \geq 0\)</span>. Then <span class="math inline">\(cf\)</span> is a non-negative simple function. Therefore, <span class="math display">\[\begin{aligned}
\int_E cf(x)\text{d}x&amp;=\sum_{i=1}^s cc_i \cdot m(E_i)
\\&amp;=c\sum_{i=1}^s c_i \cdot m(E_i)
\\&amp;=c\int_E f(x)\text{d}x.
\end{aligned}\]</span></p>
<p>Suppose <span class="math inline">\(\displaystyle f(x)=\sum_{i=1}^s c_i \cdot \chi_{E_i}(x)\)</span> and <span class="math inline">\(\displaystyle g(x)=\sum_{i=1}^p d_i \cdot \chi_{\widetilde{E}_i}(x)\)</span>.</p>
<p>When <span class="math inline">\(x \in E_i \cap \widetilde{E}_j\)</span>, <span class="math inline">\(f(x)+g(x)=c_i+d_j \geq 0\)</span>. Hence, <span class="math display">\[f(x)+g(x)=\sum_{\substack{1 \leq i \leq s \\ 1 \leq j \leq p}} (c_i+d_j) \cdot \chi_{E_i \cap \widetilde{E}_j}(x),\]</span> and <span class="math inline">\(f+g\)</span> is a non-negative simple function. Therefore, <span class="math display">\[\begin{aligned}
\int_E (f(x)+g(x))\text{d}x&amp;=\sum_{\substack{1 \leq i \leq s \\ 1 \leq j \leq p}} (c_i+d_j) \cdot m(E_i \cap \widetilde{E}_j)
\\&amp;=\sum_{\substack{1 \leq i \leq s \\ 1 \leq j \leq p}} c_i \cdot m(E_i \cap \widetilde{E}_j)+\sum_{\substack{1 \leq i \leq s \\ 1 \leq j \leq p}} d_j \cdot m(E_i \cap \widetilde{E}_j)
\\&amp;=\sum_{1 \leq i \leq s} c_i\sum_{1 \leq j \leq p} m(E_i \cap \widetilde{E}_j)+\sum_{1 \leq j \leq p} d_j\sum_{1 \leq i \leq s} m(E_i \cap \widetilde{E}_j)
\\&amp;=\sum_{1 \leq i \leq s} c_i \cdot m(E_i)+\sum_{1 \leq j \leq p} d_j \cdot m(\widetilde{E}_j)
\\&amp;=\int_E f(x)\text{d}x+\int_E g(x)\text{d}x.
\end{aligned}\]</span></p>
Hence, it is easy to show that <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="lebesgue-integral-of-non-negative-measurable-function">2. Lebesgue Integral of Non-negative Measurable Function</h1>
<h2 id="definition">2.1. Definition</h2>
<p><strong>Definition 2.1</strong>    Suppose <span class="math inline">\(E\)</span> is measurable, and <span class="math inline">\(f\)</span> is a non-negative measurable function on <span class="math inline">\(E\)</span>. The <strong>Lebesgue integral</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span> is <span class="math display">\[\int_E f(x)\text{d}x=\sup\left\{\int_E \varphi(x)\text{d}x\right\},\]</span> where <span class="math inline">\(\varphi\)</span> is a non-negative simple function on <span class="math inline">\(E\)</span>, and <span class="math inline">\(\varphi(x) \leq f(x)\)</span>.</p>
<p><strong>Theorem 2.1</strong>    Suppose <span class="math inline">\(\varphi\)</span> and <span class="math inline">\(\psi\)</span> are non-negative simple functions on <span class="math inline">\(E\)</span>. If <span class="math inline">\(\varphi(x) \leq \psi(x)\)</span>, then <span class="math display">\[\int_E \varphi(x)\text{d}x \leq \int_E \psi(x)\text{d}x.\]</span></p>
<p><strong>Corollary 2.2</strong>    Suppose <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are non-negative measurable functions on <span class="math inline">\(E\)</span>. If <span class="math inline">\(f(x) \leq g(x)\)</span>, then <span class="math display">\[\int_E f(x)\text{d}x \leq \int_E g(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> For any <span class="math inline">\(0 \leq \varphi(x) \leq f(x)\)</span>, where <span class="math inline">\(\varphi\)</span> is a non-negative simple function, we have <span class="math inline">\(0 \leq \varphi(x) \leq g(x)\)</span>, <span class="math inline">\(x \in E\)</span>. Then <span class="math display">\[\int_E g(x)\text{d}x \geq \int_E \varphi(x)\text{d}x.\]</span> Therefore, <span class="math display">\[\int_E g(x)\text{d}x \geq \sup_{0 \leq \varphi(x) \leq f(x)}\left\{\int_E \varphi(x)\text{d}x\right\}=\int_E f(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 2.2</strong>    If <span class="math inline">\(\displaystyle \int_E f(x)\text{d}x&lt;\infty\)</span>, then <span class="math inline">\(f\)</span> is <strong>Lebesgue integrable</strong> on <span class="math inline">\(E\)</span>.</p>
<p><strong>Theorem 2.3</strong>    If <span class="math inline">\(f\)</span> is Lebesgue integrable on <span class="math inline">\(E\)</span>, then <span class="math inline">\(f\)</span> is finite almost everywhere on <span class="math inline">\(E\)</span>, i.e., <span class="math display">\[m(E[f=\infty])=0.\]</span></p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(f(x) \geq f(x) \cdot \chi_{E[f \geq n]}(x)\)</span>, then <span class="math display">\[\infty&gt;\int_E f(x)\text{d}x \geq \int_E f(x) \cdot \chi_{E[f \geq n]}(x)\text{d}x=\int_{E[f \geq n]} f(x)\text{d}x \geq \int_{E[f \geq n]} n\text{d}x=n \cdot m(E[f \geq n]).\]</span> Therefore, <span class="math display">\[m(E[f \geq n]) \leq \frac{1}{n}\int_E f(x)\text{d}x.\]</span> Since <span class="math inline">\(E[f \geq n] \supset E[f \geq n+1]\)</span> and <span class="math inline">\(\displaystyle E[f=\infty]=\bigcap_{n=1}^\infty E[f \geq n]\)</span>, then <span class="math display">\[m(E[f=\infty])=\lim_{n \to \infty} m(E[f \geq n])=0,\]</span> i.e., <span class="math inline">\(f\)</span> is finite almost everywhere on <span class="math inline">\(E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.4</strong>    Suppose <span class="math inline">\(A\)</span> is a measurable subset of <span class="math inline">\(E\)</span>. Then <span class="math display">\[\int_A f(x)\text{d}x=\int_E f(x) \cdot \chi_A(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\begin{aligned}
\int_A f(x)\text{d}x&amp;=\sup_{\substack{\varphi(x) \leq f(x) \\ x \in A}}\left\{\int_A \varphi(x)\text{d}x\right\}
\\&amp;=\sup_{\substack{\varphi(x)\chi_A(x) \leq f(x)\chi_A(x) \\ x \in E}}\left\{\int_A \varphi(x)\text{D}x\right\}
\\&amp;=\int_E f(x)\varphi_A(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.5</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are disjoint measurable subsets of <span class="math inline">\(E\)</span>, then <span class="math display">\[\int_{A \cup B} f(x)\text{d}x=\int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary simple function <span class="math inline">\(\varphi\)</span> on <span class="math inline">\(A \cup B\)</span>, where <span class="math inline">\(0 \leq \varphi(x) \leq f(x)\)</span> for all <span class="math inline">\(x \in E\)</span>. We have <span class="math display">\[\int_{A \cup B} \varphi(x)\text{d}x=\int_A \varphi(x)\text{d}x+\int_B \varphi(x)\text{d}x \leq \int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span> Therefore, <span class="math display">\[\int_{A \cup B} f(x)\text{d}x \leq \int_{A \cup B} \varphi(x)\text{d}x \leq \int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span></p>
Take an arbitrary simple function <span class="math inline">\(\varphi_1\)</span> on <span class="math inline">\(A\)</span>, where <span class="math inline">\(0 \leq \varphi_1(x) \leq f(x)\)</span> for all <span class="math inline">\(x \in A\)</span>, and an arbitrary simple function <span class="math inline">\(\varphi_2\)</span> on <span class="math inline">\(B\)</span>, where <span class="math inline">\(0 \leq \varphi_2(x) \leq f(x)\)</span> for all <span class="math inline">\(x \in B\)</span>. Then <span class="math display">\[\varphi(x)=\begin{cases}
\varphi_1(x), &amp;x \in A \\
\varphi_2(x), &amp;x \in B
\end{cases}\]</span> is a simple function on <span class="math inline">\(A \cup B\)</span>, where <span class="math inline">\(0 \leq \varphi(x) \leq f(x)\)</span>. Hence, <span class="math display">\[\int_{A \cup B} f(x)\text{d}x \geq \int_{A \cup B} \varphi(x)\text{d}x=\int_A \varphi(x)\text{d}x+\int_B \varphi(x)\text{d}x=\int_A \varphi_1(x)\text{d}x+\int_B \varphi_2(x)\text{d}x.\]</span> Therefore, <span class="math display">\[\int_{A \cup B} f(x)\text{d}x \geq \int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span> As a consequence, <span class="math display">\[\int_{A \cup B} f(x)\text{d}x=\int_A f(x)\text{d}x+\int_B f(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="limit-and-integral">2.2. Limit and Integral</h2>
<p><strong>Theorem 2.6</strong>    Suppose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are non-negative real numbers, then <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span></p>
<div class="note ">
            <p><span id="thm2.6.1"><strong>Lemma 2.6.1 (Levi's Monotonicity Theorem)</strong></span>    Suppose <span class="math inline">\(\{f_n\}_{n=1}^\infty\)</span> is a sequence of non-negative measurable functions on <span class="math inline">\(E\)</span>. For <span class="math inline">\(x \in E\)</span>, and for all <span class="math inline">\(n\)</span>, we have <span class="math inline">\(f_n(x) \leq f_{n+1}(x)\)</span>. Then <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x=\int_E \lim_{n \to \infty} f_n(x)\text{d}x.\]</span></p><p><strong><em>Proof.</em></strong> Let <span class="math inline">\(\displaystyle f(x)=\lim_{n \to \infty} f_n(x)\)</span>. We know <span class="math inline">\(f\)</span> is a non-negative measurable function on <span class="math inline">\(E\)</span>, and <span class="math inline">\(f_n(x) \leq f(x)\)</span> for all <span class="math inline">\(n \in \mathbb{N}\)</span>. Hence, <span class="math display">\[\int_E f_n(x)\text{d}x \leq \int_E f(x)\text{d}x.\]</span> Since <span class="math display">\[\int_E f_n(x)\text{d}x \leq \int_E f_{n+1}(x)\text{d}x,\]</span> we have <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x \leq \int_E f(x)\text{d}x.\]</span></p><p>Take an arbitrary non-negative simple function <span class="math inline">\(\varphi\)</span> on <span class="math inline">\(E\)</span>, where <span class="math inline">\(0 \leq \varphi(x) \leq f(x)\)</span>. Take an arbitrary <span class="math inline">\(0&lt;c&lt;1\)</span>. Let <span class="math inline">\(E_n=E[f_n \geq c\varphi]\)</span>, which is a measurable subset of <span class="math inline">\(E\)</span>.</p><p>Since <span class="math inline">\(f_n(x) \leq f_{n+1}(x)\)</span>, then <span class="math display">\[f_n(x) \geq c\varphi(x) \Rightarrow f_{n+1}(x) \geq c\varphi(x),\]</span> i.e., <span class="math inline">\(E_n \subset E_{n+1}\)</span>.</p><p>Since <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n(x)=f(x)\)</span>, and <span class="math inline">\(c\varphi(x)&lt;f(x)\)</span>, then when <span class="math inline">\(n \gg 0\)</span>, <span class="math inline">\(c\varphi(x) \leq f_n(x)\)</span>. Hence, <span class="math display">\[\bigcup_{n=1}^\infty E_n=E.\]</span></p><p>Therefore, <span class="math display">\[\int_E \varphi(x)\text{d}x=\lim_{n \to \infty} \int_{E_n} \varphi(x)\text{d}x.\]</span> Since <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x \geq \lim_{n \to \infty} \int_{E_n} f_n(x)\text{d}x \geq \lim_{n \to \infty} \int_{E_n} c\varphi(x)\text{d}x=c\lim_{n \to \infty} \int_{E_n} \varphi(x)\text{d}x,\]</span> then <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x \geq c\int_E \varphi(x)\text{d}x.\]</span> Take <span class="math inline">\(c \to 1\)</span>, we have <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x \geq \int_E \varphi(x)\text{d}x.\]</span> Hence, <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x \geq \int_E f(x)\text{d}x.\]</span></p>Therefore, <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x=\int_E \lim_{n \to \infty} f_n(x)\text{d}x.\]</span><p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(c \in \mathbb{R} \geq 0\)</span>. When <span class="math inline">\(c&gt;0\)</span>, we have <span class="math display">\[\int_E cf(x)\text{d}x=\sup\left\{\int_E \varphi(x)\text{d}x\right\},\]</span> where <span class="math inline">\(0 \leq \varphi(x) \leq cf(x)\)</span>, and <span class="math inline">\(\varphi\)</span> is a simple function on <span class="math inline">\(E\)</span>. Let <span class="math inline">\(\displaystyle \psi(x)=\frac{1}{c}\varphi(x)\)</span>, which is a simple function on <span class="math inline">\(E\)</span>, then we have <span class="math display">\[\int_E cf(x)\text{d}x=\sup\left\{\int_E c\psi(x)\text{d}x\right\}=\sup\left\{c\int_E \psi(x)\text{d}x\right\}=c\sup\left\{\int_E \psi(x)\text{d}x\right\},\]</span> where <span class="math inline">\(0 \leq \psi(x) \leq f(x)\)</span>. Hence, <span class="math display">\[\int_E cf(x)\text{d}x=c\int_E f(x)\text{d}x.\]</span></p>
<p>When <span class="math inline">\(c=0\)</span>, it is obvious that <span class="math display">\[\int_E cf(x)\text{d}x=c\int_E f(x)\text{d}x.\]</span></p>
<p>There exist two sequences of non-negative simple function <span class="math inline">\(\{\varphi_n\}\)</span> and <span class="math inline">\(\{\psi_n\}\)</span> on <span class="math inline">\(E\)</span> such that <span class="math display">\[\lim_{n \to \infty} \varphi_n(x)=f(x), \lim_{n \to \infty} \psi_n(x)=g(x),\]</span> where for all <span class="math inline">\(x \in E\)</span>, <span class="math inline">\(\varphi_n(x) \leq \varphi_{n+1}(x) \leq f(x)\)</span>, and <span class="math inline">\(\psi_n(x) \leq \psi_{n+1}(x) \leq g(x)\)</span>. We know <span class="math inline">\(\varphi_n+\psi_n\)</span> is a non-negative simple function on <span class="math inline">\(E\)</span>, <span class="math inline">\(\varphi_n(x)+\psi_n(x) \leq \varphi_{n+1}(x)+\psi_{n+1}(x) \leq f(x)+g(x)\)</span>, and <span class="math display">\[\lim_{n \to \infty} (\varphi_n(x)+\psi_n(x))=f(x)+g(x).\]</span> By <a href="#thm2.6.1">Levi's monotonicity theorem</a>, we have <span class="math display">\[\begin{aligned}
\int_E \lim_{n \to \infty} (\varphi_n(x)+\psi_n(x))\text{d}x&amp;=\int_E (f(x)+g(x))\text{d}x
\\&amp;=\lim_{n \to \infty} \int_E (\varphi_n(x)+\psi_n(x))\text{d}x
\\&amp;=\lim_{n \to \infty} \left(\int_E \varphi_n(x)\text{d}x+\int_E \psi_n(x)\text{d}x\right)
\\&amp;=\lim_{n \to \infty} \int_E \varphi_n(x)\text{d}x+\lim_{n \to \infty} \int_E \psi_n(x)\text{d}x
\\&amp;=\int_E \lim_{n \to \infty} \varphi_n(x)\text{d}x+\int_E \lim_{n \to \infty} \psi_n(x)\text{d}x
\\&amp;=\int_E f(x)\text{d}x+\int_E g(x)\text{d}x.
\end{aligned}\]</span></p>
Hence, it is easy to show that <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.7</strong>    Suppose <span class="math inline">\(\{f_n\}_{n=1}^\infty\)</span> is a sequence of non-negative measurable functions on <span class="math inline">\(E\)</span>. Then <span class="math display">\[\int_E \left(\sum_{n=1}^\infty f_n(x)\right)\text{d}x=\sum_{n=1}^\infty \int_E f_n(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(\displaystyle g_n(x)=\sum_{k=1}^n f_k(x)\)</span>, which is a non-negative measurable function. Besides, <span class="math display">\[g_{n+1}(x)=g_n(x)+f_{n+1}(x) \geq g_n(x).\]</span> By <a href="#thm2.6.1">Levi's monotonicity theorem</a>, we have <span class="math display">\[\lim_{n \to \infty} \int_E g_n(x)\text{d}x=\int_E \lim_{n \to \infty} g_n(x)\text{d}x=\int_E \left(\sum_{n=1}^\infty f_n(x)\right)\text{d}x.\]</span> Since <span class="math display">\[\lim_{n \to \infty} \int_E g_n(x)\text{d}x=\lim_{n \to \infty} \sum_{k=1}^n \int_E f_k(x)\text{d}x=\sum_{n=1}^\infty \int_E f_n(x)\text{d}x,\]</span> then <span class="math display">\[\int_E \left(\sum_{n=1}^\infty f_n(x)\right)\text{d}x=\sum_{n=1}^\infty \int_E f_n(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm2.8"><strong>Theorem 2.8 (Fatou's Lemma)</strong></span>    Suppose <span class="math inline">\(\{f_n\}_{n=1}^\infty\)</span> is a sequence of non-negative measurable functions on <span class="math inline">\(E\)</span>. Then <span class="math display">\[\int_E \varliminf_{n \to \infty} f_n(x)\text{d}x \leq \varliminf_{n \to \infty} \int_E f_n(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(\displaystyle \left\{\inf_{k \geq n} f_k\right\}\)</span> is an increasing sequence of non-negative measurable functions on <span class="math inline">\(E\)</span>. By <a href="#thm2.6.1">Levi's monotonicity theorem</a>, we have <span class="math display">\[\int_E \varliminf_{n \to \infty} f_n(x)\text{d}x=\int_E \left(\lim_{n \to \infty} \inf_{k \geq n} f_k(x)\right)\text{d}x=\lim_{n \to \infty} \int_E \inf_{k \geq n} f_k(x)\text{d}x.\]</span> Since <span class="math display">\[\int_E \inf_{k \geq n} f_k(x)\text{d}x \leq \inf_{k \geq n} \int_E f_k(x)\text{d}x,\]</span> then <span class="math display">\[\lim_{n \to \infty} \int_E \inf_{k \geq n} f_k(x)\text{d}x \leq \lim_{n \to \infty} \inf_{k \geq n} \int_E f_k(x)\text{d}x=\varliminf_{n \to \infty} \int_E f_n(x)\text{d}x.\]</span> Hence, <span class="math display">\[\int_E \varliminf_{n \to \infty} f_n(x)\text{d}x \leq \varliminf_{n \to \infty} \int_E f_n(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>The equal sign in the <a href="#thm2.8">Fatou's lemma</a> does not always hold. For example, let <span class="math display">\[f_n(x)=\begin{cases}n, &amp;\displaystyle 0&lt;x&lt;\frac{1}{n} \\0, &amp;\displaystyle x \geq \frac{1}{n}\end{cases}.\]</span> For any <span class="math inline">\(x \in (0, \infty)\)</span>, <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n(x)=0\)</span>, and thus <span class="math display">\[\int_{(0, \infty)} \lim_{n \to \infty} f_n(x)\text{d}x=0.\]</span> However, <span class="math display">\[\int_{(0, \infty)} f_n(x)\text{d}x=\int_{\left(0, \frac{1}{n}\right)} f_n(x)\text{d}x+\int_{\left[\frac{1}{n}, \infty\right)} f_n(x)\text{d}x=1,\]</span> and thus <span class="math display">\[\lim_{n \to \infty} \int_{(0, \infty)} f_n(x)\text{d}x=1.\]</span></p>
          </div>
<h2 id="zero-measure-set-and-integral">2.3. Zero Measure Set and Integral</h2>
<p><strong>Theorem 2.9</strong>    Suppose <span class="math inline">\(E\)</span> is a zero measure set. Then <span class="math display">\[\int_E f(x)\text{d}x=0.\]</span></p>
<strong><em>Proof.</em></strong> By definition, <span class="math display">\[\int_E f(x)\text{d}x=\sup\left\{\int_E \varphi(x)\text{d}x\right\},\]</span> where <span class="math inline">\(\varphi\)</span> is a non-negative simple function on <span class="math inline">\(E\)</span>, and <span class="math inline">\(\varphi(x) \leq f(x)\)</span>. Since <span class="math display">\[\int_E \varphi(x)\text{d}x=0,\]</span> then <span class="math display">\[\int_E f(x)\text{d}x=0.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.10</strong>    If <span class="math inline">\(f(x) \leq g(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\int_E f(x)\text{d}x \leq \int_E g(x)\text{d}x.\]</span></p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(E_1=E[f \leq g]\)</span> and <span class="math inline">\(E_2=E[f&gt;g]\)</span>, where <span class="math inline">\(m(E_2)=0\)</span>, then <span class="math inline">\(E=E_1 \cup E_2\)</span>, and <span class="math inline">\(E_1 \cap E_2=\varnothing\)</span>. Hence, <span class="math display">\[\begin{aligned}
\int_E f(x)\text{d}x&amp;=\int_{E_1} f(x)\text{d}x+\int_{E_2} f(x)\text{d}x
\\&amp; \leq \int_{E_1} g(x)\text{d}x+0
\\&amp;=\int_{E_1} g(x)\text{d}x+\int_{E_2} g(x)\text{d}x
\\&amp;=\int_E g(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 2.11</strong>    If <span class="math inline">\(f(x)=g(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\int_E f(x)\text{d}x=\int_E g(x)\text{d}x.\]</span> Moreover, if <span class="math inline">\(f(x)=0\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\int_E f(x)\text{d}x=0.\]</span></p>
<p><strong>Theorem 2.12</strong>    If <span class="math display">\[\int_E f(x)\text{d}x=0,\]</span> then <span class="math inline">\(f(x)=0\ \text{a.e.}\ x \in E\)</span>.</p>
<strong><em>Proof.</em></strong> Since <span class="math display">\[0=\int_E f(x)\text{d}x \geq \int_{E\left[f \geq \frac{1}{n}\right]} f(x)\text{d}x \geq \int_{E\left[f \geq \frac{1}{n}\right]} \frac{1}{n}\text{d}x=\frac{1}{n} \cdot m\left(E\left[f \geq \frac{1}{n}\right]\right),\]</span> then <span class="math inline">\(\displaystyle m\left(E\left[f \geq \frac{1}{n}\right]\right)=0\)</span>. Hence, <span class="math display">\[m(E[f \neq 0])=m\left(\bigcup_{n=1}^\infty E\left[f \geq \frac{1}{n}\right]\right) \leq \sum_{n=1}^\infty m\left(E\left[f \geq \frac{1}{n}\right]\right)=0.\]</span> Therefore, <span class="math inline">\(m(E[f \geq 0])=0\)</span>, i.e., <span class="math inline">\(f(x)=0\ \text{a.e.}\ x \in E\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="lebesgue-integral-of-measurable-function">3. Lebesgue Integral of Measurable Function</h1>
<p><strong>Definition 3.1</strong>    Suppose <span class="math inline">\(E\)</span> is measurable, and <span class="math inline">\(f\)</span> is a measurable function on <span class="math inline">\(E\)</span>. When at least one of <span class="math inline">\(\displaystyle \int_E f^+(x)\text{d}x\)</span> and <span class="math inline">\(\displaystyle \int_E f^-(x)\text{d}x\)</span> is finite, the <strong>Lebesgue integral</strong> of <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span> is <span class="math display">\[\int_E f(x)\text{d}x=\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x.\]</span> If <span class="math inline">\(\displaystyle \int_E f^+(x)\text{d}x\)</span> and <span class="math inline">\(\displaystyle \int_E f^-(x)\text{d}x\)</span> are finite, then <span class="math inline">\(f\)</span> is <strong>Lebesgue integrable</strong> on <span class="math inline">\(E\)</span>. The collection of all Lebesgue integrable functions on <span class="math inline">\(E\)</span> is denoted <span class="math inline">\(L(E)\)</span>.</p>
<p><strong>Theorem 3.1</strong>    Suppose <span class="math inline">\(E \neq \varnothing\)</span> and <span class="math inline">\(m(E)=0\)</span>. Then any real function <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span> is Lebesgue integrable, and <span class="math display">\[\int_E f(x)\text{d}x=0.\]</span></p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(m(E)=0\)</span>, and <span class="math inline">\(f^+\)</span> and <span class="math inline">\(f^-\)</span> are non-negative measurable on <span class="math inline">\(E\)</span>, then <span class="math display">\[\int_E f^+(x)\text{d}x=\int_E f^-(x)\text{d}x=0.\]</span> Hence, <span class="math inline">\(f \in L(E)\)</span>, and <span class="math display">\[\int_E f(x)\text{d}x=\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x=0.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.2</strong>    Suppose <span class="math inline">\(\displaystyle E=\bigcup_{n=1}^\infty E_n\)</span>, and <span class="math inline">\(\{E_n\}\)</span> is a collection of disjoint measurable subsets. If the Lebesgue integral of <span class="math inline">\(f\)</span> on <span class="math inline">\(E\)</span> exists, then <span class="math display">\[\int_E f(x)\text{d}x=\sum_{n=1}^\infty \int_{E_n} f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(f\)</span> is a non-negative measurable function. We have <span class="math display">\[\int_{E_n} f(x)\text{d}x=\int_E f(x) \cdot \chi_{E_n}(x)\text{d}x\]</span> and thus <span class="math display">\[\sum_{n=1}^\infty \int_{E_n} f(x)\text{d}x=\sum_{n=1}^\infty \int_E f(x) \cdot \chi_{E_n}(x)\text{d}x=\int_E \sum_{n=1}^\infty f(x) \cdot \chi_{E_n}(x)\text{d}x=\int_E f(x)\text{d}x.\]</span></p>
Suppose <span class="math inline">\(f\)</span> is a measurable function. We know at least one of <span class="math inline">\(\displaystyle \sum_{n=1}^\infty \int_{E_n} f^+(x)\text{d}x\)</span> and <span class="math inline">\(\displaystyle \sum_{n=1}^\infty \int_{E_n} f^-(x)\text{d}x\)</span> is convergent. Hence, <span class="math display">\[\begin{aligned}
\int_E f(x)\text{d}x&amp;=\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x
\\&amp;=\left(\sum_{n=1}^\infty \int_{E_n} f^+(x)\text{d}x\right)-\left(\sum_{n=1}^\infty \int_{E_n} f^-(x)\text{d}x\right)
\\&amp;=\sum_{n=1}^\infty \left(\int_{E_n} f^+(x)\text{d}x-\int_{E_n} f^-(x)\text{d}x\right)
\\&amp;=\sum_{n=1}^\infty \int_{E_n} f(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.3</strong>    <span class="math inline">\(f \in L(E) \Leftrightarrow |f| \in L(E)\)</span>.</p>
<p><strong><em>Proof.</em></strong> (<span class="math inline">\(\Leftarrow\)</span>) Suppose <span class="math inline">\(f \in L(E)\)</span>. Then <span class="math inline">\(\displaystyle \int_E f^+(x)\text{d}x&lt;\infty\)</span> and <span class="math inline">\(\displaystyle \int_E f^-(x)\text{d}x\)</span>, and thus <span class="math display">\[\int_E |f(x)|\text{d}x=\int_E (f^+(x)+f^-(x))\text{d}x=\int_E f^+(x)\text{d}x+\int_E f^-(x)\text{d}x&lt;\infty,\]</span> i.e., <span class="math inline">\(|f| \in L(E)\)</span>.</p>
(<span class="math inline">\(\Rightarrow\)</span>) Suppose <span class="math inline">\(|f| \in L(E)\)</span>. Then <span class="math inline">\(f^+(x) \leq |f(x)|\)</span> and <span class="math inline">\(f^-(x) \leq |f(x)|\)</span>. Hence, <span class="math display">\[\int_E f^+(x)\text{d}x \leq \int_E |f(x)|\text{d}x&lt;\infty\]</span> and <span class="math display">\[\int_E f^-(x)\text{d}x \leq \int_E |f(x)|\text{d}x&lt;\infty.\]</span> Therefore, <span class="math inline">\(f \in L(E)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.4</strong>    Suppose <span class="math inline">\(|f(x)| \leq g(x)\ \text{a.e.}\ x \in E\)</span>, and <span class="math inline">\(g\)</span> is a non-negative Lebesgue integrable function on <span class="math inline">\(E\)</span>. Then <span class="math inline">\(f \in L(E)\)</span>, and <span class="math display">\[\left|\int_E f(x)\text{d}x\right| \leq \int_E |f(x)|\text{d}x \leq \int_E g(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(|f(x)| \leq g(x)\ \text{a.e.}\ x \in E\)</span>, <span class="math inline">\(|f|\)</span> and <span class="math inline">\(g\)</span> are non-negative measurable functions on <span class="math inline">\(E\)</span>, and <span class="math inline">\(g\)</span> is Lebesgue integrable, then <span class="math display">\[\int_E |f(x)|\text{d}x \leq \int_E g(x)\text{d}x&lt;\infty,\]</span> i.e., <span class="math inline">\(|f| \in L(E)\)</span>. Hence, <span class="math inline">\(f \in L(E)\)</span>.</p>
In addition, <span class="math display">\[\begin{aligned}
\left|\int_E f(x)\text{d}x\right|&amp;=\left|\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x\right|
\\&amp;\leq \left|\int_E f^+(x)\text{d}x\right|+\left|\int_E f^-(x)\text{d}x\right|
\\&amp;=\int_E f^+(x)\text{d}x+\int_E f^-(x)\text{d}x
\\&amp;=\int_E (f^+(x)+f^-(x))\text{d}x
\\&amp;=\int_E |f(x)|\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.5</strong>    Suppose <span class="math inline">\(f \in L(E)\)</span>, and <span class="math inline">\(A\)</span> is a measurable subset of <span class="math inline">\(E\)</span>, then <span class="math display">\[\forall \varepsilon&gt;0, \exists \delta&gt;0\ \text{s.t.}\ m(A)&lt;\delta \Rightarrow \left|\int_A f(x)\text{d}x\right| \leq \int_A |f(x)|\text{d}x&lt;\varepsilon.\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>. Suppose <span class="math inline">\(f\)</span> is a non-negative simple function. We know <span class="math inline">\(f(x) \leq M\)</span>, for all <span class="math inline">\(x \in A\)</span>. Take <span class="math inline">\(\displaystyle \delta=\frac{\varepsilon}{M+1}\)</span>, when <span class="math inline">\(m(A)&lt;\delta\)</span>, we have <span class="math display">\[\int_A f(x)\text{d}x \leq M \cdot m(A)&lt;\varepsilon.\]</span></p>
<p>Suppose <span class="math inline">\(f\)</span> is a measurable function, then <span class="math inline">\(|f|\)</span> is a non-negative measurable function. There exists a non-negative simple function such that <span class="math inline">\(\varphi(x) \leq |f(x)|\)</span> and <span class="math display">\[\int_E \varphi(x)\text{d}x \leq \int_E |f(x)|\text{d}x \leq \int_E \varphi(x)+\frac{\varepsilon}{2}.\]</span></p>
We take a <span class="math inline">\(\delta\)</span> such that when <span class="math inline">\(m(A)&lt;\delta\)</span>, <span class="math inline">\(\displaystyle \int_A \varphi(x)\text{d}x&lt;\frac{\varepsilon}{2}\)</span>. Hence, <span class="math display">\[\begin{aligned}
\int_A |f(x)|\text{d}x&amp;=\int_A \varphi(x)\text{d}x+\int_A (|f(x)|-\varphi(x))\text{d}x
\\&amp;\leq \int_A \varphi(x)\text{d}x+\int_E(|f(x)|-\varphi(x))\text{d}x
\\&amp;=\int_A \varphi(x)\text{d}x+\int_E |f(x)|\text{d}x-\int_E \varphi(x)\text{d}x
\\&amp;&lt;\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.6</strong>    Suppose <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are Lebesgue integrable functions, and <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are real numbers. Then <span class="math inline">\(\alpha f+\beta g \in L(E)\)</span>, and <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(c \in \mathbb{R}\)</span>. When <span class="math inline">\(c=0\)</span>, it is obvious that <span class="math inline">\(cf \in L(E)\)</span>, and <span class="math display">\[\int_E cf(x)\text{d}x=c\int_E f(x)\text{d}x.\]</span></p>
<p>When <span class="math inline">\(c&gt;0\)</span>, <span class="math inline">\((cf)^+=cf^+\)</span> and <span class="math inline">\((cf)^-=cf^-\)</span>.</p>
<p>Since <span class="math inline">\(f \in L(E)\)</span>, then <span class="math inline">\(\displaystyle \int_E f^+(x)\text{d}x&lt;\infty\)</span> and <span class="math inline">\(\displaystyle \int_E f^-(x)\text{d}x&lt;\infty\)</span>, and thus <span class="math display">\[\int_E (cf)^+(x)\text{d}x=\int_E cf^+(x)\text{d}x=c\int_E f^+(x)\text{d}x&lt;\infty\]</span> and <span class="math display">\[\int_E (cf)^-(x)\text{d}x=\int_E cf^-(x)\text{d}x=c\int_E f^-(x)\text{d}x&lt;\infty.\]</span> Therefore, <span class="math inline">\(cf(x) \in L(E)\)</span>, and <span class="math display">\[\begin{aligned}
\int_E cf(x)\text{d}x&amp;=\int_E (cf)^+(x)\text{d}x-\int_E (cf)^-(x)\text{d}x
\\&amp;=c\int_E f^+(x)\text{d}x-c\int_E f^-(x)\text{d}x
\\&amp;=c\left(\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x\right)
\\&amp;=c\int_E f(x)\text{d}x.
\end{aligned}\]</span></p>
<p>When <span class="math inline">\(c&lt;0\)</span>, <span class="math inline">\((cf)^+=|c|f^-\)</span> and <span class="math inline">\((cf)^-=|c|f^+\)</span>. Similarly, we can show that <span class="math inline">\(cf \in L(E)\)</span>, and <span class="math display">\[\int_E cf(x)\text{d}x=c\int_E f(x)\text{d}x.\]</span></p>
<p>In addition, we know <span class="math inline">\(|f|\)</span> and <span class="math inline">\(|g|\)</span> are Lebesgue integrable, and <span class="math display">\[\int_E(|f|+|g|)(x)\text{d}x=\int_E |f|(x)\text{d}x+\int_E |g|(x)\text{d}x&lt;\infty,\]</span> i.e., <span class="math inline">\(|f|+|g| \in L(E)\)</span>. Since <span class="math inline">\(|f+g| \leq |f|+|g|\)</span>, then <span class="math inline">\(|f+g| \in L(E)\)</span>.</p>
<p>Since <span class="math inline">\(f+g=(f+g)^+-(f+g)^-=f^+-f^-+g^+-g^-\)</span>, then <span class="math display">\[(f+g)^++f^-+g^-=(f+g)^-+f^++g^+.\]</span> Hence, <span class="math display">\[\int_E ((f+g)^+(x)+f^-(x)+g^-(x))\text{d}x=\int_E ((f+g)^-(x)+f^+(x)+g^+(x))\text{d}x,\]</span> and thus <span class="math display">\[\int_E (f+g)^+(x)\text{d}x+\int_E f^-(x)\text{d}x+\int_E g^-(x)\text{d}x=\int_E (f+g)^-(x)\text{d}x+\int_E f^+(x)\text{d}x+\int_E g^+(x)\text{d}x.\]</span> Therefore, <span class="math display">\[\int_E (f+g)^+(x)\text{d}x-\int_E (f+g)^-(x)\text{d}x=\int_E f^+(x)\text{d}x-\int_E f^-(x)\text{d}x+\int_E g^+(x)\text{d}x-\int_E g^-(x)\text{d}x,\]</span> i.e., <span class="math display">\[\int_E (f(x)+g(x))\text{d}x=\int_E f(x)\text{d}x+\int_E g(x)\text{d}x.\]</span></p>
Hence, it is easy to show that <span class="math inline">\(\alpha f+\beta g \in L(E)\)</span>, and <span class="math display">\[\int_E (\alpha f(x)+\beta g(x))\text{d}x=\alpha\int_E f(x)\text{d}x+\beta\int_E g(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="lebesgues-dominated-convergence-theorem">3.1. Lebesgue's Dominated Convergence Theorem</h2>
<p><span id="thm3.7"><strong>Theorem 3.7 (Lebesgue's Dominated Convergence Theorem, DCT)</strong></span>    Suppose <span class="math inline">\(\{f_n\}\)</span> is a sequence of measurable functions on <span class="math inline">\(E\)</span>, and <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n(x)=f(x)\ \text{a.e.}\ x \in E\)</span>. If there exists a non-negative Lebesgue integrable function <span class="math inline">\(F\)</span> on <span class="math inline">\(E\)</span> such that for all <span class="math inline">\(n \in \mathbb{N}\)</span>, <span class="math inline">\(|f_n(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x=\int_E f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(|f_n(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math inline">\(|f(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>. Since <span class="math inline">\(F\)</span> is Lebesgue integrable, then <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are Lebesgue integrable, and thus <span class="math inline">\(F\)</span>, <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are finite almost everywhere.</p>
<p>Assume without loss of generality that <span class="math inline">\(F\)</span>, <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are finite. Let <span class="math inline">\(g_n(x)=|f_n(x)-f(x)| \geq 0\)</span>. We know <span class="math inline">\(g_n(x) \leq |f_n(x)|+|f(x)| \leq 2F(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math inline">\(2F-g_n\)</span> is a non-negative measurable function.</p>
By <a href="#thm2.8">Fatou's lemma</a>, <span class="math display">\[\int_E \varliminf_{n \to \infty} (2F(x)-g_n(x))\text{d}x \leq \varliminf_{n \to \infty} \int_E (2F(x)-g_n(x))\text{d}x.\]</span> Hence, <span class="math display">\[\int_E 2F(x)\text{d}x-\int_E \varlimsup_{n \to \infty} g_n(x)\text{d}x \leq \int_E 2F(x)\text{d}x-\varlimsup_{n \to \infty} \int_E g_n(x)\text{d}x,\]</span> and thus <span class="math display">\[\varlimsup_{n \to \infty} \int_E g_n(x)\text{d}x \leq \int_E \varlimsup_{n \to \infty} g_n(x)\text{d}x.\]</span> Since <span class="math inline">\(\displaystyle \varlimsup_{n \to \infty} g_n(x)\text{d}x=0\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\varlimsup_{n \to \infty} \int_E g_n(x)\text{d}x=0,\]</span> and thus <span class="math display">\[\lim_{n \to \infty} \int_E g_n(x)\text{d}x=\lim_{n \to \infty} \int_E |f_n(x)-f(x)|\text{d}x=0.\]</span> Since <span class="math display">\[\left|\int_E (f_n(x)-f(x))\text{d}x\right| \leq \int_E |f_n(x)-f(x)|\text{d}x,\]</span> then <span class="math display">\[\lim_{n \to \infty} \int_E (f_n(x)-f(x))\text{d}x=0,\]</span> i.e., <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x=\int_E f(x)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><a href="#thm2.6.1">Levi's monotonicity theorem</a>, <a href="#thm2.8">Fatou's lemma</a> and <a href="#thm3.7">Lebesgue's dominated convergence theorem</a> are equivalent to each other.</p>
          </div>
<p><strong>Theorem 3.8</strong>    Suppose <span class="math inline">\(\{f_n\}\)</span> is a sequence of measurable functions on <span class="math inline">\(E\)</span>, and <span class="math inline">\(f_n \Rightarrow f\)</span>. If there exists a non-negative Lebesgue integrable function <span class="math inline">\(F\)</span> on <span class="math inline">\(E\)</span> such that for all <span class="math inline">\(n \in \mathbb{N}\)</span>, <span class="math inline">\(|f_n(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math display">\[\lim_{n \to \infty} \int_E f_n(x)\text{d}x=\int_E f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(|f_n(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>, then <span class="math inline">\(|f(x)| \leq F(x)\ \text{a.e.}\ x \in E\)</span>. Since <span class="math inline">\(F\)</span> is Lebesgue integrable, then <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are Lebesgue integrable, and thus <span class="math inline">\(F\)</span>, <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are finite almost everywhere.</p>
<p>Assume without loss of generality that <span class="math inline">\(F\)</span>, <span class="math inline">\(f\)</span> and <span class="math inline">\(f_n\)</span> are finite. Suppose <span class="math display">\[\int_E |f_n(x)-f(x)|\text{d}x \not\to0,\]</span> then we can find a subsequence <span class="math inline">\(\{f_{n_k}\}_{k=1}^\infty\)</span> such that <span class="math inline">\(\displaystyle \lim_{k \to \infty} \int_E |f_{n_k}(x)-f(x)|\text{d}x=\alpha&gt;0\)</span>.</p>
By Riesz's theorem, there exists a subsequence <span class="math inline">\(\{f_{n_{k_i}}\}_{i=1}^\infty\)</span> such that <span class="math inline">\(f_{n_{k_i}}(x) \to f(x)\ \text{a.e.}\ x \in E\)</span>, and thus <span class="math display">\[\lim_{i \to \infty} \int_E |f_{n_{k_i}}(x)-f(x)|\text{d}x=0,\]</span> which is a contradiction.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.9</strong>    Suppose <span class="math inline">\(\{f_n\}\)</span> is a sequence of Lebesgue integrable functions on <span class="math inline">\(E\)</span>. If <span class="math display">\[\sum_{n=1}^\infty \int_E |f_n(x)|\text{d}x&lt;\infty,\]</span> then <span class="math inline">\(\displaystyle \sum_{n=1}^\infty f_n(x)&lt;\infty\ \text{a.e.}\ x \in E\)</span>, <span class="math inline">\(\displaystyle \sum_{n=1}^\infty f_n\)</span> is Lebesgue integrable on <span class="math inline">\(E\)</span>, and <span class="math display">\[\int_E \left(\sum_{n=1}^\infty f_n(x)\right)\text{d}x=\sum_{n=1}^\infty \int_E f_n(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Let <span class="math display">\[F(x)=\sum_{n=1}^\infty |f_n(x)|.\]</span> We have <span class="math display">\[\int_E F(x)\text{d}x=\sum_{n=1}^\infty \int_E |f_n(x)|\text{d}x&lt;\infty.\]</span> Hence, <span class="math inline">\(F\)</span> is a non-negative Lebesgue integrable function on <span class="math inline">\(E\)</span>, and thus <span class="math inline">\(F\)</span> is finite almost everywhere on <span class="math inline">\(E\)</span>, i.e., <span class="math inline">\(\displaystyle \sum_{n=1}^\infty |f_n(x)|&lt;\infty\ \text{a.e.}\ x \in E\)</span>. Therefore, <span class="math inline">\(\displaystyle \sum_{n=1}^\infty f_n(x)&lt;\infty\ \text{a.e.}\ x \in E\)</span>.</p>
Let <span class="math inline">\(\displaystyle g_n(x)=\sum_{k=1}^n f_k(x)\)</span> and <span class="math inline">\(\displaystyle g(x)=\sum_{n=1}^\infty f_n(x)=\lim_{n \to \infty} g_n(x)\)</span>. We have <span class="math display">\[|g_n(x)|=\left|\sum_{k=1}^n f_k(x)\right| \leq \sum_{k=1}^n |f_k(x)| \leq F(x).\]</span> By <a href="#thm3.7">Lebesgue's dominated convergence theorem</a>, <span class="math inline">\(g \in L(E)\)</span>, and <span class="math display">\[\lim_{n \to \infty} \int_E g_n(x)\text{d}x=\int_E g(x)\text{d}x,\]</span> i.e., <span class="math inline">\(\displaystyle \sum_{n=1}^\infty f_n\)</span> is Lebesgue integrable on <span class="math inline">\(E\)</span>, and <span class="math display">\[\sum_{n=1}^\infty \int_E f_n(x)\text{d}x=\lim_{n \to \infty} \sum_{k=1}^n \int_E f_k(x)\text{d}x=\lim_{n \to \infty} \int_E \sum_{k=1}^n f_k(x)\text{d}x=\int_E \left(\sum_{n=1}^\infty f_n(x)\right)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.10</strong>    Suppose <span class="math inline">\(f\)</span> is a real function on <span class="math inline">\(E \times (a, b)\)</span>. If for all <span class="math inline">\(t \in (a, b)\)</span>, <span class="math inline">\(f(x, t)\)</span> (as a function of <span class="math inline">\(x\)</span>) is Lebesgue integrable on <span class="math inline">\(E\)</span>, for <span class="math inline">\(\text{a.e.}\ x \in E\)</span>, <span class="math inline">\(f(x, t)\)</span> (as a function of <span class="math inline">\(t\)</span>) is differentiable on <span class="math inline">\((a, b)\)</span>, and there exists a non-negative Lebesgue integrable function <span class="math inline">\(F\)</span> on <span class="math inline">\(E\)</span> such that <span class="math display">\[\left|\frac{\partial}{\partial t}f(x, t)\right| \leq F(x),\]</span> then <span class="math inline">\(\displaystyle \frac{\partial}{\partial t}f(x, t)\)</span> is Lebesgue integrable on <span class="math inline">\(E\)</span>, <span class="math inline">\(\displaystyle \int_E f(x, t)\text{d}x\)</span> is differentiable on <span class="math inline">\((a, b)\)</span>, and <span class="math display">\[\frac{\text{d}}{\text{d}t}\int_E f(x, t)\text{d}x=\int_E \frac{\partial}{\partial t}f(x, t)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Fix a <span class="math inline">\(t \in (a, b)\)</span> and take an arbitrary sequence <span class="math inline">\(\{h_n\}\)</span>, where <span class="math inline">\(h_n \to 0\)</span> and <span class="math inline">\(h_n \neq 0\)</span>. Let <span class="math display">\[g_n(x)=\frac{f(x, t+h_n)-f(x, t)}{h_n},\]</span> then <span class="math display">\[\lim_{n \to \infty} g_n(x)=\frac{\partial}{\partial t}f(x, t),\]</span> and <span class="math display">\[|g_n(x)|=\left|\frac{\partial}{\partial t}f(x, t+\theta_nh_n)\right| \leq F(x),\]</span> where <span class="math inline">\(0 \leq \theta_n \leq 1\)</span>.</p>
Since <span class="math inline">\(\displaystyle \frac{\partial}{\partial t}f(x, t)\)</span> is Lebesgue integrable on <span class="math inline">\(E\)</span>, and by <a href="#thm3.7">Lebesgue's dominated convergence theorem</a>, <span class="math display">\[\lim_{n \to \infty} \frac{\displaystyle \int_E f(x, t+h_n)\text{d}x-\int_E f(x, t)\text{d}x}{h_n}=\lim_{n \to \infty} \int_E g_n(x)\text{d}x=\int_E \lim_{n \to \infty} g_n(x)\text{d}x=\int_E \frac{\partial}{\partial t}f(x, t)\text{d}x&lt;\infty.\]</span> Therefore, <span class="math inline">\(\displaystyle \int_E f(x, t)\text{d}t\)</span> is differentiable on <span class="math inline">\((a, b)\)</span>, and <span class="math display">\[\frac{\text{d}}{\text{d}t}\int_E f(x, t)\text{d}x=\int_E \frac{\partial}{\partial t}f(x, t)\text{d}x.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="riemann-integral-and-lebesgue-integral">4. Riemann Integral and Lebesgue Integral</h1>
<div class="note info">
            <p><strong>Recall 4.1</strong>    Suppose <span class="math inline">\(f\)</span> is bounded on <span class="math inline">\([a, b]\)</span>. A <strong>partition sequence</strong> of <span class="math inline">\([a, b]\)</span>, <span class="math inline">\(\{P^{(n)}\}\)</span> is a finite sequence of values <span class="math inline">\(x_i\)</span> such that <span class="math display">\[a=x_0^{(n)}&lt;x_1^{(n)}&lt;\cdots&lt;x_{k_n}^{(n)}=b.\]</span> Let <span class="math inline">\(M_i^{(n)}=\sup\{f(x): x_{i-1}^{(n)} \leq x \leq x_i^{(n)}\}\)</span> and <span class="math inline">\(m_i^{(n)}=\inf\{f(x): x_{i-1}^{(n)} \leq x \leq x_i^{(n)}\}\)</span>. The <strong>upper Darboux sum</strong> of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P^{(n)}\)</span> is <span class="math display">\[S(f, P^{(n)})=\sum_{i=1}^{k_n} M_i^{(n)}(x_i^{(n)}-x_{i-1}^{(n)}).\]</span> The <strong>lower Darboux sum</strong> of <span class="math inline">\(f\)</span> with respect to <span class="math inline">\(P^{(n)}\)</span> is <span class="math display">\[s(f, P^{(n)})=\sum_{i=1}^{k_n} m_i^{(n)}(x_i^{(n)}-x_{i-1}^{(n)}).\]</span> The <strong>upper Darboux integral</strong> of <span class="math inline">\(f\)</span> is <span class="math display">\[\overline{\int_a^b} f(x)\text{d}x=\inf_{P^{(n)}} S(f, P^{(n)})=\lim_{n \to \infty} S(f, P^{(n)}).\]</span> The <strong>lower Darboux integral</strong> of <span class="math inline">\(f\)</span> is <span class="math display">\[\underline{\int_a^b} f(x)\text{d}x=\sup_{P^{(n)}} s(f, P^{(n)})=\lim_{n \to \infty} s(f, P^{(n)}).\]</span> <span class="math inline">\(f(x)\)</span> is <strong>Riemann integrable</strong> on <span class="math inline">\([a, b]\)</span> if and only if <span class="math display">\[\overline{\int_a^b} f(x)\text{d}x=\underline{\int_a^b} f(x)\text{d}x.\]</span></p><p><strong>Recall 4.2</strong>    The <strong>amplitude</strong> at <span class="math inline">\(x\)</span> is <span class="math display">\[\omega(x)=\lim_{\delta \to 0^+}\sup\{|f(y)-f(z)|: y, z \in (x-\delta, x+\delta) \cap [a, b]\}.\]</span></p>
          </div>
<p><strong>Theorem 4.1</strong>    A bounded function <span class="math inline">\(f\)</span> is Riemann integrable on <span class="math inline">\([a, b]\)</span> if and only if <span class="math inline">\(f\)</span> is continuous almost everywhere on <span class="math inline">\([a, b]\)</span>.</p>
<p><strong><em>Proof.</em></strong> Let <span class="math display">\[h_n(x)=\begin{cases}
M_i^{(n)}-m_i^{(n)}, &amp;x_{i-1}^{(n)}&lt;x&lt;x_i^{(n)} \\
0, &amp;x=x_{i-1}^{(n)} \vee x=x_i^{(n)}
\end{cases}.\]</span> We know <span class="math inline">\(h_n\)</span> is a non-negative measurable function. Suppose <span class="math inline">\(|f(x)| \leq M\)</span>, <span class="math inline">\(x \in [a, b]\)</span>, then <span class="math display">\[|h_n(x)| \leq M_i^{(n)}+m_i^{(n)} \leq 2M.\]</span> Since <span class="math inline">\(g(x)=2M\)</span> is Lebesgue integrable on <span class="math inline">\([a, b]\)</span>, then <span class="math display">\[\begin{aligned}
\overline{\int_a^b} f(x)\text{d}x-\underline{\int_a^b} f(x)\text{d}x&amp;=\lim_{n \to \infty} (S(f, P^{(n)})-s(f, P^{(n)}))
\\&amp;=\lim_{n \to \infty} \sum_{i=1}^{k_n} (M_i^{(n)}-m_i^{(n)})(x_i^{(n)}-x_{i-1}^{(n)})
\\&amp;=\lim_{n \to \infty} \int_{[a, b]} h_n(x)\text{d}x
\\&amp;=\int_{[a, b]} \lim_{n \to \infty} h_n(x)\text{d}x &amp; \text{(DCT)}
\\&amp;=\int_{[a, b]} \omega(x)\text{d}x=0.
\end{aligned}\]</span></p>
Therefore, <span class="math inline">\(f\)</span> is Riemann integrable on <span class="math inline">\([a, b]\)</span> if and only if <span class="math display">\[\begin{aligned}
&amp;\quad \quad \int_{[a, b]} \omega(x)\text{d}x=0
\\&amp;\Leftrightarrow \omega(x)=0\ \text{a.e.}\ x \in [a, b]
\\&amp;\Leftrightarrow \text{$f$ is continuous almost everywhere on $[a, b]$.}
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.2</strong>    If a bounded function <span class="math inline">\(f\)</span> is Riemann integrable on <span class="math inline">\([a, b]\)</span>, then <span class="math inline">\(f\)</span> is Lebesgue integrable on <span class="math inline">\([a, b]\)</span>, and <span class="math display">\[\int_{[a, b]} f(x)\text{d}x=\int_a^b f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(f\)</span> is Riemann integrable on <span class="math inline">\([a, b]\)</span>, then <span class="math inline">\(f\)</span> is continuous almost everywhere on <span class="math inline">\([a, b]\)</span>. Hence, <span class="math inline">\(f\)</span> is a bounded measurable function on <span class="math inline">\([a, b]\)</span>, and thus <span class="math inline">\(f \in L([a, b])\)</span>.</p>
Let <span class="math display">\[g_n(x)=\begin{cases}
M_i^{(n)}, &amp;x_{i-1}^{(n)}&lt;x&lt;x_i^{(n)} \\
0, &amp;x=x_{i-1}^{(n)} \vee x=x_i^{(n)}
\end{cases},\]</span> where <span class="math inline">\(\displaystyle \lim_{n \to \infty} g_n(x)=f(x)\ \text{a.e.}\ x \in [a, b]\)</span>, and <span class="math inline">\(g_n\)</span> is bounded. We have <span class="math display">\[\begin{aligned}
\int_a^b f(x)\text{d}x&amp;=\lim_{n \to \infty} \sum_{i=1}^{k_n} M_i^{(n)}(x_i^{(n)}-x_{i-1}^{(n)})
\\&amp;=\lim_{n \to \infty} \int_{[a, b]} g_n(x)\text{d}x
\\&amp;=\int_{[a, b]} \lim_{n \to \infty} g_n(x)\text{d}x
\\&amp;=\int_{[a, b]} f(x)\text{d}x.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.4</strong>    Suppose <span class="math inline">\(f\)</span> is a non-negative real function on <span class="math inline">\([a, b]\)</span>. If for all <span class="math inline">\(A&gt;a\)</span>, <span class="math inline">\(f\)</span> is Riemann integrable on <span class="math inline">\([a, A]\)</span>, and <span class="math inline">\(\displaystyle \int_a^{\infty} f(x)\text{d}x&lt;\infty\)</span>, then <span class="math inline">\(f\)</span> is Lebesgue integrable on <span class="math inline">\([a, \infty)\)</span>, and <span class="math display">\[\int_{[a, \infty)} f(x)\text{d}x=\int_a^\infty f(x)\text{d}x.\]</span></p>
<p><strong><em>Proof.</em></strong> Take an arbitrary sequence <span class="math inline">\(\{A_n\}\)</span> such that <span class="math inline">\(A_n&gt;a\)</span> and <span class="math inline">\(\displaystyle \lim_{n \to \infty} A_n=\infty\)</span>. Hence, <span class="math display">\[\int_{[a, \infty)} f(x)\text{d}x=\lim_{n \to \infty} \int_{[a, A_n]} f(x)\text{d}x=\lim_{n \to \infty}\int_a^{A_n} f(x)\text{d}x=\int_a^\infty f(x)\text{d}x.\]</span></p>
In addition, if <span class="math inline">\(\displaystyle \int_a^\infty f(x)\text{d}x\)</span> diverges, then <span class="math inline">\(\displaystyle \int_{[a, \infty)} f(x)\text{d}x=\infty\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>If <span class="math inline">\(f\)</span> is not non-negative function, then Lebesgue integral <strong>is not</strong> a generalization of Riemann improper integral.</p>
          </div>
<h1 id="geometric-interpretation-of-lebesgue-integral">5. Geometric Interpretation of Lebesgue Integral</h1>
<p><strong>Definition 5.1</strong>    Suppose <span class="math inline">\(f\)</span> is a non-negative function on <span class="math inline">\(E \subset \mathbb{R}^n\)</span>. We define <span class="math display">\[G(E, f)=\{(x, z): x \in E, 0 \leq z&lt;f(x)\} \subset \mathbb{R}^{n+1}.\]</span></p>
<p><strong>Theorem 5.1</strong>    Suppose <span class="math inline">\(f\)</span> is a non-negative function on <span class="math inline">\(E \subset \mathbb{R}^n\)</span>, then</p>
<p>    (1) <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E\)</span> if and only if <span class="math inline">\(G(E, f)\)</span> is measurable;</p>
<p>    (2) If <span class="math inline">\(f\)</span> is measurable on <span class="math inline">\(E\)</span>, then <span class="math display">\[\int_E f(x)\text{d}x=m(G(E, f)).\]</span></p>
<h1 id="fubinis-theorem">6. Fubini's Theorem</h1>
<p><span id="thm6.1"><strong>Theorem 6.1 (Fubini's Theorem)</strong></span>    Suppose <span class="math inline">\(A \subset \mathbb{R}^p\)</span> and <span class="math inline">\(B \subset \mathbb{R}^q\)</span> are measurable. If <span class="math inline">\(f\)</span> is a non-negative measurable function on <span class="math inline">\(A \times B\)</span>, then for <span class="math inline">\(\text{a.e.} x \in A\)</span>, <span class="math inline">\(f(x, y)\)</span> (as a function of <span class="math inline">\(y\)</span>) is measurable on <span class="math inline">\(B\)</span>, and <span class="math display">\[\int_{A \times B} f(x, y)\text{d}x\text{d}y=\int_A \left(\int_B f(x, y)\text{d}y\right)\text{d}x.\]</span> If <span class="math inline">\(f\)</span> is Lebesgue integrable on <span class="math inline">\(A \times B\)</span>, then for <span class="math inline">\(\text{a.e.} x \in A\)</span>, <span class="math inline">\(f(x, y)\)</span> (as a function of <span class="math inline">\(y\)</span>) is Lebesgue integrable on <span class="math inline">\(B\)</span>, <span class="math inline">\(\displaystyle \int_B f(x, y)\text{d}y\)</span> (as a function of <span class="math inline">\(x\)</span>) is Lebesgue integrable on <span class="math inline">\(A\)</span>, and <span class="math display">\[\int_{A \times B} f(x, y)\text{d}x\text{d}y=\int_A \left(\int_B f(x, y)\text{d}y\right)\text{d}x.\]</span></p>
<p><strong>Example 6.1</strong>    Let <span class="math inline">\(\displaystyle f(x, y)=\frac{x^2-y^2}{(x^2+y^2)^2}\)</span> and <span class="math inline">\(E=(0, 1) \times (0, 1)\)</span>. We have <span class="math display">\[\int_{(0, 1)} \left(\int_{(0, 1)} f(x, y)\text{d}y\right)\text{d}x=\frac{\pi}{4}\]</span> and <span class="math display">\[\int_{(0, 1)} \left(\int_{(0, 1)} f(x, y)\text{d}x\right)\text{d}y=-\frac{\pi}{4}.\]</span> By <a href="#thm6.1">Fubini's theorem</a>, <span class="math inline">\(f\)</span> is not Lebesgue integrable.</p>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Measure</tag>
        <tag>Integral</tag>
      </tags>
  </entry>
  <entry>
    <title>DNN Initialization</title>
    <url>/Computer-science/Deep-learning/DNN_Initialization.html</url>
    <content><![CDATA[<h1 id="xavier-weight-initialization">1. Xavier Weight Initialization</h1>
<p>Xavier Glorot and Yoshua Bengio (2010) considered random initialized networks and the associated variance of pre-activation values and the gradients as they pass from layer to layer. Glorot and Bengio noted that for symmetric <span class="math inline">\(\phi(\cdot)\)</span> such as <span class="math inline">\(\tanh(\cdot)\)</span>, the hidden layer values are approximately Gaussian with a variance depending on the variance of the weight matrices <span class="math inline">\(W^{(i)}\)</span>.</p>
<p>In particular, if <span class="math inline">\(\sigma_W\)</span> is selected appropriately (<span class="math inline">\(\sigma_W^2=1/(3n)\)</span>) then the variance of hidden layer values is approximately constant through layers (bottom plot of <a href="#f1">Figure 1</a>); if <span class="math inline">\(\sigma_W\)</span> is small then the variance of hidden layer values converges towards zero with depth (top plot of <a href="#f1">Figure 1</a>).</p>
<p><span id="f1"><img data-src="/images/DNN_Initialization_1.png"></span></p>
<center>
Figure 1: Activation values normalized histograms with hyperbolic tangent activation, with standard (top) vs normalized (bottom) initialization <a href="#GB10">(Glorot and Bengio, 2010)</a>.
</center>
<p>Similarly, the gradient of a loss function used for training shows the similar Gaussian behavior depending on <span class="math inline">\(\sigma_W\)</span>.</p>
<p><span id="f2"><img data-src="/images/DNN_Initialization_2.png"></span></p>
<center>
Figure 2: Back-propagated gradients normalized histograms with hyperbolic tangent activation, with standard (top) vs normalized (bottom) initialization <a href="#GB10">(Glorot and Bengio, 2010)</a>.
</center>
<p>The suggested <strong>Xavier initialization</strong> <span class="math inline">\(\sigma_W^2=1/(3n)\)</span> follows from balancing the variance of hidden layer values and the gradient to be constant through depth.</p>
<h1 id="random-dnn-recursion-correlation-map">2. Random DNN Recursion Correlation Map</h1>
<p>Let <span class="math inline">\(f_{NN}(\mathbf{x})\)</span> denote a random Gaussian DNN: <span class="math display">\[\mathbf{h}^{(\mathscr{l})}=W^{(\mathscr{l})}\mathbf{z}^{(\mathscr{l})}+\mathbf{b}^{(\mathscr{l})}, \mathbf{z}^{(\mathscr{l}+1)}=\phi(\mathbf{h}^{(\mathscr{l})}), \mathscr{l}=0, \ldots, L-1,\]</span> which takes as input the vector <span class="math inline">\(\mathbf{x}\)</span>, and is parameterized by random weight matrices <span class="math inline">\(W^{(\mathscr{l})}\)</span> and bias vectors <span class="math inline">\(\mathbf{b}^{(\mathscr{l})}\)</span> with entries sampled i.i.d. from the Gaussian normal distribution <span class="math inline">\(\mathcal{N}(0, \sigma_W^2)\)</span> and <span class="math inline">\(\mathcal{N}(0, \sigma_\mathbf{b}^2)\)</span>. Define the <strong><span class="math inline">\(\mathscr{l}^2\)</span> length of the pre-activation hidden layer</strong> <span class="math inline">\(\mathbf{h}^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l}}\)</span> output as <span class="math display">\[q^\mathscr{l}=n_\mathscr{l}^{-1}\|\mathbf{h}^{(\mathscr{l})}\|^2_{\mathscr{l}^2}:=\frac{1}{n_\mathscr{l}}\sum_{i=1}^{n_\mathscr{l}} (\mathbf{h}^{(\mathscr{l})}(i))^2,\]</span> which is the sample mean of the random entries <span class="math inline">\((\mathbf{h}^{(\mathscr{l})}(i))^2\)</span>, each of which are identically distributed.</p>
<p>The norm of the hidden layer output <span class="math inline">\(q^\mathscr{l}\)</span> has an expected value over the random draws of <span class="math inline">\(W^{(\mathscr{l})}\)</span> and <span class="math inline">\(\mathbf{b}^{(\mathscr{l})}\)</span> which satisfies <span class="math inline">\(\mathbb{E}[q^\mathscr{l}]=\mathbb{E}[(\mathbf{h}^{(\mathscr{l})}(i))^2]\)</span> which as <span class="math inline">\(\mathbf{h}^{(\mathscr{l})}=W^{(\mathscr{l})}\phi(\mathbf{h}^{(\mathscr{l}-1)})+\mathbf{b}^{(\mathscr{l})}\)</span> is <span class="math display">\[\begin{aligned}
\mathbb{E}[q^\mathscr{l}]&amp;=\mathbb{E}[(W_i^{(\mathscr{l})}\phi(\mathbf{h}^{(\mathscr{l}-1)}))^2]+\mathbb{E}[(b_i^{(\mathscr{l})})^2]
\\&amp;=\sigma_W^2n_{\mathscr{l}-1}^{-1}\sum_{i=1}^{n_\mathscr{l}-1} \phi(h_i^{(\mathscr{l}-1)})^2+\sigma_\mathbf{b}^2
\end{aligned}\]</span> where <span class="math inline">\(W_i^{(\mathscr{l})}\)</span> denotes the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(W^{(\mathscr{l})}\)</span>.</p>
<p>Approximating <span class="math inline">\(h_i^{(\mathscr{l}-1)}\)</span> as Gaussian with variance <span class="math inline">\(q^{(\mathscr{l}-1)}\)</span>: <span class="math display">\[n_{\mathscr{l}-1}^{-1}\sum_{i=1}^{n_{\mathscr{l}-1}} \phi(h_i^{(\mathscr{l}-1)})^2=\mathbb{E}[\phi(h^{(\mathscr{l}-1)})^2]=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \phi\left(\sqrt{q^{(\mathscr{l}-1)}}z\right)^2e^{-z^2/2}\text{d}z\]</span> which gives a <strong>recursive map of <span class="math inline">\(q^\mathscr{l}\)</span> between layers</strong> <span class="math display">\[q^{(\mathscr{l})}=\sigma_\mathbf{b}^2+\sigma_W^2\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty \phi\left(\sqrt{q^{(\mathscr{l}-1)}}z\right)^2e^{-z^2/2}\text{d}z=:\mathcal{V}(q^{(\mathscr{l}-1)} \mid \sigma_W, \sigma_\mathbf{b}, \phi(\cdot)).\]</span> Note that the integral is larger for <span class="math inline">\(\phi(x)=|x|\)</span> than ReLU, which are larger than <span class="math inline">\(\phi(x)=\tanh(x)\)</span>, indicating smaller <span class="math inline">\(\sigma_W\)</span> and <span class="math inline">\(\sigma_\mathbf{b}\)</span> needed to ensure <span class="math inline">\(q^{(\mathscr{l})}\)</span> has a finite nonzero limit <span class="math inline">\(q^*\)</span>. As <a href="#f3">Figure 3</a> shown, the fixed points are all stable.</p>
<p><span id="f3"><img data-src="/images/DNN_Initialization_3.png"></span></p>
<center>
Figure 3: Dynamics of the squared length <span class="math inline">\(q^\mathscr{l}\)</span> for a sigmoidal network (<span class="math inline">\(\phi(h)=\tanh(h)\)</span>) with <span class="math inline">\(1000\)</span> hidden units <a href="#P+16">(Poole et al., 2016)</a>.
</center>
<p>A single input <span class="math inline">\(\mathbf{x}\)</span> has hidden pre-activation output converging to a fixed expected length. Consider the <strong>map governing the angle between the hidden layer pre-activations of two distinct inputs</strong> <span class="math inline">\(\mathbf{x}^{(0, a)}\)</span> and <span class="math inline">\(\mathbf{x}^{(0, b)}\)</span>: <span class="math display">\[q_{ab}^{(\mathscr{l})}=n_\mathscr{l}^{-1}\sum_{i=1}^{n_\mathscr{l}} h_i^{(\mathscr{l})}(\mathbf{x}^{(0, a)})h_i^{(\mathscr{l})}(\mathbf{x}^{(0, b)}).\]</span> Replacing the average in the sum with the expected values gives <span class="math display">\[q_{ab}^{(\mathscr{l})}=\sigma_\mathbf{b}^2+\sigma_W^2\mathbb{E}[\phi(\mathbf{h}^{(\mathscr{l}-1)}(\mathbf{x}^{(0, a)}))\phi(\mathbf{h}^{(\mathscr{l}-1)}(\mathbf{x}^{(0, b)}))]\]</span> where as before, <span class="math inline">\(h_i^{(\mathscr{l}-1)}\)</span> is well modeled as being Gaussian with expected length <span class="math inline">\(q^{(\mathscr{l}-1)}\)</span> which converges to fix points <span class="math inline">\(q^*\)</span>.</p>
<p>Define the <strong>angle between the hidden layers</strong> as <span class="math inline">\(c^{(\mathscr{l})}=q_{12}^{(\mathscr{l})}/q^*\)</span>. Writing the expectation as integral, we have <span class="math display">\[c^{(\mathscr{l})}=\mathcal{C}(c^{(\mathscr{l}-1)} \mid \sigma_W, \sigma_\mathbf{b}, \phi(\cdot)):=\sigma_\mathbf{b}^2+\sigma_W^2\iint \text{D}z_1\text{D}z_2\phi(u_1)\phi(u_2)\]</span> where <span class="math inline">\(\text{D}z=(2\pi)^{-1/2}e^{-z^2/2}\text{d}z\)</span>, <span class="math inline">\(u_1=\sqrt{q^*}z_1\)</span> and <span class="math inline">\(u_2=\sqrt{q^*}\left(c^{(\mathscr{l}-1)}z_1+\sqrt{1-(c^{(\mathscr{l}-1)})^2}z_2\right)\)</span>. Normalizing by <span class="math inline">\(q^*\)</span>, we have the <strong>correlation map</strong> <span class="math display">\[\rho^{(\mathscr{l}+1)}=c^{(\mathscr{l}+1)}:=R(\rho^{(\mathscr{l})}; \sigma_W, \sigma_\mathbf{b}, \phi(\cdot)),\]</span> where <span class="math inline">\(R(\rho)\)</span> measures how the correlation between two inputs evolve through the layers. By construction <span class="math inline">\(R(1)=1\)</span> as <span class="math inline">\(c^{(\mathscr{l})} \to q^*\)</span> as the two inputs converge to one another.</p>
<p>For <span class="math inline">\(\sigma_\mathbf{b}^2&gt;0\)</span>, the correlation map satisfies <span class="math inline">\(R(0)&gt;0\)</span>, orthogonal <span class="math inline">\(\mathbf{x}^{(0, a)}\)</span> and <span class="math inline">\(\mathbf{x}^{(0, b)}\)</span> become increasingly correlated with depth.</p>
<p>The stability of a point and its perturbation is determined by the slope of <span class="math inline">\(R(\cdot)\)</span> at <span class="math inline">\(\rho=1\)</span> is <span class="math display">\[\chi:=\frac{\partial R(\rho)}{\partial \rho}\bigg|_{\rho=1}=\frac{\partial \rho^{(\mathscr{l})}}{\partial \rho^{(\mathscr{l}-1)}}\bigg|_{\rho=1}=\sigma_W^2\int \text{D}z\left[\phi&#39;\left(\sqrt{q^*}z\right)^2\right].\]</span> Stability of the fixed point at <span class="math inline">\(\rho=1\)</span> is determined by <span class="math inline">\(\chi\)</span>: if <span class="math inline">\(\chi&lt;1\)</span>, <span class="math inline">\(\rho=1\)</span> is locally stable and points which are sufficiently correlated all converge, with depth, to the same point; if <span class="math inline">\(\chi&gt;1\)</span>, <span class="math inline">\(\rho=1\)</span> is unstable and nearby points become uncorrelated with depth; it is preferable to choose <span class="math inline">\(\chi=1\)</span> if possible for <span class="math inline">\((\sigma_W, \sigma_\mathbf{b}, \phi(\cdot))\)</span>.</p>
<p>As <a href="#f4">Figure 4</a> shown, initialization on the curve allows training very deep networks.</p>
<p><span id="f4"><img data-src="/images/DNN_Initialization_4.png"></span></p>
<center>
Figure 4: Edge of chaos curves for nonlinear activations <a href="#MAT21">(Murray, Abrol, and Tanner, 2021)</a>.
</center>
<h1 id="spectrum-of-the-jacobian">3. Spectrum of the Jacobian</h1>
<h2 id="dnn-jacobian">3.1. DNN Jacobian</h2>
<p>The <strong>Jacobian</strong> is given by <span class="math display">\[J=\frac{\partial \mathbf{z}^{(\mathscr{l})}}{\partial \mathbf{x}^{(0)}}=\prod_{\mathscr{l}=0}^{L-1} D^{(\mathscr{l})}W^{(\mathscr{l})}\]</span> where <span class="math inline">\(D^{(\mathscr{l})}\)</span> is diagonal with entries <span class="math inline">\(D_{ii}^{(\mathscr{l})}=\phi&#39;(h_i^{(\mathscr{l})})\)</span>.</p>
<p>The Jacobian can bound the local stability of the DNN: <span class="math display">\[\|H(\mathbf{x}+\varepsilon; \theta)-H(\mathbf{x}; \theta)\|=\|J\varepsilon+\mathcal{O}(\|\varepsilon\|^2)\| \leq \|\varepsilon\| \cdot \max\|J\|.\]</span></p>
<p>For the sum of squares loss <span class="math inline">\(\mathcal{L}\)</span>, the gradient is computed as <span class="math display">\[\delta_\mathscr{l}=D^\mathscr{l}(W^{(\mathscr{l})})^T\delta_{\mathscr{l}+1} \quad \text{and} \quad \delta_L=D^{(\mathscr{l})}\nabla_{\mathbf{h}^{(\mathscr{l})}}\mathcal{L}\]</span> which gives the formula for computing the <span class="math inline">\(\delta_\mathscr{l}\)</span> for each layer as <span class="math display">\[\delta_\mathscr{l}=\left(\prod_{k=\mathscr{l}}^{L-1} D^{(k)}(W^{(k)})^T\right)D^{(\mathscr{l})}\nabla_{\mathbf{h}^{(\mathscr{l})}}\mathcal{L}\]</span> and the resulting gradient <span class="math inline">\(\nabla_\theta \mathcal{L}\)</span> with entries as <span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(\mathscr{l})}}=\delta_{\mathscr{l}+1} \cdot (\mathbf{h}^{(\mathscr{l})})^T \quad \text{and} \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\mathscr{l})}}=\delta_{\mathscr{l}+1}.\]</span></p>
<h2 id="stieltjes-and-mathcals-transform">3.2. Stieltjes and \(\mathcal{S}\) Transform</h2>
<p>For <span class="math inline">\(z \in \mathbb{C}-\mathbb{R}\)</span>, the <strong>Stieltjes transform</strong> <span class="math inline">\(G_\rho(z)\)</span> of a probability distribution and its inverse are given by <span class="math display">\[G_\rho(z)=\int_\mathbb{R} \frac{\rho(t)}{z-t}\text{d}t \quad \text{and} \quad \rho(\lambda)=-\pi^{-1}\lim_{\varepsilon \to 0_+} \text{Imag}(G_\rho(\lambda+i\varepsilon)).\]</span> The Stieltjes transform and <strong>moment generating function</strong> are related by <span class="math display">\[M_\rho(z):=zG_\rho(z)-1=\sum_{k=1}^\infty \frac{m_k}{z^k}\]</span> and the <strong><span class="math inline">\(\mathcal{S}\)</span> transform</strong> is defined as <span class="math display">\[S_\rho(z)=\frac{1+z}{zM_\rho^{-1}(z)}.\]</span> The <span class="math inline">\(\mathcal{S}\)</span> transform has the property that if <span class="math inline">\(\rho_1\)</span> and <span class="math inline">\(\rho_2\)</span> are freely independent, then <span class="math inline">\(S_{\rho_1\rho_2}=S_{\rho_1}S_{\rho_2}\)</span>.</p>
<p>The <span class="math inline">\(\mathcal{S}\)</span> transform of <span class="math inline">\(JJ^T\)</span> is then given by <span class="math display">\[\mathcal{S}_{JJ^T}=\mathcal{S}_{D^2}^L\mathcal{S}_{W^TW}^L,\]</span> which can be computed through the moments <span class="math display">\[M_{JJ^T}(z)=\sum_{k=1}^\infty \frac{m_k}{z^k} \quad \text{and} \quad M_{D^2}(z)=\sum_{k=1}^\infty \frac{\mu_k}{z^k}\]</span> where <span class="math inline">\(\displaystyle \mu_k=\int (2\pi)^{-1/2}\phi&#39;\left(\sqrt{q^*}z\right)^{2k}e^{-z^2/2}\text{d}z\)</span>.</p>
<p>In particular, <span class="math inline">\(m_1=(\sigma_W^2\mu_1)^L\)</span> and <span class="math inline">\(m_2=(\sigma_W^2\mu_1)^{2L}L(\mu_2^{-1}\mu_1^2+L^{-1}-1-s_1)\)</span>, where <span class="math inline">\(\sigma_W^2\mu_1=\chi\)</span> is the growth factor we observe with the edge of chaos, requiring <span class="math inline">\(\chi=1\)</span> to avoid rapid convergence of correlations to fixed points.</p>
<h2 id="nonlinear-activation-stability">3.3. Nonlinear Activation Stability</h2>
<p>Recall that <span class="math inline">\(m_1=\chi^L\)</span> is the expected value of the spectrum of <span class="math inline">\(JJ^T\)</span>, while the variance of the spectrum of <span class="math inline">\(JJ^T\)</span> is given by <span class="math display">\[\sigma_{JJ^T}^2=m_2-m_1^2=L(\mu_2\mu_1^{-2}-1-s_1)\]</span> where for <span class="math inline">\(W\)</span> Gaussian, <span class="math inline">\(s_1=-1\)</span>, and for <span class="math inline">\(W\)</span> orthogonal, <span class="math inline">\(s_1=0\)</span>.</p>
<p>Pennington et al. (2018) found that for linear networks, the fixed point equation is <span class="math inline">\(q^*=\sigma_W^2q^*+\sigma^2_\mathbf{b}\)</span>, and <span class="math inline">\((\sigma_W, \sigma_\mathbf{b})=(1, 0)\)</span> is the only critical point; for ReLU networks, the fixed point equation is <span class="math inline">\(\displaystyle q^*=\frac{1}{2}\sigma_W^2q^*+\sigma_\mathbf{b}^2\)</span>, and <span class="math inline">\((\sigma_W, \sigma_\mathbf{b})=(\sqrt{2}, 0)\)</span> is the only critical point; Hard Tanh and Erf have curves as fixed points <span class="math inline">\(\chi(\sigma_W, \sigma_\mathbf{b})\)</span>.</p>
<h1 id="controlling-the-variance-of-the-jacobian-spectra">4. Controlling the Variance of the Jacobian Spectra</h1>
<p><strong>Definition</strong>    An activation function <span class="math inline">\(\phi: \mathbb{R} \to \mathbb{R}\)</span> satisfying the following properties is called <strong>scaled-bounded activation</strong>:</p>
<p>    (1) continuous;</p>
<p>    (2) odd, i.e., <span class="math inline">\(\phi(z)=-\phi(-z)\)</span> for all <span class="math inline">\(z \in \mathbb{R}\)</span>;</p>
<p>    (3) linear around the origin and bounded, i.e., there exists <span class="math inline">\(a, k \in \mathbb{R}_{&gt;0}\)</span> s.t. <span class="math inline">\(\phi(z)=kz\)</span> for all <span class="math inline">\(z \in [-a, a]\)</span> and <span class="math inline">\(\phi(z) \leq ak\)</span> for all <span class="math inline">\(z \in \mathbb{R}\)</span>;</p>
<p>    (4) twice differentiable at all points <span class="math inline">\(z \in \mathbb{R}-\mathcal{D}\)</span>, where <span class="math inline">\(\mathcal{D} \subset \mathbb{R}\)</span> is a finite set; furthermore <span class="math inline">\(|\phi&#39;(z)| \leq k\)</span> for all <span class="math inline">\(z \in \mathbb{R}-\mathcal{D}\)</span>.</p>
<p><strong>Theorem (Murray et al., 2021)</strong>     Let <span class="math inline">\(\phi\)</span> be a scaled-bounded activation, <span class="math inline">\(\chi_1:=\sigma_W^2\mathbb{E}[\phi&#39;(\sqrt{q^*}Z)^2]=1\)</span>, where <span class="math inline">\(q^*&gt;0\)</span> is a fixed point of <span class="math inline">\(V_\phi\)</span>, and <span class="math inline">\(\sigma_\mathbf{b}^2&gt;0\)</span>. Let input <span class="math inline">\(\mathbf{x}\)</span> satisfy <span class="math inline">\(\|\mathbf{x}\|_2^2=q^*\)</span>. Then as <span class="math inline">\(y:=\sigma_\mathbf{b}^2/a^2 \to 0\)</span>, <span class="math display">\[\max_{\rho \in [0, 1]} |R_{\phi, q^*}(\rho)-\rho| \to 0 \quad \text{and} \quad |\mu_2/\mu_1^2-1| \to 0.\]</span></p>
<div class="note info">
            <p>This is independent of details of <span class="math inline">\(\phi(\cdot)\)</span> outside its linear region <span class="math inline">\([-a, a]\)</span>. Best performance is observed with <span class="math inline">\(a \sim 3\)</span>, or preferably a decreasing from about <span class="math inline">\(5\)</span> to <span class="math inline">\(2\)</span> during training.</p>
          </div>
<h1 id="dnn-random-initialization-summary">5. DNN Random Initialization Summary</h1>
<ul>
<li><p>The hidden layers converge to fixed expected length.</p></li>
<li><p>Poole et al. (2016) showed pre-activation output is well modeled as Gaussian with variance <span class="math inline">\(q^*\)</span> determined by <span class="math inline">\((\sigma_W, \sigma_\mathbf{b}, \phi(\cdot))\)</span>. Moreover, the correlation between two inputs follows a similar map with correlations converging to a fixed point, with the behavior determined in part by <span class="math inline">\(\chi\)</span> where <span class="math inline">\(\chi=1\)</span> avoids correlation to the same point, or nearby points diverging.</p></li>
<li><p>Very DNNs can be especially hard to train for activations with unfavorable initializations; e.g., ReLU with <span class="math inline">\(\chi=1\)</span> requires <span class="math inline">\((\sigma_W, \sigma_\mathbf{b})=(\sqrt{2}, 0)\)</span>.</p></li>
<li><p>Pennington et al. (2018) showed more generally how to compute the moments for the Jacobian spectra, where <span class="math inline">\(\chi=1\)</span> is needed to avoid exponential growth or shrinkage with depth of gradients.</p></li>
</ul>
<h1 id="reference">6. Reference</h1>
<div id="P+16" style="line-height: 18px; font-size: 15px;">
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential Expressivity in Deep Neural Networks through Transient Chaos. In <em>30th Conference on Neural Information Processing Systems</em>, 2016.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="MAT21" style="line-height: 18px; font-size: 15px;">
Michael Murray, Vinayak Abrol, and Jared Tanner. Activation Function Design for Deep Networks: Linearity and Effective Initialisation. <em>arXiv Preprint arXiv:2105.07741</em>, 2021.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="GB10" style="line-height: 18px; font-size: 15px;">
Xavier Glorot and Yoshua Bengio. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In <em>Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em>, 2010.
</div>
]]></content>
      <categories>
        <category>Computer science</category>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>DNN</tag>
        <tag>Initialization</tag>
      </tags>
  </entry>
  <entry>
    <title>Measure Theory</title>
    <url>/Mathematics/Real-analysis/Measure_Theory.html</url>
    <content><![CDATA[<h1 id="outer-measure">1. Outer Measure</h1>
<h2 id="definition-and-example">1.1. Definition and Example</h2>
<p><span id="def1.1"><strong>Definition 1.1</strong></span>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span>. The <strong>outer measure</strong> of <span class="math inline">\(E\)</span> is <span class="math display">\[m^*(E):=\inf\left\{\sum_{i=1}^\infty |I_i|: \bigcup_{i=1}^\infty I_i \supset E \right\},\]</span> where <span class="math inline">\(I_i\)</span> is open, and <span class="math inline">\(|I_i|\)</span> is the volume of <span class="math inline">\(I_i\)</span>.</p>
<div class="note warning">
            <p>In <a href="#def1.1">definition 1.1</a>, we need <strong>at most countable</strong> <span class="math inline">\(\{I_i\}\)</span>. For example, if <span class="math inline">\([0, 1] \cap \mathbb{Q}\)</span> is covered by finite intervals, then those finite intervals and endpoints cover <span class="math inline">\([0, 1]\)</span>. Therefore, <span class="math inline">\(m^*([0, 1] \cap \mathbb{Q}) \geq 1\)</span>. Similarly, <span class="math inline">\(m^*([0, 1] \cap \mathbb{Q}^c) \geq 1\)</span>, and thus <span class="math inline">\(m^*([0, 1]) \geq 2\)</span>. In other cases, <span class="math inline">\(I_i\)</span> can be an empty set and thus <span class="math inline">\(\{I_i\}\)</span> can be finite.</p>
          </div>
<p><strong>Example 1.1</strong>    <span class="math inline">\(m^*(\varnothing)=0\)</span>.</p>
<p><strong>Example 1.2</strong>    If <span class="math inline">\(p \in \mathbb{R}\)</span>, then <span class="math inline">\(m^*(\{p\})=0\)</span>.</p>
<strong><em>Proof.</em></strong> <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, define <span class="math inline">\(\displaystyle I=\left(x-\frac{\varepsilon}{2}, x+\frac{\varepsilon}{2}\right)\)</span>. We have <span class="math inline">\(|I|=\varepsilon\)</span> and <span class="math inline">\(\{p\} \subset I\)</span>. Therefore, <span class="math inline">\(m^*(\{p\}) \leq \varepsilon\)</span>, i.e., <span class="math inline">\(m^*(\{p\})=0\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 1.3</strong>    If <span class="math inline">\(p_1, \ldots, p_k \in \mathbb{R}\)</span>, then <span class="math inline">\(m^*(\{p_1, \ldots, p_k\})=0\)</span>.</p>
<strong><em>Proof.</em></strong> <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, define <span class="math inline">\(\displaystyle I_i=\left(p_i-\frac{\varepsilon}{2k}, p_i+\frac{\varepsilon}{2k}\right)\)</span>, we have <span class="math inline">\(\displaystyle |I_i|=\frac{\varepsilon}{k}\)</span>. Hence, <span class="math inline">\(\displaystyle \{p_1, \ldots, p_k\} \subset \bigcup_{i=1}^k I_i\)</span> and <span class="math inline">\(\displaystyle \sum_{i=1}^n |I_i|=\varepsilon\)</span>. Therefore, <span class="math inline">\(m^*(\{p_1, \ldots, p_n\}) \leq \varepsilon\)</span>, i.e., <span class="math inline">\(m^*(\{p_1, \ldots, p_n\})=0\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 1.4</strong>    If <span class="math inline">\(p_1, \ldots, p_n, \ldots \in \mathbb{R}\)</span>, then <span class="math inline">\(m^*(\{p_1, \ldots, p_n, \ldots\})=0\)</span>.</p>
<strong><em>Hint of proof.</em></strong> Take <span class="math inline">\(\displaystyle I_i=\left(x_i-\frac{\varepsilon}{2^{i+1}}, x_i+\frac{\varepsilon}{2^{i+1}}\right)\)</span>, then <span class="math inline">\(\displaystyle |I_i|=\frac{\varepsilon}{2^i}\)</span>, and <span class="math inline">\(\displaystyle \sum_{i=1}^\infty \frac{\varepsilon}{2^i}=\varepsilon\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Example 1.5</strong>    <span class="math inline">\(m^*([0, 1] \cap \mathbb{Q})=0\)</span>.</p>
<h2 id="basic-property">1.2. Basic Property</h2>
<p><strong>Property 1.1 (Nonnegativity)</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span>, <span class="math inline">\(m^*(E) \geq 0\)</span>. Besides, <span class="math inline">\(m^*(\varnothing)=0\)</span>.</p>
<p><strong>Property 1.2 (Countable Subadditivity)</strong>    Suppose <span class="math inline">\(A_i \subset \mathbb{R}^n\)</span>. <span class="math inline">\(\displaystyle m^*\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^\infty m^*(A_i)\)</span>.</p>
<strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an open cover of <span class="math inline">\(A_i\)</span>, <span class="math inline">\(\displaystyle \bigcup_{m=1}^\infty I_{i, m}\)</span> such that <span class="math display">\[m^*(A_i) \geq \sum_{m=1}^\infty |I_{i, m}|-\frac{\varepsilon}{2^i}, \bigcup_{i=1}^\infty A_i \subset \bigcup_{i, m=1}^\infty I_{i, m}.\]</span> Therefore, <span class="math display">\[\begin{aligned}
m^*\left(\bigcup_{i=1}^\infty A_i\right) &amp;\leq \sum_{i, m=1}^\infty |I_{i, m}| \\
&amp;\leq \sum_{i=1}^\infty m^*(A_i)+\sum_{i=1}^\infty \frac{\varepsilon}{2^i} \\
&amp;=\sum_{i=1}^\infty m^*(A_i)+\varepsilon.
\end{aligned}\]</span> Hence, <span class="math inline">\(\displaystyle m^*\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^\infty m^*(A_i).\)</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Property 1.3 (Monotonicity)</strong>    Suppose <span class="math inline">\(A \subset B\)</span>, then <span class="math inline">\(m^*(A) \leq m^*(B)\)</span>.</p>
<strong><em>Proof.</em></strong> Take an arbitrary open cover of <span class="math inline">\(B\)</span>, denoted <span class="math inline">\(\{I_i\}_{i=1}^\infty\)</span>. Since <span class="math inline">\(A \subset B\)</span>, then <span class="math inline">\(\{I_i\}_{i=1}^\infty\)</span> is an open cover of <span class="math inline">\(A\)</span>, and thus <span class="math inline">\(\displaystyle m^*(A) \leq \sum_{i=1}^\infty |I_i|\)</span>. By the definition of outer measure and infimum, we have <span class="math inline">\(m^*(B) \geq m^*(A)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 1.4</strong>    Suppose <span class="math inline">\(I\)</span> is an interval, then <span class="math inline">\(m^*(I)=|I|\)</span>.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(I\)</span> is a closed interval. Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, we can find an open interval <span class="math inline">\(I&#39; \supset I\)</span> such that <span class="math inline">\(|I&#39;| \leq |I|+\varepsilon\)</span>. Hence, <span class="math inline">\(m^*(I) \leq |I&#39;| \leq |I|+\varepsilon\)</span>, i.e., <span class="math inline">\(m^*(I) \leq |I|\)</span>.</p>
<p>Besides, we can find an open cover of <span class="math inline">\(I\)</span>, denoted <span class="math inline">\(\{I_i\}_{i=1}^\infty\)</span>, such that <span class="math inline">\(\displaystyle m^*(I) \geq \sum_{i=1}^\infty |I_i|-\varepsilon\)</span>. Since <span class="math inline">\(I\)</span> is bounded and closed, then there exists a finite open cover of <span class="math inline">\(I\)</span>, denoted <span class="math inline">\(\{I_i\}_{i=1}^m\)</span>, such that <span class="math inline">\(\displaystyle m^*(I) \geq \sum_{i=1}^m |I_i|-\varepsilon\)</span>. Since <span class="math inline">\(\displaystyle \sum_{i=1}^m |I_i| \geq |I|\)</span>, then <span class="math inline">\(m^*(I) \geq |I|-\varepsilon\)</span>, i.e., <span class="math inline">\(m^*(I) \geq |I|\)</span>.</p>
<p>Therefore, <span class="math inline">\(m^*(I)=|I|\)</span>.</p>
We now suppose <span class="math inline">\(I\)</span> is an arbitrary interval. We have <span class="math inline">\(\forall \varepsilon&gt;0, \exists I_1, I_2\)</span> s.t. <span class="math inline">\(I_1 \subset I \subset I_2\)</span>, where <span class="math inline">\(I_1, I_2\)</span> are closed, <span class="math inline">\(|I_1|+\varepsilon&gt;|I|\)</span>, and <span class="math inline">\(|I_2|-\varepsilon&lt;|I|\)</span>. Therefore, <span class="math display">\[|I|-\varepsilon&lt;m^*(I_1) \leq m^*(I) \leq m^*(I_2)&lt;|I|+\varepsilon.\]</span> As a consequence, <span class="math inline">\(m^*(I)=|I|\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="measurable-set">2. Measurable Set</h1>
<h2 id="definition">2.1. Definition</h2>
<p><strong>Definition 2.1</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span> is bounded, and <span class="math inline">\(I \supset E\)</span> is open. The <strong>inner measure</strong> of <span class="math inline">\(E\)</span> is <span class="math display">\[m_*(E)=|I|-m^*(I-E).\]</span></p>
<div class="note info">
            <p><font color="#696969" font size="2">显然，若<span class="math inline">\(E\)</span>有界，且<span class="math inline">\(m^*(E)=m_*(E)\)</span>，我们能得到<strong>可测</strong>的定义. 然而，我们希望能对任何一个<span class="math inline">\(E\)</span>定义可测，而上述的定义则有所限制. 此外，这一定义需要同时使用内外测度的定义. 因此，Carathéodory提出了一个更简洁地等价定义.</font></p>
          </div>
<p><strong>Definition 2.2</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span> is a set. If for any set <span class="math inline">\(T \subset \mathbb{R}^n\)</span>, we have <span class="math display">\[m^*(T)=m^*(T \cap E)+m^*(T \cap E^c),\]</span> then <span class="math inline">\(E\)</span> is <strong>L-measurable</strong> or <strong>measurable</strong>. The <strong>L-measure</strong>, denoted <span class="math inline">\(m(E)\)</span>, equals to <span class="math inline">\(m^*(E)\)</span>. The collection of all measurable sets is called family of measurable sets, denoted as <span class="math inline">\(\mathscr{M}\)</span>.</p>
<p><span id="thm2.1"><strong>Theorem 2.1</strong></span>    <span class="math inline">\(E\)</span> is measurable if and only if for any <span class="math inline">\(A \subset E\)</span> and <span class="math inline">\(B \subset E^c\)</span>, we have <span class="math display">\[m^*(A \cup B)=m^*(A)+m^*(B).\]</span></p>
<p><strong><em>Proof.</em></strong> <span class="math inline">\((\Rightarrow)\)</span> Take <span class="math inline">\(T=A \cup B\)</span> for any <span class="math inline">\(A \subset E\)</span> and <span class="math inline">\(B \subset E^c\)</span>. Then <span class="math display">\[m^*(A \cup B)+m^*((A \cup B) \cap E)+m^*((A \cup B) \cap E^c)=m^*(A)+m^*(B).\]</span></p>
<span class="math inline">\((\Leftarrow)\)</span> Take <span class="math inline">\(A=T \cap E\)</span> and <span class="math inline">\(B=T \cap E^c\)</span> for any <span class="math inline">\(T\)</span>. Then <span class="math display">\[m^*(T \cap E)+m^*(T \cap E^c)=m^*(A)+m^*(B)=m^*(A \cup B)=m^*(T).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="operation">2.2. Operation</h2>
<p><strong>Theorem 2.2</strong>    <span class="math inline">\(S\)</span> is measurable if and only if <span class="math inline">\(S^c\)</span> is measurable.</p>
<strong><em>Proof.</em></strong> If <span class="math inline">\(S\)</span> is measurable, then for any set <span class="math inline">\(T\)</span>, <span class="math display">\[m^*(T)=m^*(T \cap S)+m^*(T \cap S^c)=m^*(T \cap (S^c)^c)+m^*(T \cap S^c),\]</span> which is equivalent to say that <span class="math inline">\(S^c\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.3</strong>    If <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are measurable, then <span class="math inline">\(S_1 \cup S_2\)</span> is measurable.</p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are measurable, then for any set <span class="math inline">\(T\)</span>, <span class="math display">\[\begin{aligned}
m^*(T)&amp;=m^*(T \cap S_1)+m^*(T \cap S_1^c) \\
&amp;=m^*((T \cap (S_1 \cup S_2)) \cap S_1)+[m^*((T \cap S_1^c) \cap S_2)+m^*((T \cap S_1^c) \cap S_2^c))] \\
&amp;=m^*((T \cap (S_1 \cup S_2)) \cap S_1)+[m^*((T \cap (S_1 \cup S_2)) \cap S_1^c)+m^*(T \cap (S_1 \cup S_2)^c)] \\
&amp;=m^*(T \cap (S_1 \cup S_2))+m^*(T \cap (S_1 \cup S_2)^c).
\end{aligned}\]</span> Therefore, <span class="math inline">\(S_1 \cup S_2\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p><font color="#696969" font size="2">对于不熟悉集合运算的读者，我们建议利用韦恩图逆推.</font></p>
          </div>
<p><strong>Theorem 2.4</strong>    If <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are measurable, and <span class="math inline">\(S_1 \cap S_2=\varnothing\)</span>, then for any set <span class="math inline">\(T\)</span>, <span class="math display">\[m^*(T \cap (S_1 \cup S_2))=m^*(T \cap S_1)+m^*(T \cap S_2).\]</span></p>
<strong><em>Proof.</em></strong> Since <span class="math inline">\(S_1 \cap S_2=\varnothing\)</span>, then for any set <span class="math inline">\(T\)</span>, we have <span class="math inline">\(T \cap S_1 \subset S_1\)</span> and <span class="math inline">\(T \cap S^2 \subset S_1^c\)</span>. Since <span class="math inline">\(S_1\)</span> is measurable, then by <a href="#thm2.1">theorem 2.1</a>, <span class="math display">\[m^*(T \cap (S_1 \cup S_2))=m^*((T \cap S_1) \cup (T \cap S_2))=m^*(T \cap S_1)+m^*(T \cap S_2).\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm2.5"><strong>Corollary 2.5</strong></span>    If <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are measurable, and <span class="math inline">\(S_1 \cap S_2=\varnothing\)</span>, then <span class="math display">\[m(S_1 \cup S_2)=m(S_1)+m(S_2).\]</span></p>
<p><strong>Corollary 2.6</strong>    If <span class="math inline">\(S_1, \ldots, S_k\)</span> are measurable, then <span class="math inline">\(\displaystyle \bigcup_{i=1}^k S_i\)</span> and <span class="math inline">\(\displaystyle \bigcap_{i=1}^k S_i\)</span> are measurable. Moreover, if <span class="math display">\[S_1 \cap \cdots \cap S_k=\varnothing,\]</span> then for any set <span class="math inline">\(T\)</span>, <span class="math display">\[m^*(T \cap (S_1 \cup \cdots \cup S_k))=m^*(T \cap S_1)+\cdots+m^*(T \cap S_k).\]</span> Specifically, <span class="math display">\[m(S_1 \cup \cdots \cup S_k)=m(S_1)+\cdots+m(S_k),\]</span> which is <strong>finite additivity</strong>.</p>
<p><strong>Corollary 2.7</strong>    If <span class="math inline">\(S_1\)</span> and <span class="math inline">\(S_2\)</span> are measurable, then <span class="math inline">\(S_1-S_2\)</span> is measurable.</p>
<p><strong>Theorem 2.8</strong>    If <span class="math inline">\(S_1 \supset S_2\)</span> and <span class="math inline">\(m(S_2)&lt;\infty\)</span>, then <span class="math display">\[m(S_1-S_2)=m(S_1)-m(S_2).\]</span></p>
<strong><em>Proof.</em></strong> We know <span class="math inline">\(S_1=(S_1-S_2) \cup S_2\)</span> and <span class="math inline">\((S_1-S_2) \cap S_2=\varnothing\)</span>. Then by <a href="#thm2.5">corollary 2.5</a>, <span class="math display">\[m(S_1)=m(S_1-S_2)+m(S_2).\]</span> Since <span class="math inline">\(m(S_2)&lt;\infty\)</span>, then <span class="math inline">\(m(S_1-S_2)=m(S_1)-m(S_2)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.9</strong>    If <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of mutually disjoint measurable sets, then <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty S_i\)</span> is measurable. Besides, <span class="math inline">\(\displaystyle m\left(\bigcup_{i=1}^\infty S_i\right)=\sum_{i=1}^\infty m(S_i)\)</span>, which is <strong>countable additivity</strong>.</p>
<p><strong><em>Proof.</em></strong> We know <span class="math display">\[\begin{aligned}
m^*(T)&amp;=m^*\left(T \cap \left(\bigcup_{i=1}^k S_i\right)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^k S_i\right)^c\right) \\
&amp;\geq m^*\left(T \cap \left(\bigcup_{i=1}^k S_i\right)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right) &amp; \text{(Monotonicity)} \\
&amp;=\sum_{i=1}^k m^*(T \cap S_i)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right).
\end{aligned}\]</span> Take <span class="math inline">\(k \to \infty\)</span>, then <span class="math display">\[\begin{aligned}
m^*(T)&amp;\geq \sum_{i=1}^\infty m^*(T \cap S_i)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right) \\
&amp;\geq m^*\left(\bigcup_{i=1}^\infty (T \cap S_i)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right) &amp; \text{(Countable Subadditivity)} \\
&amp;=m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right).
\end{aligned}\]</span> Because of countable subadditivity, we know <span class="math display">\[m^*(T) \leq m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right),\]</span> and thus <span class="math display">\[m^*(T)=m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)\right)+m^*\left(T \cap \left(\bigcup_{i=1}^\infty S_i\right)^c\right),\]</span> i.e., <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty S_i\)</span> is measurable.</p>
Let <span class="math inline">\(\displaystyle T=\bigcup_{i=1}^\infty S_i\)</span>, then <span class="math inline">\(\displaystyle m\left(\bigcup_{i=1}^\infty S_i\right) \geq \sum_{i=1}^\infty m(S_i)\)</span>. Because of countable subadditivity, we have <span class="math inline">\(\displaystyle m\left(\bigcup_{i=1}^\infty S_i\right) \leq \sum_{i=1}^\infty m(S_i)\)</span>. Hence, <span class="math inline">\(\displaystyle m\left(\bigcup_{i=1}^\infty S_i\right)=\sum_{i=1}^\infty m(S_i)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.10</strong>    If <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of measurable sets, then <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty S_i\)</span> is measurable.</p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(S_0=\varnothing\)</span>, and <span class="math inline">\(\displaystyle \widetilde{S}_i=S_i-\bigcup_{k=0}^{i-1} S_k\)</span>, where <span class="math inline">\(i \geq 1\)</span>. Then <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty S_i=\bigcup_{i=1}^\infty \widetilde{S}_i\)</span>, and <span class="math inline">\(\{\widetilde{S}_i\}_{i=1}^\infty\)</span> is a collection of mutually disjoint measurable sets. Hence, <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty \widetilde{S}_i\)</span> is measurable, i.e., <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty S_i\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 2.11</strong>    If <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of measurable sets, then <span class="math inline">\(\displaystyle \bigcap_{i=1}^\infty S_i\)</span> is measurable.</p>
<p><strong>Theorem 2.12</strong>    If <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of increasing measurable sets, then <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i\)</span> is measurable, and <span class="math inline">\(\displaystyle m\left(\lim_{i \to \infty} S_i\right)=\lim_{i \to \infty} m(S_i)\)</span>.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i=\bigcup_{i=1}^\infty S_i\)</span>, then <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i\)</span> is measurable.</p>
Let <span class="math inline">\(S_0=\varnothing\)</span>, and <span class="math inline">\(\widetilde{S}_i=S_i-S_{i-1}\)</span>, where <span class="math inline">\(i \geq 1\)</span>. Then <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i=\bigcup_{i=1}^\infty S_i=\bigcup_{i=1}^\infty \widetilde{S}_i\)</span>, and <span class="math inline">\(\{\widetilde{S}_i\}_{i=1}^\infty\)</span> is a collection of mutually disjoint measurable sets. Hence, <span class="math display">\[\begin{aligned}
m\left(\lim_{i \to \infty} S_i\right)&amp;=\sum_{i=1}^\infty m(\widetilde{S}_i) \\
&amp;=\sum_{i=1}^\infty m(S_i-S_{i-1}) \\
&amp;=\lim_{k \to \infty}\sum_{i=1}^k m(S_i-S_{i-1}) \\
&amp;=\lim_{k \to \infty}m\left(\bigcup_{i=1}^k (S_i-S_{i-1})\right) \\
&amp;=\lim_{k \to \infty}m(S_k)=\lim_{i \to \infty}m(S_i).
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.13</strong>    If <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of decreasing measurable sets, then <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i\)</span> is measurable. Moreover, when <span class="math inline">\(m(S_1)&lt;\infty\)</span>, <span class="math inline">\(\displaystyle m\left(\lim_{i \to \infty} S_i\right)=\lim_{i \to \infty} m(S_i)\)</span>.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i=\bigcap_{i=1}^\infty S_i\)</span>, then <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i\)</span> is measurable.</p>
Let <span class="math inline">\(\widetilde{S}_i=S_1-S_i\)</span>, then <span class="math inline">\(\{\widetilde{S}_i=S_1-S_i\}_{i=1}^\infty\)</span> is a collection of increasing sets. Therefore, <span class="math display">\[m\left(\lim_{i \to \infty} (S_1-S_i)\right)=\lim_{i \to \infty} m(S_1-S_i).\]</span> Since <span class="math inline">\(m(S_1)&lt;\infty\)</span>, then <span class="math inline">\(m(S_i)&lt;\infty\)</span> and <span class="math inline">\(\displaystyle \lim_{i \to \infty} S_i&lt;\infty\)</span>. Hence, <span class="math display">\[m\left(\lim_{i \to \infty} (S_1-S_i)\right)=m\left(S_1-\lim_{i \to \infty} S_i\right)=m(S_1)-m\left(\lim_{i \to \infty} S_i\right),\]</span> and <span class="math display">\[\lim_{i \to \infty} m(S_1-S_i)=\lim_{i \to \infty} (m(S_1)-m(S_i))=m(S_1)-\lim_{i \to \infty} m(S_i).\]</span> Therefore, <span class="math inline">\(\displaystyle m\left(\lim_{i \to \infty} S_i\right)=\lim_{i \to \infty} m(S_i)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>The condition <span class="math inline">\(m(S_1)&lt;\infty\)</span> is important. For example, <span class="math inline">\(S_n=(n, \infty)\)</span>. It is obvious that <span class="math inline">\(\{S_n\}\)</span> is decreasing. Therefore, <span class="math inline">\(\displaystyle \lim_{n \to \infty} S_n=\bigcap_{n=1}^\infty S_n=\varnothing\)</span>, and <span class="math inline">\(\displaystyle m\left(\lim_{n \to \infty} S_n\right)=0\)</span>. However, <span class="math inline">\(m(S_n)=\infty\)</span>, and <span class="math inline">\(\displaystyle \lim_{n \to \infty} m(S_n)=\infty\)</span>.</p>
          </div>
<p><strong>Theorem 2.14</strong>    Suppose <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> is a collection of measurable sets. If the upper limit and lower limit of <span class="math inline">\(\{S_i\}_{i=1}^\infty\)</span> and measure exist, then <span class="math inline">\(\displaystyle m\left(\varliminf_{i \to \infty} S_i \right) \leq \varliminf_{i \to \infty} m(S_i) \leq \varlimsup_{i \to \infty} m(S_i) \leq m\left(\varlimsup_{i \to \infty} S_i\right)\)</span>. Note that we need <span class="math inline">\(\displaystyle m\left(\bigcup_{i=1}^\infty S_i\right)&lt;\infty\)</span> to prove <span class="math inline">\(\displaystyle \varlimsup_{i \to \infty} m(S_i) \leq m\left(\varlimsup_{i \to \infty} S_i\right)\)</span>.</p>
<h1 id="sigma-algebra">3. \(\sigma\)-Algebra</h1>
<p><strong>Definition 3.1</strong>    Suppose <span class="math inline">\(\{F_i\}_{i=1}^\infty\)</span> is a collection of closed sets, and <span class="math inline">\(\{G_i\}_{i=1}^\infty\)</span> is a collection of open sets. If <span class="math inline">\(\displaystyle F=\bigcup_{i=1}^\infty F_i\)</span>, then <span class="math inline">\(F\)</span> is an <strong><span class="math inline">\(F_\sigma\)</span> set</strong>. If <span class="math inline">\(\displaystyle G=\bigcap_{i=1}^\infty G_i\)</span>, then <span class="math inline">\(G\)</span> is a <strong><span class="math inline">\(G_\delta\)</span> set</strong>.</p>
<div class="note info">
            <p><font color="#696969" font size="2"><span class="math inline">\(F\)</span><strong>可能</strong>来自于法语中的fermé，意为闭. <span class="math inline">\(\sigma\)</span><strong>可能</strong>来自于德语中的Summe，意为总和. 因此<span class="math inline">\(F_\sigma\)</span>型集表示闭集的可数并. <span class="math inline">\(G\)</span><strong>可能</strong>来自于德语中的Gebiet，意为区域，可以理解为连通的开集. <span class="math inline">\(\delta\)</span><strong>可能</strong>来自于德语中的Durchschnitt，意为横断. 因此<span class="math inline">\(G_\delta\)</span>型集表示开集的可数交.</font></p>
          </div>
<p><strong>Definition 3.2</strong>    Suppose <span class="math inline">\(\Omega\)</span> is the family of sets including some subsets of <span class="math inline">\(X\)</span>. <span class="math inline">\(\Omega\)</span> is called a <strong><span class="math inline">\(\sigma\)</span>-algebra</strong> if it has the following properties:</p>
<p>    (1) <span class="math inline">\(X \in \Omega\)</span>;</p>
<p>    (2) <span class="math inline">\(\Omega\)</span> is closed under countable union;</p>
<p>    (3) <span class="math inline">\(\Omega\)</span> is closed under complement.</p>
<div class="note info">
            <p><font color="#696969" font size="2">显然，可测集类<span class="math inline">\(\mathscr{M}\)</span>是<span class="math inline">\(\mathbb{R}^n\)</span>上的<span class="math inline">\(\sigma\)</span>-代数.</font></p>
          </div>
<p><strong>Definition 3.3</strong>    Suppose <span class="math inline">\(\Sigma\)</span> is the family of sets including some subsets of <span class="math inline">\(X\)</span>. <strong><span class="math inline">\(\sigma\)</span>-algebra generated by <span class="math inline">\(\Sigma\)</span></strong> is the intersection of all <span class="math inline">\(\sigma\)</span>-algebras containing <span class="math inline">\(\Sigma\)</span> on <span class="math inline">\(X\)</span>.</p>
<p><strong>Definition 3.4</strong>    <strong>Borel <span class="math inline">\(\sigma\)</span>-algebra</strong> is a <span class="math inline">\(\sigma\)</span>-algebra generated by all open sets on <span class="math inline">\(\mathbb{R}^n\)</span>, denoted as <span class="math inline">\(\mathscr{B}\)</span>. The element in <span class="math inline">\(\mathscr{B}\)</span> is called <strong>Borel set</strong>.</p>
<div class="note info">
            <p>A <strong>Borel set</strong> is any set that can be formed from open sets through the operations of countable union, countable intersection, and complement.</p>
          </div>
<p><strong>Example 3.1</strong>    Intervals, open sets, closed sets, <span class="math inline">\(F_\sigma\)</span> sets, and <span class="math inline">\(G_\delta\)</span> sets are Borel sets.</p>
<p><strong>Theorem 3.1</strong>    Suppose <span class="math inline">\(f: E \subset \mathbb{R} \to \mathbb{R}\)</span> is a function, and <span class="math inline">\(E \in \Sigma\)</span>, where <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span>. Let <span class="math display">\[\mathscr{A}=\{X \subset \mathbb{R}: f^{-1}(X) \in \Sigma\},\]</span> then <span class="math inline">\(\mathscr{A}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra.</p>
<p><strong><em>Proof.</em></strong> We check three properties of <span class="math inline">\(\sigma\)</span>-algebra:</p>
<p>(1) Since <span class="math inline">\(f^{-1}(\mathbb{R})=E \in \Sigma\)</span>, then <span class="math inline">\(\mathbb{R} \in \mathscr{A}\)</span>.</p>
<p>(2) Since <span class="math inline">\(\displaystyle f^{-1}\left(\bigcup_{i=1}^\infty X_i\right)=\bigcup_{i=1}^\infty f^{-1}(X_i)\)</span>, then if <span class="math inline">\(X_i \in \mathscr{A}\)</span>, we have <span class="math inline">\(\displaystyle \bigcup_{i=1}^\infty X_i \in \mathscr{A}\)</span>.</p>
<p>(3) Since <span class="math inline">\(f^{-1}(X^c)=(f^{-1}(X))^c\)</span>, then if <span class="math inline">\(X \in \mathscr{A}\)</span>, we have <span class="math inline">\(X^c \in \mathscr{A}\)</span>.</p>
Therefore, <span class="math inline">\(\mathscr{A}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 3.2</strong>    Suppose <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span> is continuous, then the preimage of a Borel set is a Borel set.</p>
<strong><em>Proof.</em></strong> Suppose <span class="math inline">\(\mathscr{B}\)</span> is the Borel <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\mathbb{R}\)</span>, and <span class="math inline">\(\mathscr{A}=\{X \subset \mathbb{R}: f^{-1}(X) \in \mathscr{B}\}\)</span>. We know <span class="math inline">\(\mathscr{A}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra. Since the preimage of an open set is open, then all open sets belong to <span class="math inline">\(\mathscr{A}\)</span>. Since <span class="math inline">\(\mathscr{B}\)</span> is generated by all open sets, then <span class="math inline">\(\mathscr{B} \subset \mathscr{A}\)</span>. Therefore, the preimage of a Borel set is a Borel set.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="family-of-measurable-sets">4. Family of Measurable Sets</h1>
<p><strong>Theorem 4.1</strong>    Any interval <span class="math inline">\(I\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> is measurable.</p>
<p><strong>Theorem 4.2</strong>    Any Borel set is measurable.</p>
<p><strong>Theorem 4.3</strong>    Any zero measure set <span class="math inline">\(A\)</span> is measurable.</p>
<p><strong><em>Proof.</em></strong> For any set <span class="math inline">\(T\)</span>, we have <span class="math inline">\(0 \leq m^*(T \cap A) \leq m^*(A)=0\)</span>, and thus <span class="math inline">\(m^*(T \cap A)=0\)</span>. Besides, <span class="math inline">\(m^*(T \cap A^c) \leq m^*(T)\)</span>. Therefore, <span class="math inline">\(m^*(T) \geq m^*(T \cap A)+m^*(T \cap A^c)\)</span>.</p>
Because of countable subadditivity, we have <span class="math inline">\(m^*(T)=m^*(T \cap A)+m^*(T \cap A^c)\)</span>, i.e., <span class="math inline">\(A\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 4.4</strong>    Suppose <span class="math inline">\(A\)</span> has measure zero. Any subset of <span class="math inline">\(A\)</span> has measure zero.</p>
<strong><em>Proof.</em></strong> Take an arbitrary subset <span class="math inline">\(A&#39;\)</span> of <span class="math inline">\(A\)</span>, then <span class="math inline">\(A&#39; \subset A\)</span>. Hence, <span class="math inline">\(0 \leq m^*(A&#39;) \leq m^*(A)=0\)</span>, i.e., <span class="math inline">\(m^*(A&#39;)=0\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Corollary 4.5</strong>     The at most countable union of zero measure sets has measure zero.</p>
<strong><em>Proof.</em></strong> Suppose <span class="math inline">\(\{A_i\}_{i=1}^\infty\)</span> is a collection of zero measure sets. Because of countable subadditivity, we have <span class="math display">\[0 \leq m^*\left(\bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^\infty m^*(A_i)=0,\]</span> i.e., <span class="math inline">\(\displaystyle m^*\left(\bigcup_{i=1}^\infty A_i\right)=0.\)</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.6</strong>    If <span class="math inline">\(E\)</span> is measurable, <span class="math inline">\(G\)</span> is open, and <span class="math inline">\(K\)</span> is compact, then <span class="math display">\[m(E)=\inf\{m(G): E \subset G\}=\sup\{m(K): E \supset K\}.\]</span> We define <span class="math inline">\(\inf\{m(G): E \subset G\}\)</span> as <strong>external regularity</strong>, and <span class="math inline">\(\sup\{m(K): E \supset K\}\)</span> as <strong>inner regularity</strong>. On the contrary, if <span class="math inline">\(E\)</span> is bounded, and <span class="math inline">\(\inf\{m(G): E \subset G\}=\sup\{m(K): E \supset K\}\)</span>, where <span class="math inline">\(G\)</span> is open and <span class="math inline">\(K\)</span> is compact, then <span class="math inline">\(E\)</span> is measurable.</p>
<div class="note ">
            <p><span id="thm4.6.1"><strong>Lemma 4.6.1</strong></span>    Suppose <span class="math inline">\(E\)</span> is measurable. <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, <span class="math inline">\(\exists G \supset E\)</span>, s.t. <span class="math inline">\(m(G-E)&lt;\varepsilon\)</span>, where <span class="math inline">\(G\)</span> is open.</p><p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an open cover of <span class="math inline">\(E\)</span>, <span class="math inline">\(\{I_i\}_{i=1}^\infty\)</span>, such that <span class="math display">\[\sum_{i=1}^\infty |I_i|&lt;m(E)+\varepsilon.\]</span> Let <span class="math inline">\(\displaystyle G=\bigcup_{i=1}^\infty I_i\)</span>, where <span class="math inline">\(G\)</span> is open, and <span class="math inline">\(\displaystyle m(G) \leq \sum_{i=1}^\infty m(I_i)&lt;m(E)+\varepsilon\)</span>, i.e., <span class="math inline">\(m(G)-m(E)&lt;\varepsilon\)</span>.</p><p>When <span class="math inline">\(m(E)&lt;\infty\)</span>, <span class="math inline">\(m(G)-m(E)=m(G-E)&lt;\varepsilon\)</span>.</p>When <span class="math inline">\(m(E)=\infty\)</span>, let <span class="math inline">\(E_n=E \cap B_n(0)\)</span>, then <span class="math inline">\(\displaystyle E=\bigcup_{i=1}^\infty E_n\)</span>, and <span class="math inline">\(m(E_n)&lt;\infty\)</span>. Hence, <span class="math inline">\(\forall \varepsilon&gt;0\)</span>, <span class="math inline">\(\exists G_n \supset E_n\)</span> s.t. <span class="math display">\[m(G_n-E_n)&lt;\sum_{i=1}^\infty \frac{\varepsilon}{2^n},\]</span> where <span class="math inline">\(G_n\)</span> is open. We also know <span class="math inline">\(\displaystyle G=\bigcup_{n=1}^\infty G_n\)</span> is open, and <span class="math inline">\(G \supset E\)</span>. Since <span class="math display">\[G-E=\bigcup_{n=1}^\infty G_n-\bigcup_{n=1}^\infty E_n \subset \bigcup_{n=1}^\infty (G_n-E_n),\]</span> then <span class="math display">\[m(G-E) \leq \sum_{n=1}^\infty m(G_n-E_n)&lt;\sum_{n=1}^\infty \frac{\varepsilon}{2^n}=\varepsilon.\]</span><p align="right"><span class="math inline">\(\square\)</span></p><p><strong>Lemma 4.6.2</strong>    Suppose <span class="math inline">\(E\)</span> is measurable. Then <span class="math display">\[\forall \varepsilon&gt;0, \exists F \subset E\ \text{s.t.}\ m(E-F)&lt;\varepsilon,\]</span> where <span class="math inline">\(F\)</span> is closed.</p><strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an open set <span class="math inline">\(G \supset E^c\)</span> such that <span class="math inline">\(m(G-E^c)&lt;\varepsilon\)</span>. Let <span class="math inline">\(F=G^c \subset E\)</span>, where <span class="math inline">\(F\)</span> is closed. Therefore, <span class="math inline">\(m(G-E^c)=m(E-F)&lt;\varepsilon\)</span>.<p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong><em>Proof.</em></strong> <strong>External Regularity.</strong> Take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists an open set <span class="math inline">\(G \supset E\)</span> such that <span class="math inline">\(m(G-E)&lt;\varepsilon\)</span>. When <span class="math inline">\(m(E)&lt;\infty\)</span>, <span class="math inline">\(m(G)=m(E)+m(G-E)&lt;m(E)+\varepsilon\)</span>, i.e., <span class="math inline">\(m(E)&gt;m(G)-\varepsilon\)</span>. Since <span class="math inline">\(G \supset E\)</span>, then <span class="math inline">\(m(E) \leq m(G)\)</span>. Hence, <span class="math inline">\(m(E)=\inf\{m(G): E \subset G\}\)</span>.</p>
<p>When <span class="math inline">\(m(E)=\infty\)</span>, it is obvious that <span class="math inline">\(m(G)=\infty\)</span>, and thus <span class="math inline">\(m(E)=\inf\{m(G): E \subset G\}\)</span>.</p>
<p><strong>Inner Regularity.</strong> When <span class="math inline">\(E\)</span> is bounded, take an arbitrary <span class="math inline">\(\varepsilon&gt;0\)</span>, there exists a closed set <span class="math inline">\(K \subset E\)</span> such that <span class="math inline">\(m(E-K)&lt;\varepsilon\)</span>. Since <span class="math inline">\(E\)</span> is bounded, then <span class="math inline">\(K\)</span> is bounded and closed, i.e., <span class="math inline">\(K\)</span> is compact. Besides, we have <span class="math inline">\(m(E)&lt;\infty\)</span>, and therefore <span class="math inline">\(m(E)=m(K)+m(E-K)&lt;m(K)+\varepsilon\)</span>. Since <span class="math inline">\(K \subset E\)</span>, then <span class="math inline">\(m(K) \leq m(E)\)</span>. Hence, <span class="math inline">\(m(E)=\sup\{m(K): E \supset K\}\)</span>.</p>
<p>When <span class="math inline">\(E\)</span> is an arbitrary set, let <span class="math inline">\(E_n=E \cap B_n(0)\)</span>, then <span class="math inline">\(E_n\)</span> is bounded, and <span class="math inline">\(\displaystyle E=\bigcup_{n=1}^\infty E_n\)</span>. Since <span class="math inline">\(\{E_n\}_{n=1}^\infty\)</span> is a collection of increasing measurable sets, then <span class="math inline">\(\displaystyle m(E)=\lim_{n \to \infty} m(E_n)\)</span>. We can take a compact set <span class="math inline">\(K_n \subset E_n\)</span> such that <span class="math inline">\(\displaystyle m(K_n) \geq m(E_n)-\frac{1}{n}\)</span> and <span class="math inline">\(m(K_n) \leq m(E_n)\)</span>. Hence, <span class="math inline">\(\displaystyle \lim_{n \to \infty} m(K_n)=\lim_{n \to \infty} m(E_n)=m(E)\)</span>, and thus <span class="math inline">\(m(E)=\sup\{m(K): E \supset K\}\)</span>.</p>
<strong>Measurability.</strong> Let <span class="math inline">\(\alpha=\inf\{m(G): E \subset G\}=\sup\{m(K): E \supset K\}\)</span>. Take an arbitrary <span class="math inline">\(n \in \mathbb{N}\)</span>, there exists a open set <span class="math inline">\(G_n \supset E\)</span> and compact set <span class="math inline">\(K_n \subset E\)</span> such that <span class="math inline">\(\displaystyle m(G_n)&lt;\alpha+\frac{1}{2n}\)</span> and <span class="math inline">\(\displaystyle m(K_n)&gt;\alpha-\frac{1}{2n}\)</span>. Hence, <span class="math display">\[0 \leq m(G_n)-m(K_n)&lt;\frac{1}{n}.\]</span> Take <span class="math inline">\(\displaystyle A=\bigcap_{n=1}^\infty G_n\)</span> and <span class="math inline">\(\displaystyle B=\bigcup_{n=1}^\infty K_n\)</span>, then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are measurable. Since <span class="math inline">\(m(A) \leq m(G_n)\)</span>, <span class="math inline">\(m(B) \geq m(K_n)\)</span>, and <span class="math inline">\(m(A) \geq m(B)\)</span>, then <span class="math display">\[0 \leq m(A)-m(B) \leq m(G_n)-m(K_n)&lt;\frac{1}{n},\]</span> i.e., <span class="math inline">\(m(A)=m(B)\)</span>. Since <span class="math inline">\(E \supset B\)</span> is bounded, then <span class="math inline">\(m(B)&lt;\infty\)</span>, and <span class="math inline">\(m(A-B)=m(A)-m(B)=0\)</span>. Since <span class="math inline">\(E-B \subset A-B\)</span>, then <span class="math inline">\(m(E-B)=0\)</span>. Therefore, <span class="math inline">\(E=B \cup (E-B)\)</span> is measurable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm4.7"><strong>Theorem 4.7</strong></span>    If <span class="math inline">\(E\)</span> is measurable, then there exists a <span class="math inline">\(G_\delta\)</span> set <span class="math inline">\(G\)</span> such that <span class="math inline">\(G \supset E\)</span> and <span class="math inline">\(m(G-E)=0\)</span>.</p>
<strong><em>Proof.</em></strong> Take an arbitrary <span class="math inline">\(n \in \mathbb{N}\)</span>. By <a href="#thm4.6.1">lemma 4.6.1</a>, there exists an open set <span class="math inline">\(G \supset E\)</span> such that <span class="math inline">\(\displaystyle m(G-E)&lt;\frac{1}{n}\)</span>. Take <span class="math inline">\(\displaystyle G=\bigcap_{n=1}^\infty G_n\)</span>, which is a <span class="math inline">\(G_\delta\)</span> set. Since <span class="math inline">\(G-E \subset G_n-E\)</span>, then <span class="math display">\[m(G-E) \leq m(G_n-E)&lt;\frac{1}{n},\]</span> i.e., <span class="math inline">\(m(G-E)=0\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm4.8"><strong>Corollary 4.8</strong>    If <span class="math inline">\(E\)</span> is measurable, then there exists an <span class="math inline">\(F_\sigma\)</span> set <span class="math inline">\(F\)</span> such that <span class="math inline">\(F \subset E\)</span> and <span class="math inline">\(m(E-F)=0\)</span>.</span></p>
<div class="note info">
            <p><font color="#696969" font size="2">根据<a href="#thm4.7">定理4.7</a>与其<a href="#thm4.8">推论</a>，如果我们有了所有的<span class="math inline">\(G_\delta\)</span>型集（或<span class="math inline">\(F_\sigma\)</span>型集）与零测集，那么我们就知道了所有的可测集. 可测集一定可以表示为<span class="math display">\[E=G_\delta-M_0=F_\sigma \cup M_0.\]</span></font></p>
          </div>
<h1 id="non-measurable-set-in-mathbbr">5. Non-Measurable Set in \(\mathbb{R}\)</h1>
<div class="note default">
            <p><font color="#696969" font size="2">此前，我们一直在回答什么是可测集，并发现可测集是存在的——我们构造集合通常都是从区间出发，经过一系列并、交、差等运算来获得，而这样的集合都是Borel集，总是可测的. 我们还需要回答不可测集是否存在的问题——我们可以利用Lebesgue测度的平移不变性，构造一个不可测集. 当然，在一般的数学实践中，遇到不可测集的机会是极少的，它通常只是被用来构成各种特例.</font></p>
          </div>
<p><strong>Property 5.1 (Translation Invariance)</strong>    Suppose <span class="math inline">\(E\)</span> is measurable and <span class="math inline">\(\tau_\alpha E=\{x+\alpha: x \in E\}\)</span>, then <span class="math inline">\(\tau_\alpha E\)</span> is measurable and <span class="math inline">\(m^*(E)=m^*(\tau_\alpha E)\)</span>.</p>
<p><strong>Property 5.2 (Reflection Invariance)</strong>    Suppose <span class="math inline">\(E\)</span> is measurable and <span class="math inline">\(\tau E=\{-x: x \in E\}\)</span>, then <span class="math inline">\(\tau E\)</span> is measurable and <span class="math inline">\(m^*(E)=m^*(\tau E)\)</span>.</p>
<p><strong>Example 5.1</strong>    We define a relation on <span class="math inline">\([0, 1]\)</span>: <span class="math inline">\(x \sim y \Leftrightarrow x-y \in \mathbb{Q}\)</span>, which is an equivalence relation. From this equivalence relation, we can find equivalence classes. We take a representative element from each equivalence class (<strong>Axiom of choice</strong>). Let <span class="math inline">\(Z=\{\text{All representative elements}\}\)</span>. Take an arbitrary <span class="math inline">\(\xi \in Z\)</span>. All elements in the class including <span class="math inline">\(\xi\)</span> have the form of <span class="math inline">\(\xi+r \in [0, 1]\)</span>, where <span class="math inline">\(r \in \mathbb{Q} \cap [-\xi, 1-\xi]\)</span>.</p>
<p>Therefore, <span class="math inline">\(\displaystyle \bigcup_{r \in \mathbb{Q} \cap [-\xi, 1-\xi]} \{\tau_r \xi\}\)</span> represents all elements in the class including <span class="math inline">\(\xi\)</span>, and <span class="math inline">\(\displaystyle \bigcup_{r \in \mathbb{Q} \cap [-1, 1]} \tau_r Z \supset [0, 1]\)</span>.</p>
<p>If <span class="math inline">\(\xi \in \tau_{r_1} Z \cap \tau_{r_2} Z\)</span>, where <span class="math inline">\(r_1 \neq r_2\)</span>, then <span class="math inline">\(\xi-r_1, \xi-r_2 \in Z\)</span>. Since <span class="math inline">\(\xi-r_1 \sim \xi-r_2\)</span>, then we have two representative elements for one equivalence class, which is a contradiction. Therefore, <span class="math inline">\(\tau_{r_1} Z \cap \tau_{r_2} Z=\varnothing\)</span>.</p>
<p>We know order <span class="math inline">\(\mathbb{Q} \cap [-1, 1]\)</span>, denoted <span class="math inline">\(r_1, r_2, \ldots\)</span>. Let <span class="math inline">\(Z_n=\tau_{r_n} Z\)</span>. We know <span class="math inline">\(Z_n \subset [-1, 2]\)</span>, is bounded and mutually disjoint from <span class="math inline">\(Z_{n&#39;}\)</span>, where <span class="math inline">\(n \neq n&#39;\)</span>. Hence, <span class="math inline">\(\displaystyle [0, 1] \subset \bigcup_{n=1}^\infty Z_n\)</span>. If <span class="math inline">\(Z\)</span> is measurable, then <span class="math inline">\(Z_n\)</span> is measurable, and <span class="math display">\[m^*\left(\bigcup_{n=1}^\infty Z_n\right)=\sum_{n=1}^\infty m^*(Z_n)=\sum_{n=1}^\infty m^*(Z)&lt;\infty.\]</span> Therefore, <span class="math display">\[m^*(Z)=0=m^*\left(\bigcup_{n=1}^\infty Z_n\right),\]</span> which is a contradiction. As a consequence, <span class="math inline">\(Z\)</span> is a non-measurable set, a.k.a. <strong>Vitali set</strong>.</p>
<div class="note info">
            <p>There will be non-measurable subsets in any set with measure greater than zero, and we can also construct non-measurable sets in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
          </div>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Measure</tag>
      </tags>
  </entry>
  <entry>
    <title>Set</title>
    <url>/Mathematics/Real-analysis/Set.html</url>
    <content><![CDATA[<h1 id="set-operation">1. Set Operation</h1>
<h2 id="symmetric-difference">1.1. Symmetric Difference</h2>
<p><strong>Definition 1.1</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are two sets. The <strong>symmetric difference</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, denoted <span class="math inline">\(A \triangle B\)</span>, is the set <span class="math inline">\((A-B) \cup (B-A)\)</span>. The symmetric difference of two sets, a.k.a. the <strong>disjunctive union</strong> is the set of elements which are in either of the sets, but not in their intersection.</p>
<p><strong>Property 1.1</strong>    Suppose <span class="math inline">\(A, B\)</span> and <span class="math inline">\(C\)</span> are sets, and <span class="math inline">\(X\)</span> is a <strong>universal set</strong>:</p>
<p>    (1) <span class="math inline">\(A \cup B=(A \cap B) \cup (A \triangle B)\)</span>;</p>
<p>    (2) <span class="math inline">\(A \triangle \varnothing=A, A \triangle A=\varnothing, A \triangle A^c=X, A \triangle X=A^c\)</span>;</p>
<p>    (3) <span class="math inline">\(A \triangle B=B \triangle A\)</span>;</p>
<p>    (4) <span class="math inline">\((A \triangle B) \triangle C=A \triangle (B \triangle C)\)</span>;</p>
<p>    (5) <span class="math inline">\(A \cap (B \triangle C)=(A \cap B) \triangle (A \cap C)\)</span>;</p>
<p>    (6) <span class="math inline">\(A^c \triangle B^c= A \triangle B\)</span>;</p>
<p>    (7) <span class="math inline">\(A=A \triangle B \Leftrightarrow B=\varnothing\)</span>;</p>
<p>    (8) For any <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, there exists an only set <span class="math inline">\(E\)</span> such that <span class="math inline">\(E \triangle A=B.\)</span> In fact, <span class="math inline">\(E=B \triangle A\)</span>.</p>
<h2 id="limit-of-a-sequence-of-sets">1.2. Limit of a Sequence of Sets</h2>
<p><strong>Definition 1.2</strong>    Suppose <span class="math inline">\(\{A_k\}\)</span> is a sequence of sets. If <span class="math display">\[A_1 \supset \cdots \supset A_k \supset \cdots,\]</span> then <span class="math inline">\(\{A_k\}\)</span> is a <strong>decreasing sequence of sets</strong>. The limit of <span class="math inline">\(\{A_k\}\)</span>, denoted <span class="math inline">\(\displaystyle \lim_{k \to \infty} A_k\)</span>, is <span class="math inline">\(\displaystyle \bigcap_{k=1}^\infty A_k\)</span>.</p>
<p>If <span class="math display">\[A_1 \subset \cdots \subset A_k \subset \cdots,\]</span> then <span class="math inline">\(\{A_k\}\)</span> is a <strong>increasing sequence of sets</strong>. The limit of <span class="math inline">\(\{A_k\}\)</span>, denoted <span class="math inline">\(\displaystyle \lim_{k \to \infty} A_k\)</span>, is <span class="math inline">\(\displaystyle \bigcup_{k=1}^\infty A_k\)</span>.</p>
<p><strong>Example 1.1</strong>    Suppose <span class="math inline">\(A_n=[n, \infty), n \in \mathbb{N}\)</span>, then <span class="math inline">\(\displaystyle \lim_{n \to \infty} A_n=\varnothing\)</span>.</p>
<p><strong>Example 1.2</strong>    Suppose <span class="math display">\[f_1(x) \leq \cdots \leq f_n(x) \leq \cdots, x \in \mathbb{R},\]</span> and <span class="math inline">\(\displaystyle \lim_{n \to \infty} f_n(x)=f(x).\)</span> For a given real number <span class="math inline">\(t\)</span>, let <span class="math display">\[E_n=\{x: f_n(x)&gt;t\}.\]</span> Obviously, <span class="math inline">\(E_1 \subset \cdots \subset E_n \subset \cdots\)</span>, and <span class="math display">\[\lim_{n \to \infty} E_n=\bigcup_{n=1}^\infty \{x: f_n(x)&gt;t\}=\{x: f(x)&gt;t\}.\]</span></p>
<p><strong>Definition 1.3</strong>    Suppose <span class="math inline">\(\{A_k\}\)</span> is a sequence of sets. Let <span class="math display">\[B_j=\bigcup_{k=j}^\infty A_k, j \in \mathbb{N}.\]</span> Since <span class="math inline">\(B_j \supset B_{j+1}\)</span>, then we define <span class="math display">\[\lim_{j \to \infty} B_j=\lim_{j \to \infty}\bigcup_{k=j}^\infty A_k=\bigcap_{j=1}^\infty B_j=\bigcap_{j=1}^\infty\bigcup_{k=j}^\infty A_k\]</span> as the <strong>upper limit of sets</strong> of <span class="math inline">\(\{A_k\}\)</span>, denoted as <span class="math inline">\(\displaystyle \varlimsup_{k \to \infty} A_k.\)</span></p>
<p>Similarly, we define <span class="math display">\[\varliminf_{k \to \infty} A_k= \bigcup_{j=1}^\infty \bigcap_{k=j}^\infty A_k\]</span> as the <strong>lower limit of sets</strong> of <span class="math inline">\(\{A_k\}\)</span>.</p>
<p>If <span class="math inline">\(\displaystyle \varlimsup_{k \to \infty} A_k=\varliminf_{k \to \infty} A_k\)</span>, then we say the limit of <span class="math inline">\(\{A_k\}\)</span> exists, denoted as <span class="math inline">\(\displaystyle \lim_{k \to \infty} A_k\)</span>.</p>
<p><strong>Example 1.3</strong>    Suppose <span class="math inline">\(E\)</span> and <span class="math inline">\(F\)</span> are sets. Let <span class="math display">\[A_k=\begin{cases}
E, &amp;k\ \text{is odd}, \\
F, &amp;k\ \text{is even}.
\end{cases}\]</span> Then we have <span class="math display">\[\varliminf_{k \to \infty} A_k=E \cup F, \varliminf_{k \to \infty} A_k=E \cap F.\]</span></p>
<p><strong>Theorem 1.2</strong>    Suppose <span class="math inline">\(\{A_k\}\)</span> is a sequence of sets, then <span class="math display">\[\varlimsup_{k \to \infty} A_k=\{x: \forall j \in \mathbb{N}, \exists k \geq j\ \text{s.t.}\ x \in A_k\},\]</span> and <span class="math display">\[\varliminf_{k \to \infty} A_k=\{x: \exists j_0 \in \mathbb{N}\ \text{s.t.}\ k \geq j_0 \Rightarrow x \in A_k\}.\]</span></p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(\displaystyle x \in \varlimsup_{k \to \infty} A_k=\bigcap_{j=1}^\infty \bigcup_{k=j}^\infty A_k.\)</span> This is equivalent to <span class="math display">\[\forall j \in \mathbb{N}, x \in \bigcup_{k=j}^\infty A_k,\]</span> which is equivalent to <span class="math display">\[\forall j \in \mathbb{N}, \exists k \geq j\ \text{s.t.}\ x \in A_k.\]</span></p>
Suppose <span class="math inline">\(\displaystyle x \in \varliminf_{k \to \infty} A_k=\bigcup_{j=1}^\infty \bigcap_{k=j}^\infty A_k.\)</span> This is equivalent to <span class="math display">\[\exists j_0 \in \mathbb{N}\ \text{s.t.}\ x \in \bigcap_{k=j_0}   ^\infty A_k,\]</span> which is equivalent to <span class="math display">\[\exists j_0 \in \mathbb{N}\ \text{s.t.}\ k \geq j_0 \Rightarrow x \in A_k.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The upper limit of <span class="math inline">\(\{A_k\}\)</span> includes the elements that belong to an infinite number of sets in <span class="math inline">\(\{A_k\}\)</span>. The lower limit of <span class="math inline">\(\{A_k\}\)</span> includes the elements that do not belong to only finitely many sets in <span class="math inline">\(\{A_k\}\)</span>. Therefore, <span class="math inline">\(\displaystyle \varlimsup_{k \to \infty} A_k \subset \varliminf_{k \to \infty} A_k.\)</span></p>
          </div>
<p><strong>Example 1.4</strong>    Suppose <span class="math display">\[A_{2m+1}=\left[0, 2-\frac{1}{2m+1}\right], m=0, 1, \ldots\]</span> and <span class="math display">\[A_{2m}=\left[0, 1+\frac{1}{2m}\right], m=1, 2, \ldots.\]</span> Find <span class="math inline">\(\displaystyle \varlimsup_{n \to \infty} A_n\)</span> and <span class="math inline">\(\displaystyle \varliminf_{n \to \infty} A_n\)</span>.</p>
<p><strong><em>Solution.</em></strong> Obviously, <span class="math inline">\([0, 1] \subset A_n\)</span>. For every <span class="math inline">\(x \in (1, 2)\)</span>, when <span class="math inline">\(m \to \infty\)</span>, there are infinite many sets in <span class="math inline">\(\{A_{2m}\}\)</span> such that <span class="math inline">\(x \notin A_{2m}\)</span>, and thus <span class="math inline">\(\displaystyle x \notin \varliminf_{n \to \infty} A_n\)</span>. There are infinite many sets in <span class="math inline">\(\{A_{2m+1}\}\)</span> such that <span class="math inline">\(x \in A_{2m+1}\)</span>, and thus <span class="math inline">\(\displaystyle \varlimsup_{n \to \infty} A_n=[0, 2)\)</span> and <span class="math inline">\(\displaystyle\varliminf_{n \to \infty} A_n=[0, 1]\)</span>.</p>
<p><strong>Example 1.5</strong>    Use set notation to represent <span class="math inline">\(\displaystyle \lim_{n \to \infty} a_n=a\)</span>.</p>
<p><strong><em>Solution.</em></strong> Since <span class="math inline">\(\displaystyle \lim_{n \to \infty} a_n=a\)</span>, then <span class="math display">\[\forall \varepsilon&gt;0, \exists N \in \mathbb{N}\ \text{s.t.}\ \forall n \geq N, |a_n-a|&lt;\varepsilon.\]</span> Therefore <span class="math display">\[a \in \bigcap_{\varepsilon \in \mathbb{R}^+} \bigcup_{N=1}^\infty \bigcap_{n \geq N} \{x: |a_n-x|&lt;\varepsilon\}=\bigcap_{\varepsilon \in \mathbb{R}^+} \varliminf_{n \to \infty} \{x: |a_n-x|&lt;\varepsilon\}.\]</span></p>
<h1 id="cardinal-number">2. Cardinal Number</h1>
<p><strong>Definition 2.1</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets. If there exists a bijection from <span class="math inline">\(A\)</span> to <span class="math inline">\(B\)</span>, then we say <span class="math inline">\(A\)</span> is <strong>equipotent</strong> to <span class="math inline">\(B\)</span>, denoted as <span class="math inline">\(A \sim B\)</span>. Specifically, <span class="math inline">\(\varnothing \sim \varnothing\)</span>.</p>
<p><strong>Property 2.1</strong>    An equipotent relationship is an equivalence relation, i.e., for any sets <span class="math inline">\(A, B\)</span> and <span class="math inline">\(C\)</span>:</p>
<p>    (1) <span class="math inline">\(A \sim A\)</span>;</p>
<p>    (2) If <span class="math inline">\(A \sim B\)</span>, then <span class="math inline">\(B \sim A\)</span>;</p>
<p>    (3) If <span class="math inline">\(A \sim B\)</span> and <span class="math inline">\(B \sim C\)</span>, then <span class="math inline">\(A \sim C\)</span>.</p>
<p><strong>Definition 2.2</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets. If <span class="math inline">\(A \sim B\)</span>, then the <strong>cardinal number</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are equal, denoted as <span class="math inline">\(\overset{=}{A}=\overset{=}{B}\)</span>.</p>
<div class="note info">
            <p><font color="#696969" font size="2">基数的概念可以看作有限集合中所含元素个数的推广. 对于一个集合<span class="math inline">\(A\)</span>，我们考虑所有与<span class="math inline">\(A\)</span>对等的集合所构成的类. 公理集合论允许我们从每一个这样的类中按确定的方式选出一个代表（即所谓良序集——依某种意义排好了次序的集），我们把这个代表定义为这一类中各个集合的基数，记之为<span class="math inline">\(\overset{=}{A}\)</span>.</font></p>
          </div>
<p><strong>Definition 2.3</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are sets. If <span class="math inline">\(A \not\sim B\)</span>, and <span class="math inline">\(B^* \sim A\)</span> where <span class="math inline">\(B^* \subset B\)</span>, then <span class="math inline">\(\overset{=}{A}&lt;\overset{=}{B}\)</span> or <span class="math inline">\(\overset{=}{B}&gt;\overset{=}{A}\)</span>.</p>
<div class="note info">
            <p>We can show that for any two sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, one and only one of the relations holds: <span class="math display">\[\overset{=}{A}&lt;\overset{=}{B}, \overset{=}{A}=\overset{=}{B}, \overset{=}{A}&gt;\overset{=}{B}.\]</span></p>
          </div>
<p><strong>Theorem 2.2 (Cantor-Bernstein Theorem)</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are non-empty sets. If <span class="math inline">\(A\)</span> is equipotent to a subset of <span class="math inline">\(B\)</span>, and <span class="math inline">\(B\)</span> is equipotent to a subset of <span class="math inline">\(A\)</span>, then <span class="math inline">\(A \sim B\)</span>.</p>
<p><strong>Corollary 2.3</strong>    If <span class="math inline">\(A \supset B \supset C\)</span> and <span class="math inline">\(A \sim C\)</span>, then <span class="math inline">\(A \sim B\)</span>.</p>
<h1 id="countable-set-and-uncountable-set">3. Countable Set and Uncountable Set</h1>
<p><strong>Definition 3.1</strong>    The <strong>countable sets</strong> are the sets that are equipotent to <span class="math inline">\(\mathbb{N}\)</span>. Note that <span class="math inline">\(\overset{=}{\mathbb{N}}=\aleph_0\)</span>.</p>
<p><strong>Definition 3.2</strong>    The <strong>uncountable sets</strong> are the sets that are not countable sets.</p>
<div class="note info">
            <p><font color="#696969" font size="2">关于可数集与不可数集的相关定理与推论在抽象数学一课中已经进行了详细探讨，在此不再赘述. 但我们仍应当记得——任何无限集合都至少包含一个可数子集，也意味着可数集在所有无限集中有最小的基数；可数集的任何无限子集仍是可数集；至多可数个至多可数集的并仍是至多可数集；<span class="math inline">\(\mathbb{Q}\)</span>是可数集；代数数集<span class="math inline">\(\mathcal{A}\)</span>是可数集；有限个至多可数集的直积是至多可数集；<span class="math inline">\(\mathbb{R}\)</span>是不可数集且<strong>连续基数</strong>记为<span class="math inline">\(\overset{=}{\mathbb{R}}=c&gt;\aleph_0\)</span>；至多可数个不可数集的并或直积是不可数集；值得特别注意的是：可数个至多可数集的直积<strong>不一定</strong>可数；<span class="math inline">\(c\)</span>个基数为<span class="math inline">\(c\)</span>的集合的并的基数仍然是<span class="math inline">\(c\)</span>.</font></p>
          </div>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
  <entry>
    <title>Optimization Algorithm for Training DNN</title>
    <url>/Computer-science/Deep-learning/Optimization_Algorithm_for_Training_DNN.html</url>
    <content><![CDATA[<h1 id="dnn-loss-function">1. DNN Loss Function</h1>
<p>Consider a fully connected <span class="math inline">\(L\)</span> layer deep network given by <span class="math display">\[\mathbf{h}^{(\mathscr{l})}=W^{(\mathscr{l})}\mathbf{z}^{(\mathscr{l})}+\mathbf{b}^{(\mathscr{l})}, \mathbf{z}^{(\mathscr{l}+1)}=\phi(\mathbf{h}^{(\mathscr{l})}), \mathscr{l}=0, \ldots, L-1,\]</span> for <span class="math inline">\(\mathscr{l}=1, \ldots, L\)</span> with nonlinear activation <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(W^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l} \times n_{\mathscr{l}-1}}\)</span>. The trainable parameters for DNN are <span class="math inline">\(\theta:=\{W^{(\mathscr{l})}, \mathbf{b}^{(\mathscr{l})}\}_{\mathscr{l}=1}^L\)</span>, which are learned by minimizing a high dimensional (<span class="math inline">\(|\theta| \sim n^2L\)</span>) loss function <span class="math inline">\(\mathcal{L}\)</span>.</p>
<p>The shape of <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span> and our knowledge about a good initial minimizer <span class="math inline">\(\theta^{(0)}\)</span> strongly influence our ability to learn the parameters <span class="math inline">\(\theta\)</span> for the DNN.</p>
<p>Let <span class="math inline">\(\displaystyle \delta_\mathscr{l}:=\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(\mathscr{l})}}\)</span> and <span class="math inline">\(D^{(\mathscr{l})}\)</span> be the diagonal matrix with <span class="math inline">\(D_{ii}^{(\mathscr{l})}=\phi&#39;(h_i^{(\mathscr{l})})\)</span>, we have <span class="math display">\[\delta_\mathscr{l}=D^{(\mathscr{l})}(W^{(\mathscr{l})})^T\delta_{\mathscr{l}+1} \quad \text{and} \quad \delta_L=D^L\nabla_{\mathbf{h}^{(L)}} \mathcal{L}\]</span> which gives the formula for computing the <span class="math inline">\(\delta_\mathscr{l}\)</span> for each layer as <span class="math display">\[\delta_\mathscr{l}=\left(\prod_{k=\mathscr{l}}^{L-1} D^{(k)}(W^{(k)})^T\right)D^{L}\nabla_{\mathbf{h}^{(L)}}\mathcal{L}\]</span> and the resulting gradient <span class="math inline">\(\nabla_\theta \mathcal{L}\)</span> with entries as <span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(\mathscr{l})}} = \delta_{\mathscr{l}+1} \cdot \mathbf{h}_\mathscr{l}^T \quad \text{and} \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\mathscr{l})}} = \delta_{\mathscr{l}+1}.\]</span></p>
<h1 id="stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h1>
<p>Given a loss function <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span>, <strong>gradient descent</strong> is given by <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \alpha \cdot \nabla_\theta \mathcal{L}(\theta; X, Y)\]</span> with <span class="math inline">\(\alpha\)</span> is called the <strong>learning rate</strong>. In deep learning, <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span> is the sum of <span class="math inline">\(m\)</span> individual loss functions for <span class="math inline">\(m\)</span> data point <span class="math display">\[\mathcal{L}(\theta; X, Y)=m^{-1}\sum_{\mu=1}^m l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu).\]</span> For <span class="math inline">\(m \gg 1\)</span>, gradient descent is computationally too costly and instead one can break apart the <span class="math inline">\(m\)</span> loss functions into mini-batches, and repeatedly solve <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \alpha |\mathcal{S}_k|^{-1} \nabla_\theta \sum_{\mu \in \mathcal{S}_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu),\]</span> which is called <strong>stochastic gradient descent</strong> as typically <span class="math inline">\(\mathcal{S}_k\)</span> is chosen in some randomized method, usually as a partition of <span class="math inline">\([m]\)</span> and a sequence of <span class="math inline">\(\mathcal{S}_k\)</span> which cover <span class="math inline">\([m]\)</span> is called an <strong>epoch</strong>.</p>
<h2 id="challenge-and-benefit">2.1. Challenge and Benefit</h2>
<ul>
<li><p>SGD is preferable for large <span class="math inline">\(m\)</span> since it reduces the per iteration computational cost depending on <span class="math inline">\(m\)</span> to instead depending on <span class="math inline">\(|\mathcal{S}_k|\)</span>.</p></li>
<li><p>SGD and gradient descent require selection of a learning rage which in deep learning is typically selected using some costly trial and error heuristics.</p></li>
<li><p>The learning rate is typically chosen adaptively in a way that satisfies <span class="math display">\[\sum_{k=1}^\infty \alpha_k=\infty \quad \text{and} \quad \sum_{k=1}^\infty \alpha_k^2&lt;\infty.\]</span> In particular, <span class="math inline">\(\alpha_k \sim k^{-1}\)</span>.</p></li>
<li><p>The optimal selection of learning weight and selection of <span class="math inline">\(\mathcal{S}_k\)</span> depend on the unknown local Lipschitz constant <span class="math display">\[\| \nabla l(\theta_1; \mathbf{x}_\mu, \mathbf{y}_\mu) - \nabla l(\theta_2; \mathbf{x}_\mu, \mathbf{y}_\mu) \| \leq L_\mu \| \theta_1 - \theta_2 \|.\]</span></p></li>
</ul>
<h2 id="global-convergence-of-gradient-descent">2.2. Global Convergence of Gradient Descent</h2>
<p><span id="thm2.1"><strong>Lemma 2.1(Overestimation Property)</strong></span>    Let <span class="math inline">\(\mathcal{L}(\theta) \in \mathcal{C}^1(\mathbb{R}^n)\)</span> with <span class="math inline">\(\nabla \mathcal{L}\)</span> Lipschitz continuous with constant <span class="math inline">\(L\)</span>. Then for any <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mathbf{d} \in \mathbb{R}^n\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>: <span class="math display">\[\mathcal{L}(\theta + \alpha \mathbf{d}) \leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T \mathbf{d} + \alpha^2 \frac{L}{2} \|\mathbf{d}\|^2.\]</span> In particular, if <span class="math inline">\(\mathbf{d}=-\nabla \mathcal{L}(\theta)\)</span>, then <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta - \alpha \nabla \mathcal{L}(\theta)) &amp;\leq \mathcal{L}(\theta) - \alpha \| \nabla \mathcal{L}(\theta) \|^2 + \frac{L}{2} \alpha^2 \| \nabla \mathcal{L}(\theta) \|^2 \\
&amp;=\mathcal{L}(\theta) - \alpha \left(1 - \frac{L}{2}\alpha \right) \| \nabla \mathcal{L}(\theta) \|^2.
\end{aligned}\]</span></p>
<strong><em>Proof.</em></strong> By Taylor's theorem in integral form, we have <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta+\alpha\mathbf{d}) &amp;= \mathcal{L}(\theta)+\int_0^1 \nabla \mathcal{L}(\theta+\alpha t \mathbf{d})^T (\alpha\mathbf{d}) \text{d}t
\\&amp;=\mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha \int_0^1 [\nabla \mathcal{L}(\theta+\alpha t \mathbf{d})-\nabla \mathcal{L}(\theta)]^T\mathbf{d} \text{d}t 
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha \int_0^1 \| \nabla \mathcal{L}(\theta+\alpha t \mathbf{d})-\nabla \mathcal{L}(\theta) \| \cdot \| \mathbf{d} \| \text{d}t 
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha L \|\mathbf{d}\| \int_0^1 \|\theta+\alpha t \mathbf{d}-\theta \|\text{d}t
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha^2 L \|\mathbf{d}\|^2 \int_0^1 t\text{d}t.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="global-convergence-of-sgd">2.3. Global Convergence of SGD</h2>
<p>Suppose for convenience that <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, the expected gradient w.r.t. data point <span class="math inline">\(\displaystyle G^k:=\nabla_\theta \sum_{\mu \in \mathcal{S}_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu)\)</span> is <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[G^k]=\mathbb{E}[G^k \mid \mathcal{S}_k]=\sum_{i=1}^m \mathbb{E}[G^k \mid \mathcal{S}_k=i]P(\mathcal{S}_k=i)=\sum_{i=1}^m \nabla l_i(\theta^k) \cdot \frac{1}{m}=\nabla \mathcal{L}(\theta^k).\]</span></p>
<p>We also assume (<span class="math inline">\(|\mathcal{S}_k|=1\)</span>):</p>
<p><span id="a1">    (1) for all <span class="math inline">\(i \leq m\)</span>, <span class="math inline">\(\nabla l_i\)</span> is Lipschitz continuous with constant <span class="math inline">\(L\)</span>;</span></p>
<p><span id="a2">    (2) there exists <span class="math inline">\(M&gt;0\)</span> s.t. <span class="math inline">\(\text{Var}[G^k \mid \mathcal{S}_k]:=\mathbb{E}[(G^k-\nabla l(\theta^k))^T(G^k- \nabla l(\theta^k)) \mid \mathcal{S}_k] \leq M\)</span> for all <span class="math inline">\(k\)</span>.</span></p>
<div class="note warning">
            <p>Bounded total variance can usually be guaranteed in a neighborhood of <span class="math inline">\(\theta^*\)</span>, but not globally for strongly convex <span class="math inline">\(\mathcal{L}(\cdot)\)</span>.</p><p>Recall that <span class="math inline">\(G^k\)</span> conditioned on current batch is an unbiased estimator of the true gradient, which is true (and for <span class="math inline">\(|\mathcal{S}_k|&gt;1\)</span>), but it would have to be assumed in a more general stochastic framework.</p>
          </div>
<p><span id="thm2.2"><strong>Lemma 2.2 (Overestimation Property in Expectation)</strong></span>    Assume <a href="#a1">assumption (1)</a> holds. When applying SGD to <span class="math inline">\(\mathcal{L}(\theta)\)</span> with <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, we have <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha \nabla\mathcal{L}(\theta^k)^T \mathbb{E}_{\mathcal{S}_k}[G^k] + \frac{L\alpha^2}{2}\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2].\]</span> If <a href="#a2">assumption (2)</a> also holds, then <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha^k\left(\frac{L\alpha^k}{2}-1\right) \|\nabla \mathcal{L}(\theta^k)\|^2+\frac{ML(\alpha^k)^2}{2}.\]</span></p>
<p><strong><em>Proof.</em></strong> Apply <a href="#thm2.1">lemma 2.1</a> to <span class="math inline">\(\mathcal{L}\)</span> with <span class="math inline">\(\theta=\theta^k\)</span>, <span class="math inline">\(\mathbf{d}=G^k\)</span> and <span class="math inline">\(\alpha=\alpha^k\)</span>. Using <span class="math inline">\(\theta^{k+1}=\theta^k-\alpha^kG^k\)</span>, we have <span class="math display">\[\mathcal{L}(\theta^{k+1}) \leq \mathcal{L}(\theta^k)-\alpha^k \nabla\mathcal{L}(\theta^k)^TG^k+\frac{L}{2}(\alpha^k)^2\|G^k\|^2.\]</span></p>
Applying expectation on both sides w.r.t. <span class="math inline">\(\mathcal{S}_k\)</span>: <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha^k \nabla\mathcal{L}(\theta^k)^T\mathbb{E}_{\mathcal{S}_k}[G^k]+\frac{L}{2}(\alpha^k)^2 \mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2]\]</span> where <span class="math inline">\(\mathcal{L}(\theta^k)\)</span> and <span class="math inline">\(\nabla \mathcal{L}(\theta^k)\)</span> do not depend on <span class="math inline">\(\mathcal{S}_k\)</span>. Since <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[G^k]=\nabla \mathcal{L}(\theta^k)\)</span>, then <span class="math display">\[\begin{aligned}
\text{Var}[G^k \mid \mathcal{S}_k]&amp;=\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2] - 2\nabla \mathcal{L}(\theta^k)^T \mathbb{E}_{\mathcal{S}_k}[G^k] + \|\nabla \mathcal{L}(\theta^k)\|^2
\\&amp;=\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2]-\|\nabla \mathcal{L}(\theta^k)\|^2,
\end{aligned}\]</span> and thus <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2] \leq M + \|\nabla \mathcal{L}(\theta^k)\|^2\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm2.3"><strong>Theorem 2.3</strong></span>    Let <span class="math inline">\(\mathcal{L}\)</span> be smooth, <strong>strongly convex</strong>, i.e., for <span class="math inline">\(\mu&gt;0\)</span>, <span class="math inline">\(\displaystyle \mathcal{L}(\mathbf{x}+\mathbf{s}) \geq \mathcal{L}(\mathbf{x})+\mathbf{s}^T \nabla\mathcal{L}(\mathbf{x})+\frac{\mu}{2}\|\mathbf{s}\|^2\)</span> for all <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{s}\)</span>, and satisfying <a href="#a1">assumption (1)</a> and <a href="#a2">(2)</a>. Let SGD with fixed learning rate be applied to minimize <span class="math inline">\(\mathcal{L}\)</span>, where <span class="math inline">\(\displaystyle \alpha^k=\alpha=\frac{\eta}{L}\)</span> and <span class="math inline">\(\eta \in (0, 1]\)</span>. Then SGD converges linearly to a residual error: for all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu} \leq \left(1-\frac{\eta\mu}{L}\right)^k \cdot \left[\mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)-\frac{\eta M}{2 \mu}\right].\]</span></p>
<p><strong><em>Proof.</em></strong> <a href="#thm2.2">Lemma 2.2</a> and <span class="math inline">\(\displaystyle \frac{L\alpha}{2}-1=\frac{\eta}{2}-1&lt;-\frac{1}{2}\)</span> give <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\frac{\alpha}{2} \|\nabla \mathcal{L}(\theta^k)\|^2+\frac{ML\alpha^2}{2}.\]</span> Taking expectation w.r.t the past, i.e., <span class="math inline">\(\mathcal{S}_0, \ldots, \mathcal{S}_{k-1}\)</span>, we note that we have a memoryless property and so current iterate only depends on previous sample size (<span class="math inline">\(\mathbb{E}=\mathbb{E}_k:=\mathbb{E}[\cdot \mid \mathcal{S}_0, \ldots, \mathcal{S}_k]=E_{\mathcal{S}_k}\)</span>):</p>
<p><span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*) \leq \mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha}{2}\mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]+\frac{ML\alpha^2}{2}.\]</span> A consequence of the strong convexity property is that global minimizer <span class="math inline">\(\theta^*\)</span> is unique and <span class="math display">\[\mathcal{L}(\theta^k)-\mathcal{L}(\theta^*) \geq \frac{1}{2\mu} \|\nabla \mathcal{L}(\theta^k)\|^2\]</span> and thus <span class="math display">\[\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)-\mathcal{L}(\theta^*)] \geq \frac{1}{2\mu} \mathbb{E}_{k-1}[\| \nabla \mathcal{L}(\theta^k)\|^2].\]</span></p>
<p>We deduce <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*) \leq (1-\mu\alpha)(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*))+\frac{ML\alpha^2}{2}\]</span> or equivalently <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha ML}{2\mu} \leq (1-\mu\alpha)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha ML}{2\mu}\right).\]</span></p>
Note that <span class="math inline">\(\displaystyle \alpha=\frac{\eta}{L} \leq \frac{1}{L} \leq \frac{1}{\mu}\)</span>. Replacing <span class="math inline">\(\alpha\)</span> gives <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{M\eta}{2\mu} \leq \left(1-\frac{\eta\mu}{L}\right)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{M\eta}{2\mu}\right).\]</span> By induction, the claim holds.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>Thus <span class="math display">\[\lim_{k \to \infty} (\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)) \leq \frac{\alpha ML}{2\mu}=\frac{\eta M}{2 \mu}.\]</span> Convergence is obtained, in expectation, up to the level <span class="math inline">\(\displaystyle \frac{\eta M}{2\mu}\)</span> (noise level), which can be decreased in various ways. The ratio <span class="math inline">\(\displaystyle \frac{L}{\mu}\)</span> is a condition number of <span class="math inline">\(\mathcal{L}\)</span>.</p>
          </div>
<h2 id="decreasing-the-sgd-noise-level">2.4. Decreasing the SGD Noise Level</h2>
<p>Though not always desirable (due to the needs for small generalization error), the SGD noise level of <span class="math inline">\(\displaystyle \frac{\eta M}{2\mu}\)</span> can be removed so that <span class="math display">\[\lim_{k \to \infty} \mathbb{E}[\mathcal{L}(\theta^k)]=\mathcal{L}(\theta^*).\]</span></p>
<h3 id="dynamic-stepsize-reduction">2.4.1. Dynamic Stepsize Reduction</h3>
<p>We dynamically reduce <span class="math inline">\(\displaystyle \alpha^k=\frac{\eta_k}{L}\)</span>. Note that <span class="math inline">\(\eta_k \to 0\)</span> makes the residual <span class="math inline">\(\displaystyle \frac{\eta_kM}{2\mu} \to 0\)</span>, but it also means that <span class="math inline">\(\displaystyle \left(1-\frac{\eta_k}{L}\right) \to 1\)</span>, so the price is that we lose linear convergence.</p>
<p><strong>Theorem 2.4 (Dynamic Stepsize Stochastic Gradient Descent, DS-SGD)</strong>    Let <span class="math inline">\(\displaystyle \alpha^k=\frac{2}{2L+k\mu}\)</span> for all <span class="math inline">\(k \geq 0\)</span>. Then SGD satisfies <span class="math display">\[0 \leq \mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*) \leq \frac{v}{2L/\mu+k}\]</span> for all <span class="math inline">\(k \geq 0\)</span>, where <span class="math inline">\(\displaystyle v:=\frac{2L}{\mu}\max\left\{\frac{M}{\mu}, \mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)\right\}\)</span>. Thus <span class="math display">\[\lim_{k \to \infty} \mathbb{E}[\mathcal{L}(\theta^k)]=\mathcal{L}(\theta^*).\]</span></p>
<div class="note warning">
            <p>The rate is sublinear, <span class="math inline">\(\displaystyle \mathcal{O}\left(\frac{1}{k}\right)\)</span>.</p>
          </div>
<p><strong><em>Proof.</em></strong> Note that <span class="math inline">\(\displaystyle \alpha^k \leq \frac{1}{L} \leq \frac{1}{\mu}\)</span> and all arguments continue to hold in the proof of <a href="#thm2.3">theorem 2.3</a> until and including, and so for all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu} \leq (1-\mu\alpha^k)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu}\right).\]</span></p>
<p>We prove the conclusion by induction. Clearly at <span class="math inline">\(k=0\)</span>, the conclusion holds. Assume the conclusion holds at <span class="math inline">\(k&gt;0\)</span>, and substitute the conclusion into the above inequality, we obtain <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu} \leq (1-\mu\alpha^k)\left(\frac{v}{2L/\mu+k}-\frac{\alpha^kML}{2\mu}\right).\]</span></p>
Using the expression of <span class="math inline">\(\alpha^k\)</span> in the above and simplifying the expressions provides the conclusion with <span class="math inline">\(k\)</span> replaced by <span class="math inline">\((k+1)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h3 id="increasing-mini-batch-size">2.4.2. Increasing Mini-batch Size</h3>
<p>We increase mini-batch size from <span class="math inline">\(|\mathcal{S}_k|=1\)</span> to <span class="math inline">\(|\mathcal{S}_k|=p \geq 1\)</span>. Let <span class="math display">\[G^k=\frac{1}{p}\sum_{j \in \mathcal{S}_k} \nabla l_j(\theta^k)\]</span> where <span class="math inline">\(j \in \mathcal{S}_k \overset{\text{i.i.d.}}{\sim} \mathcal{U}(\{1, \ldots, m\})\)</span>. Then <span class="math display">\[\begin{aligned}
\text{Var}[G^k \mid \mathcal{S}_k]&amp;=\sum_{j \in \mathcal{S}_k} \frac{1}{p^2} \mathbb{E}_{\mathcal{S}_k} [\| \nabla l_j(\theta^k)-\nabla \mathcal{L}(\theta^k)\|^2]
\\&amp;\ \ \ \ +2\sum_{j&lt;i} \frac{1}{p^2} \mathbb{E}_{\mathcal{S}_k}[\nabla l_j(\theta^k)-\nabla \mathcal{L}(\theta^k)]^T \mathbb{E}_{\mathcal{S}_k}[\nabla l_i(\theta^k)-\nabla \mathcal{L}(\theta^k)]
\\&amp;=\frac{1}{p^2}\sum_{j \in \mathcal{S}_k} \text{Var}[\nabla l_j(\theta^k)]+0 \leq \frac{M}{p},
\end{aligned}\]</span> where <span class="math inline">\(|\mathcal{S}_k|=p\)</span> and the independence of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> indices in <span class="math inline">\(\mathcal{S}_k\)</span> in the first equality as well as the lack of bias <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[\nabla l_j(\theta^k)]=\nabla \mathcal{L}(\theta^k)\)</span>. We also have <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[G_k]=\nabla \mathcal{L}(\theta^k)\)</span>, i.e., unbiased batch gradient. Then as in <a href="#thm2.3">theorem 2.3</a>, we deduce, under the same assumptions, <span class="math display">\[\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu p} \leq \left(1-\frac{\eta\mu}{L}\right)^k \cdot \left( \mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu p}\right).\]</span> Thus the noise level is decreased by batch size <span class="math inline">\(p\)</span>, without impacting the convergence factor.</p>
<h3 id="momentum-for-gradient-variance-reduction">2.4.3. Momentum for Gradient Variance Reduction</h3>
<p>We use acceleration by momentum to reduce <span class="math inline">\(\text{Var}[G^k \mid \mathcal{S}_k]\)</span>, which yields <span class="math inline">\(\mathbb{E}[\mathcal{L}(\theta^k)] \to \mathcal{L}(\theta^*)\)</span> with linear convergence rate, with a much smaller cost per iteration than mini-batching.</p>
<h3 id="other-technique">2.4.4. Other Technique</h3>
<ul>
<li><p>Variance reduction (SVRG)</p></li>
<li><p>SAG</p></li>
<li><p>SAGA</p></li>
</ul>
<h3 id="conclusion">2.4.5. Conclusion</h3>
<p>Each of the three approaches (2.4.1 to 2.4.3) for accelerating SGD have merit and are often all used at once. In particular, once SGD appears to stagnate, one both reduces the stepsize and increases the batch-size; though this is stopped once validation error begins to increase.</p>
<h2 id="global-convergence-of-sgd-non-convex">2.5. Global Convergence of SGD: Non-convex</h2>
<p><strong>Theorem 2.5 (SGD with Fixed Stepsize)</strong>    Let <span class="math inline">\(\mathcal{L} \in \mathcal{C}^1(\mathbb{R}^n)\)</span> be bounded below by <span class="math inline">\(\mathcal{L}_\text{low}\)</span>, with <span class="math inline">\(\nabla \mathcal{L}\)</span> Lipschitz continuous with constant <span class="math inline">\(L\)</span> (<a href="#a1">assumption (1)</a>). Assume <a href="#a2">assumption (2)</a> holds. Apply the SGD method with fixed stepsize <span class="math inline">\(\displaystyle \alpha=\frac{\eta}{L}\)</span>, where <span class="math inline">\(\eta \in (0, 1]\)</span>, and <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, to minimize <span class="math inline">\(\mathcal{L}\)</span>. Then <span class="math display">\[\min_{0 \leq i \leq k} \mathbb{E}[\|\nabla \mathcal{L}(\theta^i)\|^2] \leq \alpha LM + \frac{2(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{k\alpha}=\eta M + \frac{2L(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{k\eta}\]</span> and therefore the SGD method takes at most <span class="math inline">\(\displaystyle k \leq \frac{2L(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{\eta\varepsilon}\)</span> iterations/evaluations to generate <span class="math inline">\(\mathbb{E}[\|\nabla \mathcal{L}(\theta^k)\|^2] \leq \varepsilon+\eta M\)</span>.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})] \leq \mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\frac{\alpha}{2}\mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]+\frac{ML\alpha^2}{2}.\]</span> We need to connect the per iteration decrease with the gradient. For all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathbb{E}_k[\mathcal{L}(\theta^{k+1})] \geq \frac{\alpha}{2} \mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]-\frac{ML\alpha^2}{2}.\]</span> Summing up the above bound from <span class="math inline">\(i=0\)</span> to <span class="math inline">\(k\)</span>, we deduce <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta^0)-\mathcal{L}_\text{low} &amp;\geq \mathcal{L}(\theta^0)-\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]
\\&amp;\geq \frac{\alpha}{2}\sum_{i=0}^k \mathbb{E}_{i-1}[\|\nabla \mathcal{L}(\theta^i)\|^2]-(k+1)\frac{ML\alpha^2}{2}
\\&amp;\geq \frac{\alpha}{2}(k+1)\left[\min_{0 \leq i \leq k} \mathbb{E}[\|\nabla \mathcal{L}(\theta^i)\|^2]-ML\alpha\right].
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The noise floor that limits the accuracy can be obtained. The acceleration momentum method is difficult in the nonconvex case.</p>
          </div>
<h1 id="sgd-improvement">3. SGD Improvement</h1>
<p>There are many improvements of SGD typically used in practice for deep learning.</p>
<h2 id="momentum">3.1. Momentum</h2>
<ul>
<li><p><strong>Polyak momentum</strong>: <span class="math display">\[\theta^{(k+1)}=\theta^{(k)} + \beta(\theta^{(k)} - \theta^{(k-1)}) - \alpha \nabla_\theta \mathcal{L}(\theta^{(k)}).\]</span></p></li>
<li><p><strong>Nesterov's accelerated gradient</strong>: <span class="math display">\[\begin{aligned}
\widehat{\theta}^k &amp;= \theta^{(k)}+\beta(\theta^{(k)}-\theta^{(k-1)}) \\
\theta^{(k+1)} &amp;= \widehat{\theta}^{(k)} - \alpha\nabla_\theta\mathcal{L}(\widehat{\theta}^{(k)})
\end{aligned}\]</span></p></li>
</ul>
<p>These acceleration methods give substantial improvements in the linear convergence rate for convex problems. Note that the linear convergence rates are: normal GD <span class="math inline">\(\displaystyle \frac{\kappa-1}{\kappa+1}\)</span>, Polyak <span class="math inline">\(\displaystyle \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\)</span>, and NAG <span class="math inline">\(\displaystyle \sqrt{\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}}}\)</span>.</p>
<h2 id="adaptive-sub-gradient">3.2. Adaptive Sub-Gradient</h2>
<p><strong>Preconditioning</strong> improves convergence rate of line-search methods.</p>
<p>Let <span class="math inline">\(\mathbf{g}^{(k)}(\theta^{(k)})=:\nabla_\theta \mathcal{L}(\theta^{(k)})\)</span> be the gradient of the training loss function at iteration <span class="math inline">\(k\)</span> and <span class="math display">\[B_k(i, i)=\left(\sum_{j=1}^k (\mathbf{g}^{(j)}(\theta^{(j)})(i))^2\right)^{1/2}\]</span> be the diagonal of the square root of the sum of prior gradient outer products. <strong>Adaptive sub-gradient (AdaGrad)</strong> is preconditioned GD via the diagonal matrix <span class="math inline">\(B\)</span>: <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \eta|\Lambda_k|^{-1}(B^{(k)}+\varepsilon I)^{-1}\nabla_\theta\sum_{\mu \in \Lambda_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu),\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate, <span class="math inline">\(\Lambda_k\)</span> is chosen in some randomized method, and <span class="math inline">\(\varepsilon I&gt;0\)</span> is added to avoid poor scaling of small values of <span class="math inline">\(B^{(k)}(i, i)\)</span>.</p>
<h3 id="adagrad-improvement-rmsprop-and-adadelta">3.2.1. AdaGrad Improvement: RMSProp and AdaDelta</h3>
<p>RMSProp gives more weight to the current gradient: <span class="math display">\[B_k^\text{RMS}(i, i)=\gamma B_{k-1}^\text{RMS}(i, i) + (1-\gamma)(\mathbf{g}^{(k)}(\theta^{(k)})(i))^2\]</span> for some <span class="math inline">\(\gamma \in [0, 1]\)</span> and updates as <span class="math display">\[\theta^{(k+1)}=\theta^{(k)} - \eta|\Lambda_k|^{-1}(B^{(k)}+\varepsilon I)^{-1/2}\nabla_\theta\sum_{\mu \in \Lambda_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu).\]</span></p>
<p>AdaDelta (Zeiler, 2012) extended AdaGrad using a similar preconditioned as <span class="math inline">\(B_k^\text{RMS}\)</span>, but also estimated the stepsize using an average difference in <span class="math inline">\(\theta^{(k)}-\theta^{(k-1)}\)</span>.</p>
<h3 id="scalar-adagrad">3.2.2. Scalar AdaGrad</h3>
<h4 id="iteration-complexity">3.2.2.1. Iteration Complexity</h4>
<p>Initialize with <span class="math inline">\(\theta^{(0)}\)</span> and <span class="math inline">\(b_0&gt;0\)</span>, <span class="math display">\[\begin{aligned}
b_k^2 &amp;= b_{k-1}^2+\|\nabla_\theta\mathcal{L}(\theta^{(k)})\|^2 \\
\theta^{(k)} &amp;= \theta^{(k-1)} - b_k^{-1}\nabla_\theta\mathcal{L}(\theta^{(k)}).
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(\mathcal{L}(\theta) \in \mathcal{C}_L^1\)</span>, that is <span class="math inline">\(L\)</span> minimal for which <span class="math inline">\(\|\nabla_\theta\mathcal{L}(\theta_1)-\nabla_\theta\mathcal{L}(\theta_2)\| \leq L\|\theta_1-\theta_2\|\)</span> for all <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then scalar batch AdaGrad satisfies <span class="math display">\[\min_{k=1, \ldots, T-1} \|\nabla_\theta\mathcal{L}(\theta^{(k)})\|^2 \leq \varepsilon\]</span> for either <span class="math display">\[T=1+\lceil 2\varepsilon^{-1}\mathcal{L}(\theta^{(0)})(b_0+2\mathcal{L}(\theta^{(0)})) \rceil\ \ \text{if}\ \ b_0 \geq L\]</span> or <span class="math display">\[T=1+\lceil \varepsilon^{-1}(L^2-b_0^2+4(\mathcal{L}(\theta^{(0)})+(3/4+\ln(L/b_0))L)^2) \rceil\ \ \text{if}\ \ b_0&lt;L.\]</span> In contrast, if <span class="math inline">\(b_k\)</span> is a fixed constant <span class="math inline">\(b\)</span>, then if <span class="math inline">\(b&lt;L/2\)</span>, GD can diverge, while if <span class="math inline">\(b \geq L\)</span>, then <span class="math inline">\(T=2b\varepsilon^{-1}\mathcal{L}(\theta^{(0)})\)</span>.</p>
<p>Iteration complexity for scalar batch AdaGrad following properties that for any non-negative values <span class="math inline">\(a_1, \ldots, a_T\)</span> with <span class="math inline">\(a_1&gt;0\)</span> <span class="math display">\[\sum_{\mathscr{l}=1}^T \frac{a_\mathscr{l}}{\sum_{i=1}^\mathscr{l} a_i} \leq \ln\left(\sum_{i=1}^T a_i\right)+1 \quad \text{and} \quad \sum_{\mathscr{l}=1}^T \frac{a_\mathscr{l}}{\sqrt{\sum_{i=1}^\mathscr{l}}a_i} \leq 2\sqrt{\sum_{i=1}^T a_i}.\]</span></p>
<p>In addition, for any fixed <span class="math inline">\(\varepsilon \in (0, 1]\)</span> and <span class="math inline">\(L, b_0&gt;0\)</span>, the iterates <span class="math inline">\(b_{k+1}^2=b_k^2+a_k\)</span> has the property that after <span class="math inline">\(N=\lceil \varepsilon^{-1}(L^2-b_0^2) \rceil+1\)</span> iterations, either <span class="math inline">\(\displaystyle \min_{0 \leq k \leq N-1} a_k \leq \varepsilon\)</span> or <span class="math inline">\(b_N \geq L\)</span>.</p>
<p>Let <span class="math inline">\(k_0\)</span> be the first iterate s.t. <span class="math inline">\(b_{k_0} \geq L\)</span>, then for all <span class="math inline">\(k \geq k_0\)</span>, <span class="math display">\[b_k \leq b_{k_0-1}+2\mathcal{L}(\theta^{(k_0-1)}) \quad \text{(bounded above)}\]</span> and <span class="math display">\[\mathcal{L}(\theta^{(k_0-1)}) \leq \frac{L}{2}(1+2\ln(b_{k_0-1} / b_0)) \quad \text{(not diverged)}.\]</span></p>
<h4 id="influence-of-mini-batch-or-other-gradient-approximation">3.2.2.2. Influence of Mini-batch or Other Gradient Approximation</h4>
<p>Let <span class="math inline">\(\mathbf{g}^{(k)}\)</span> be an unbiased estimator of the gradient <span class="math inline">\(\nabla_\theta \mathcal{L}(\theta^{(k)})\)</span> of the training loss function at iteration <span class="math inline">\(k\)</span>, i.e., <span class="math inline">\(\mathbb{E}[\mathbf{g}^{(k)}]=\nabla_\theta\mathcal{L}(\theta^{(k)})\)</span>. Moreover, suppose there is a uniform bound <span class="math inline">\(\mathbb{E}[\|\mathbf{g}^{(k)}\|^2] \leq c_\mathbf{g}^2\)</span>. Then consider the stochastic scalar AdaGrad update as <span class="math display">\[\begin{aligned}
b_k^2 &amp;= b_{k-1}^2+\|\mathbf{g}^{(k)}\|^2 \\
\theta^{(k)} &amp;= \theta^{(k-1)}-b_k^{-1}\mathbf{g}^{(k)}.
\end{aligned}\]</span></p>
<p>Unlike in the batch version of AdaGrad where <span class="math inline">\(b_k\)</span> converges to a fixed stepsize, stochastic AdaGrad converges roughly at the rate <span class="math inline">\(b_k \approx c_\mathbf{g}k^{1/2}\)</span>. Moreover, Ward et al. (2018) showed that <span class="math display">\[\min_{\mathscr{l}=0, \ldots, N-1}(\mathbb{E} \|\nabla_\theta\mathcal{L}(\theta^{(\mathscr{l})})\|^{4/3})^{3/2} \leq \mathcal{O}\left(\frac{b_0+c_\mathbf{g}}{N}+\frac{c_\mathbf{g}}{N^{1/2}}\right)\ln(Nc_\mathbf{g}^2/b_0^2).\]</span></p>
]]></content>
      <categories>
        <category>Computer science</category>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>DNN</tag>
        <tag>Optimization algorithm</tag>
        <tag>SGD</tag>
      </tags>
  </entry>
  <entry>
    <title>Point Set</title>
    <url>/Mathematics/Real-analysis/Point_Set.html</url>
    <content><![CDATA[<h1 id="metric-space">1. Metric Space</h1>
<p><strong>Definition 1.1</strong>    Suppose <span class="math inline">\(X\)</span> is a set, and <span class="math inline">\(d: X \times X \to \mathbb{R}\)</span> satisfies</p>
<p>    (1) <span class="math inline">\(\forall x, y \in X, d(x, y) \geq 0\)</span>;</p>
<p>    (2) <span class="math inline">\(\forall x, y \in X, d(x, y)=d(y, x)\)</span>;</p>
<p>    (3) <span class="math inline">\(\forall x, y, z \in X, d(x, y) \leq d(x, z)+ d(z, y)\)</span>.</p>
<p>We define <span class="math inline">\((X, d)\)</span> as a <strong>metric space</strong>.</p>
<p><strong>Definition 1.2</strong>    Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are non-empty point sets. The <strong>distance</strong> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> is <span class="math display">\[d(A, B)=\inf_{p \in A, q \in B} d(p, q).\]</span></p>
<p><strong>Definition 1.3</strong>    Suppose <span class="math inline">\(E\)</span> is a non-empty point set. The <strong>diameter</strong> of <span class="math inline">\(E\)</span> is <span class="math display">\[\delta(E)=\sup_{p \in E, q \in E} d(p, q).\]</span></p>
<p><strong>Definition 1.4</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span>. If <span class="math inline">\(\delta(E)&lt;\infty\)</span>, then <span class="math inline">\(E\)</span> is a <strong>bounded point set</strong>.</p>
<p><strong>Definition 1.5</strong>    In a metric space <span class="math inline">\((X, d)\)</span>, a set is a <strong>neighborhood</strong> of a point <span class="math inline">\(p\)</span> if there exists an open ball with centre <span class="math inline">\(p\)</span> and radius <span class="math inline">\(r&gt;0\)</span>, such that <span class="math display">\[B_r(p)=\{x \in X: d(x, p)&lt;r\}.\]</span></p>
<p><strong>Property 1.1</strong>    The neighborhood has the following basic properties:</p>
<p>    (1) <span class="math inline">\(p \in B_r(p)\)</span>;</p>
<p>    (2) <span class="math inline">\(\forall r_1, r_2&gt;0, \exists r_3&gt;0\)</span> s.t. <span class="math inline">\(B_{r_3}(p) \subset B_{r_1}(p) \cap B_{r_2}(p)\)</span>;</p>
<p>    (3) <span class="math inline">\(\forall q \in B_{r_1}(p), \exists B_{r&#39;}(q) \subset B_r(p)\)</span>;</p>
<p>    (4) If <span class="math inline">\(p \neq q, \exists B_{r_1}(p), B_{r_2}(q)\)</span> s.t. <span class="math inline">\(B_{r_1}(p) \cap B_{r_2}(q)=\varnothing\)</span>.</p>
<h1 id="cluster-point-interior-point-and-boundary-point">2. Cluster Point, Interior Point and Boundary Point</h1>
<p><strong>Definition 2.1</strong>    A point <span class="math inline">\(p\)</span> is an <strong>interior point</strong> of <span class="math inline">\(E\)</span> if and only if <span class="math display">\[\exists B_\delta(p)\ \text{s.t.}\ B_\delta(p) \subset E.\]</span></p>
<p><strong>Definition 2.2</strong>    A point <span class="math inline">\(p\)</span> is an <strong>exterior point</strong> of <span class="math inline">\(E\)</span> if and only if <span class="math display">\[\exists B_\delta(p)\ \text{s.t.}\ B_\delta(p) \subset E^c.\]</span></p>
<p><strong>Definition 2.3</strong>    A point <span class="math inline">\(p\)</span> is a <strong>boundary point</strong> of <span class="math inline">\(E\)</span> if and only if <span class="math display">\[\forall B_\delta(p): B_\delta(p) \cap E \neq \varnothing, B_\delta(p) \cap E^c \neq \varnothing.\]</span></p>
<p><strong>Definition 2.4</strong>    The set of all interior points of <span class="math inline">\(E\)</span>, denoted <span class="math inline">\(E^\circ\)</span>, is <strong>interior</strong> or <strong>open kernel</strong>. The set of all boundary points of <span class="math inline">\(E\)</span>, denoted <span class="math inline">\(\partial E\)</span>, is <strong>boundary</strong>.</p>
<p><strong>Definition 2.5</strong>    A point <span class="math inline">\(p\)</span> is a <strong>cluster point</strong> of <span class="math inline">\(E\)</span> if and only if <span class="math display">\[\forall B_r(p), \exists q \neq p, q \in B_r(p)\ \text{s.t.}\ q \in E.\]</span></p>
<p><strong>Definition 2.6</strong>    A point <span class="math inline">\(p\)</span> is a <strong>isolated point</strong> of <span class="math inline">\(E\)</span> if and only if <span class="math display">\[p \in E, \exists B_r(p)\ \text{s.t.}\ \forall q \neq p \in B_r(p), q \notin E.\]</span></p>
<div class="note info">
            <p>The boundary point is either a cluster point or an isolated point.</p>
          </div>
<p><strong>Theorem 2.1</strong>    The following statements are equivalent:</p>
<p>    (1) <span class="math inline">\(p\)</span> is a cluster point of <span class="math inline">\(E\)</span>;</p>
<p>    (2) Any neighborhood of <span class="math inline">\(p\)</span> includes infinite points that belong to <span class="math inline">\(E\)</span>;</p>
<p>    (3) There exists a sequence <span class="math inline">\(\{p_n\}\)</span> including mutually different points in <span class="math inline">\(E\)</span> such that <span class="math inline">\(\displaystyle \lim_{n \to \infty} p_n=p\)</span>.</p>
<p><strong><em>Proof.</em></strong> We need to show (1) <span class="math inline">\(\Rightarrow\)</span> (2) <span class="math inline">\(\Rightarrow\)</span> (3) <span class="math inline">\(\Rightarrow\)</span> (1).</p>
<p>(i) Suppose <span class="math inline">\(\exists B_\delta(p)\)</span> s.t. <span class="math inline">\(B_\delta(p) \cap E=\{q_1, \ldots, q_k\}\)</span>. We can assume that <span class="math inline">\(q_1, \ldots, q_k \neq p\)</span>. Let <span class="math display">\[\delta_0=\frac{1}{2}\min\{\delta, d(p, q_1), \ldots, d(p, q_k)\},\]</span> then <span class="math display">\[\forall p^* \in B_{\delta_0}(p)-\{p\}: p^* \notin \{p, q_1, \ldots, q_k\},\]</span> i.e., <span class="math inline">\(\exists B_{\delta_0}(p)\)</span> s.t. <span class="math inline">\(\forall p^* \neq p \in B_{\delta_0}(p): p^* \notin E\)</span>, which is a contradiction. Hence, (1) <span class="math inline">\(\Rightarrow\)</span> (2).</p>
<p>(ii) Suppose <span class="math inline">\(p_1 \in B_1(p) \cap E\)</span>, where <span class="math inline">\(p_1 \neq p\)</span>. Let <span class="math display">\[\delta_1=\frac{1}{2}\min\left\{d(p_1, p), \frac{1}{2}\right\},\]</span> then we can find a <span class="math inline">\(p_2 \in B_{\delta_1} \cap E\)</span>, where <span class="math inline">\(p_2 \neq p\)</span>. We can keep constructing such neighborhood: <span class="math display">\[\delta_k=\frac{1}{2}\min\left\{d(p_k, p), \frac{1}{k}\right\}\]</span> and <span class="math inline">\(p_k \in B_{\delta_k} \cap E\)</span>, where <span class="math inline">\(p_k \neq p\)</span>. Therefore we can find a sequence <span class="math inline">\(\{p_n\}\)</span> including mutually points in <span class="math inline">\(E\)</span>. Besides, <span class="math display">\[\forall \varepsilon&gt;0, \exists N \in \mathbb{N}\ \text{s.t.}\ N&gt;\frac{1}{\varepsilon},\]</span> and thus <span class="math display">\[\forall n&gt;N, d(p_n, p)&lt;\frac{1}{n}&lt;\frac{1}{N}&lt;\varepsilon,\]</span> i.e., <span class="math inline">\(\displaystyle \lim_{n \to \infty} p_n=p\)</span>. Hence, (2) <span class="math inline">\(\Rightarrow\)</span> (3).</p>
(iii) Take an arbitrary <span class="math inline">\(B_\delta(p)\)</span>. Since <span class="math inline">\(\displaystyle \lim_{n \to \infty} p_n=p\)</span>, then <span class="math display">\[\exists N \in \mathbb{N}\ \text{s.t.}\ n&gt;N, d(p_n, p)&lt;\delta.\]</span> Since <span class="math inline">\(\{p_n\}\)</span> includes mutually different points, then <span class="math display">\[\exists m&gt;N\ \text{s.t}\ p_m \neq p,\]</span> where <span class="math inline">\(p_m \in B_\delta(p_n) \cap E\)</span>. Hence, <span class="math inline">\(p\)</span> is a cluster point of <span class="math inline">\(E\)</span>, i.e., (3) <span class="math inline">\(\Rightarrow\)</span> (1).
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 2.7</strong>    The set of all cluster points of <span class="math inline">\(E\)</span>, denoted <span class="math inline">\(E&#39;\)</span>, is <strong>derived set</strong>. The <strong>closure</strong> of <span class="math inline">\(E\)</span>, denoted <span class="math inline">\(\overline{E}\)</span>, is <span class="math inline">\(E \cup E&#39;\)</span>.</p>
<div class="note info">
            <p><span class="math inline">\(\overline{E}=E \cup \partial E=E^\circ \cup \partial E=E&#39; \cup \{\text{All isolated points of}\ E\}\)</span>.</p>
          </div>
<p><strong>Theorem 2.2</strong>    The closure and open kernel have the following relationships:</p>
<p>    (1) <span class="math inline">\((E^\circ)^c=\overline{E^c}\)</span>;</p>
<p>    (2) <span class="math inline">\((\overline{E})^c=(E^c)^\circ\)</span>;</p>
<p>    (3) <span class="math inline">\((A \cup B)&#39;=A&#39; \cup B&#39;\)</span>.</p>
<p><strong>Theorem 2.3</strong>    If <span class="math inline">\(E \subset \mathbb{R}^n \neq \varnothing\)</span> and <span class="math inline">\(E \neq \mathbb{R}^n\)</span>, then <span class="math inline">\(\partial E \neq \varnothing\)</span>.</p>
<strong><em>Hint of proof.</em></strong> We can use the connectivity in <span class="math inline">\(\mathbb{R}^n\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 2.4 (Bolzano-Weierstrass Theorem)</strong>    Suppose <span class="math inline">\(E \subset \mathbb{R}^n\)</span> is a bounded and infinite set, then <span class="math inline">\(E\)</span> includes at least one cluster point.</p>
<h1 id="open-set-and-closed-set">3. Open Set and Closed Set</h1>
<p><strong>Definition 3.1</strong>    If <span class="math inline">\(\forall x \in E\)</span>, <span class="math inline">\(x\)</span> are interior point, then <span class="math inline">\(E\)</span> is an <strong>open set</strong>. If <span class="math inline">\(\forall x \in E&#39;: x \in E\)</span>, then <span class="math inline">\(E\)</span> is a <strong>closed set</strong>.</p>
<p><strong>Theorem 3.1</strong>    <span class="math inline">\(E\)</span> is an open set if and only if <span class="math inline">\(E=E^\circ\)</span>.</p>
<p><strong>Theorem 3.2</strong>    <span class="math inline">\(E\)</span> is a closed set if and only if <span class="math inline">\(E=\overline{E}\)</span>.</p>
<div class="note info">
            <p><strong>Claim.</strong> <span class="math inline">\(E^\circ\)</span> is an open set, and <span class="math inline">\(E^\circ\)</span> is the "largest" (<span class="math inline">\(\cup\)</span>) open set that lies in <span class="math inline">\(E\)</span>. <span class="math inline">\(\overline{E}\)</span> is a closed set, and <span class="math inline">\(\overline{E}\)</span> is the "smallest" (<span class="math inline">\(\cap\)</span>) closed set that includes <span class="math inline">\(E\)</span>.</p><p><strong><em>Proof.</em></strong> Let <span class="math inline">\(E_o\)</span> be the set of all open sets of <span class="math inline">\(E\)</span>.<br></p><p>Suppose <span class="math inline">\(x \in E^\circ\)</span>. Since <span class="math inline">\(E^\circ \subset E\)</span> and <span class="math inline">\(E^\circ\)</span> is an open set, then <span class="math inline">\(\displaystyle x \in \bigcup_{G \subset E_o} G\)</span>. Suppose <span class="math inline">\(\displaystyle x \in \bigcup_{G \subset E_o} G\)</span>, then <span class="math inline">\(\exists G \subset E\)</span> s.t. <span class="math inline">\(x \in G\)</span>, where <span class="math inline">\(G\)</span> is an open set of <span class="math inline">\(E\)</span>.</p><p>Hence, <span class="math inline">\(\exists \delta&gt;0\)</span> s.t. <span class="math inline">\(B_\delta(x) \subset G \subset E\)</span>, i.e., <span class="math inline">\(x \in E^\circ\)</span>, and thus <span class="math inline">\(\displaystyle E^\circ=x \in \bigcup_{G \subset E_o} G\)</span>, i.e., <span class="math inline">\(E^\circ\)</span> is the "largest" open set that lies in <span class="math inline">\(E\)</span>.</p>Similarly, we can show that <span class="math inline">\(\overline{E}\)</span> is the "smallest" closed set that includes <span class="math inline">\(E\)</span>.<p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong>Theorem 3.3</strong>    If <span class="math inline">\(E\)</span> is an open set, then <span class="math inline">\(E^c\)</span> is a closed set. If <span class="math inline">\(E\)</span> is a closed set, then <span class="math inline">\(E^c\)</span> is an open set.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(E\)</span> is an open set. Take an arbitrary cluster point <span class="math inline">\(x\)</span> of <span class="math inline">\(E^c\)</span>, then <span class="math inline">\(\forall B_r(x), \exists x&#39; \neq x, x&#39; \in B_r(x)\)</span> s.t. <span class="math inline">\(x&#39; \in E^c\)</span>, then <span class="math inline">\(x\)</span> is not an interior point of <span class="math inline">\(E\)</span>, i.e., <span class="math inline">\(x \in E^c\)</span>. Therefore, <span class="math inline">\(E^c\)</span> is a closed set.</p>
<p>Suppose <span class="math inline">\(E\)</span> is a closed set. Take an arbitrary <span class="math inline">\(x \in E^c\)</span>. Suppose <span class="math inline">\(x\)</span> is not an interior point of <span class="math inline">\(E^c\)</span>, then <span class="math inline">\(\forall B_r(x): B_r(x) \cap E \neq \varnothing\)</span>. Since <span class="math inline">\(x \notin E\)</span>, then <span class="math inline">\(\forall B_r(x), \exists x&#39; \neq x\)</span> s.t. <span class="math inline">\(x&#39; \in E\)</span>. Therefore, <span class="math inline">\(x\)</span> is a cluster point of <span class="math inline">\(E\)</span> and. thus <span class="math inline">\(x \in E\)</span>, which is a contradiction. Hence, <span class="math inline">\(x\)</span> is an interior point of <span class="math inline">\(E^c\)</span>, and <span class="math inline">\(E^c\)</span> is an open set.</p>
<p><strong>Theorem 3.4</strong>    The union of any number of open sets is an open set. The intersection of a finite number of open sets is an open set. The intersection of any number of closed sets is a closed set. The union of a finite number of closed sets is a closed set.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(G_\lambda\)</span> are open sets, where <span class="math inline">\(\lambda \in \Lambda\)</span>, and <span class="math inline">\(\Lambda\)</span> is an arbitrary indicator set. We have <span class="math display">\[\forall x \in \bigcup_{\lambda \in \Lambda} G_\lambda, \exists \lambda \in \Lambda\ \text{s.t.}\ x \in G_\lambda,\]</span> and thus <span class="math display">\[\exists \delta&gt;0\ \text{s.t.}\ B_\delta(x) \subset G_\lambda \subset \bigcup_{\lambda \in \Lambda} G_\lambda.\]</span> Therefore, <span class="math inline">\(x\)</span> is an interior point of <span class="math inline">\(\displaystyle \bigcup_{\lambda \in \Lambda} G_\lambda\)</span>, and thus <span class="math inline">\(\displaystyle \bigcup_{\lambda \in \Lambda} G_\lambda\)</span> is an open set.</p>
<p>Suppose <span class="math inline">\(G_i\)</span> are open sets, where <span class="math inline">\(i=1, \ldots, n.\)</span> If <span class="math inline">\(\displaystyle \bigcap_{i=1}^n G_i=\varnothing\)</span>, then <span class="math inline">\(\displaystyle \bigcap_{i=1}^n G_i\)</span> is an open set. If <span class="math inline">\(\displaystyle \bigcap_{i=1}^n G_i \neq \varnothing\)</span>, then take an arbitrary <span class="math inline">\(\displaystyle x \in \bigcap_{i=1}^n G_i\)</span>, we have <span class="math inline">\(x \in G_i\)</span>. Hence, <span class="math inline">\(\exists \delta_i&gt;0\)</span> s.t. <span class="math inline">\(B_{\delta_i}(x) \subset G_i\)</span>. Take <span class="math inline">\(\delta=\min\{\delta_1, \ldots, \delta_n\}\)</span>, which is greater than <span class="math inline">\(0\)</span>, then <span class="math inline">\(B_\delta(x) \subset G_i\)</span>, i.e., <span class="math inline">\(\displaystyle B_\delta(x) \subset \bigcap_{i=1}^n G_i\)</span>. Therefore, <span class="math inline">\(x\)</span> is an interior point of <span class="math inline">\(\displaystyle \bigcap_{i=1}^n G_i\)</span>, and thus <span class="math inline">\(\displaystyle \bigcap_{i=1}^n G_i\)</span> is an open set.</p>
As a consequence, <span class="math display">\[\left(\bigcup_{\lambda \in \Lambda}G_\lambda\right)^c=\bigcap_{\lambda \in \Lambda}G_\lambda^c\]</span> is a closed set, where <span class="math inline">\(G_\lambda^c\)</span> is a closed set; <span class="math display">\[\left(\bigcap_{i=1}^n G_i\right)^c=\bigcup_{i=1}^n G_i^c\]</span> is a closed set, where <span class="math inline">\(G_i^c\)</span> is a closed set.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p>The intersection of any number of open sets <strong>is not necessarily</strong> an open set. For example, <span class="math display">\[G_n=\left(-1-\frac{1}{n}, 1+\frac{1}{n}\right),\]</span> where <span class="math inline">\(G_n\)</span> are open sets. However, <span class="math display">\[\bigcap_{n=1}^\infty G_n=[-1, 1],\]</span> which is a closed set.</p><p>The union of any number of closed sets <strong>is not necessarily</strong> a closed set. For example, <span class="math display">\[F_n=\left[-1+\frac{1}{n}, 1-\frac{1}{n}\right],\]</span> where <span class="math inline">\(F_n\)</span> are closed sets. However, <span class="math display">\[\bigcup_{n=1}^\infty F_n=(-1, 1),\]</span> which is an open set.</p>
          </div>
<p><strong>Example 3.1</strong>    Suppose <span class="math inline">\(F_1, F_2 \subset \mathbb{R}^n\)</span> are closed sets, and <span class="math inline">\(F_1 \cap F_2=\varnothing\)</span>.</p>
<h1 id="compact-set-and-complete-set">4. Compact Set and Complete Set</h1>
<p><strong>Definition 4.1</strong>    <span class="math inline">\(M\)</span> is a <strong>compact set</strong> if every open cover of <span class="math inline">\(M\)</span> has a finite sub-cover.</p>
<p><strong>Theorem 4.1</strong>    In <span class="math inline">\(\mathbb{R}^n\)</span>, if set <span class="math inline">\(F\)</span> is bounded and closed, then <span class="math inline">\(F\)</span> is compact.</p>
<div class="note ">
            <p><strong>Lemma 4.1.1</strong>    The closed subset of a compact set is compact.</p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(M\)</span> is compact, and <span class="math inline">\(F \subset M\)</span>, where <span class="math inline">\(F\)</span> is closed. Take any open cover of <span class="math inline">\(F\)</span>, denoted <span class="math inline">\(\{U_\lambda\}_{\lambda \in \Lambda}\)</span>. We know <span class="math inline">\(\{U_\lambda\}_{\lambda \in \Lambda} \cup F^c\)</span> is an open cover of <span class="math inline">\(M\)</span>, then there exists a finite sub-cover: <span class="math inline">\(U_1, \ldots, U_n, F^c\)</span>. Hence, <span class="math inline">\(\{U_i\}_{i=1}^n\)</span> is the finite sub-cover of <span class="math inline">\(F\)</span>, and thus <span class="math inline">\(F\)</span> is compact.<p align="right"><span class="math inline">\(\square\)</span></p>
          </div>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(F\)</span> is bounded and closed. Since <span class="math inline">\(F \subset \mathbb{R}^n\)</span> is bounded, then <span class="math inline">\(\exists T_0=[-a, a]^n\)</span> s.t. <span class="math inline">\(F \subset T_0\)</span>. Suppose <span class="math inline">\(T_0\)</span> is not compact, then there exists an open cover of <span class="math inline">\(T_0\)</span> such that it does not have finite sub-cover. We divide <span class="math inline">\(T_0\)</span> equally into a closed sets with edge length <span class="math inline">\(a\)</span>, and at least one set does not have finite sub-cover, denoted as <span class="math inline">\(T_1\)</span>. We divide <span class="math inline">\(T_1\)</span> equally into a closed sets with edge length <span class="math inline">\(\displaystyle \frac{a}{2}\)</span>, and at least one set does not have finite sub-cover, denoted as <span class="math inline">\(T_2\)</span>. We repeat this process, and get <span class="math display">\[T_1 \supset T_2 \supset \cdots \supset T_n \supset \cdots,\]</span> where <span class="math inline">\(T_n\)</span> has edge length <span class="math inline">\(\displaystyle \frac{a}{2^{n-1}}\)</span>.</p>
<p>Take an arbitrary <span class="math inline">\(x_n \in T_{n-1}-T_n\)</span>, then <span class="math inline">\(\{x_n\}\)</span> is bounded and infinite. Because of Bolzano-Weierstrass theorem, <span class="math inline">\(\{x_n\}\)</span> includes at least one cluster point, denoted as <span class="math inline">\(x\)</span>, i.e., <span class="math inline">\(\exists \{x_{k_s}\}\)</span> s.t. <span class="math inline">\(x_{k_s} \to x \in T_j\)</span> for some <span class="math inline">\(j\)</span>. Hence, there exists an open set <span class="math inline">\(G\)</span> in the sub-cover such that <span class="math inline">\(x \in G\)</span>, i.e., <span class="math inline">\(\exists \delta&gt;0\)</span> s.t. <span class="math inline">\(B_\delta(x) \subset G\)</span>.</p>
Take <span class="math inline">\(s \to \infty\)</span> such that the edge length of <span class="math inline">\(T_{k_s}\)</span> is less than <span class="math inline">\(\displaystyle \frac{\delta}{2}\)</span> and <span class="math inline">\(\displaystyle d(x_{k_s}, x)&lt;\frac{\delta}{2}\)</span>, and thus <span class="math inline">\(T_{k_s} \subset B_\delta(x) \subset G\)</span>, which is a contradiction. As a consequence, <span class="math inline">\(T_0\)</span> is compact and <span class="math inline">\(F \subset T_0\)</span> is compact.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Theorem 4.2</strong>    In <span class="math inline">\(\mathbb{R}^n\)</span>, if set <span class="math inline">\(M\)</span> is compact, then <span class="math inline">\(M\)</span> is bounded and closed.</p>
<p><strong><em>Proof.</em></strong> Since <span class="math inline">\(M\)</span> is compact, then we can find an open cover such that <span class="math inline">\(\displaystyle M \subset \bigcup_{p \in M} B_1(p)\)</span>, and thus there exists a finite sub-cover: <span class="math inline">\(B_1(p_1), \ldots, B_1(p_n)\)</span>. Since <span class="math inline">\(B_1(p_i)\)</span> is bounded, then <span class="math inline">\(\displaystyle \bigcup_{i=1}^n B_1(p_i)\)</span> is bounded, and thus <span class="math inline">\(\displaystyle M \subset \bigcup_{i=1}^n B_1(p_i)\)</span> is bounded.</p>
<p>Take arbitrary <span class="math inline">\(q \in M^c\)</span>, then <span class="math inline">\(\forall p \in M, \exists \delta_p&gt;0\)</span> s.t. <span class="math inline">\(B_{\delta_p}(p) \cap B_{\delta_p}(q)=\varnothing\)</span>. We know <span class="math inline">\(\displaystyle M \subset \bigcup_{p \in M} B_{\delta_p}(p)\)</span>, and thus there exists a finite sub-cover: <span class="math inline">\(B_{\delta_{p_1}}, \ldots, B_{\delta_{p_n}}\)</span>.</p>
Take <span class="math inline">\(\delta=\min\{\delta_{p_1}, \ldots, \delta_{p_n}\}\)</span>, then <span class="math inline">\(B_\delta(q) \cap B_{\delta_{p_i}}(p_i)=\varnothing\)</span> and thus <span class="math inline">\(B_\delta(q) \cap M=\varnothing\)</span>, i.e., <span class="math inline">\(B_\delta(q) \subset M^c\)</span>. Therefore, <span class="math inline">\(M^c\)</span> is open, and <span class="math inline">\(M\)</span> is closed.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p><font color="#696969" font size="2">在<span class="math inline">\(\mathbb{R}^n\)</span>中，紧集和有界闭集是等价的. 但在一般空间中，不一定有<span class="math inline">\(n\)</span>维区间，而点<span class="math inline">\(p \neq q\)</span>不一定能被开集分开，因此上述两个定理无法推广到一般度量空间或一般拓扑空间中. 在一般度量空间中，我们可以找到反例说明有界闭集不是紧集. 在一般拓扑空间中，我们可以找到反例说明紧集不是闭集.</font></p>
          </div>
<p><strong>Definition 4.2</strong>    If <span class="math inline">\(E \subset E&#39;\)</span> (i.e., every point of <span class="math inline">\(E\)</span> is a cluster point, or <span class="math inline">\(E\)</span> does not have any isolated points), then <span class="math inline">\(E\)</span> is <strong>dense-in-itself</strong> or <strong>crowded</strong>. If <span class="math inline">\(E=E&#39;\)</span> (i.e., <span class="math inline">\(E\)</span> is a closed set without any isolated points), then <span class="math inline">\(E\)</span> is <strong>complete</strong>.</p>
<p><strong>Example 4.1</strong>    <span class="math inline">\(\varnothing\)</span> is complete. <span class="math inline">\(\mathbb{Q}\)</span> is dense-in-itself. <span class="math inline">\([a, b]\)</span> is complete.</p>
<h1 id="construction-of-open-set-closed-set-and-complete-set-on-mathbbr">5. Construction of Open Set, Closed Set, and Complete Set on \(\mathbb{R}\)</h1>
<p><strong>Theorem 5.1</strong>    The set of all disjoint intervals on <span class="math inline">\(\mathbb{R}\)</span> is at most countable.</p>
<strong><em>Proof.</em></strong> Take an arbitrary interval <span class="math inline">\(I\)</span>. Take a rational number <span class="math inline">\(q_I \in I\)</span>, then we can find an injection: <span class="math inline">\(I \mapsto q_I\)</span>, i.e., the set of all disjoint intervals on <span class="math inline">\(\mathbb{R}\)</span> is at most countable.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 5.1</strong>    Suppose <span class="math inline">\(G \subset \mathbb{R}\)</span> is open. If <span class="math inline">\((\alpha, \beta) \subset G\)</span>, and <span class="math inline">\(\alpha, \beta \notin G\)</span>, then <span class="math inline">\((\alpha, \beta)\)</span> is the <strong>component interval</strong> of <span class="math inline">\(G\)</span>.</p>
<p><strong>Theorem 5.2</strong>    Suppose <span class="math inline">\(G \subset \mathbb{R}\)</span> is non-empty and open, then <span class="math inline">\(G\)</span> can be constructed by the at most countable union of disjoint component intervals.</p>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\((\alpha_1, \beta_1)\)</span> and <span class="math inline">\((\alpha_2, \beta_2)\)</span> are different component intervals of <span class="math inline">\(G\)</span> and <span class="math inline">\((\alpha_1, \beta_1) \cap (\alpha_2, \beta_2) \neq \varnothing\)</span>. Assume, without loss of generality, that <span class="math inline">\(\alpha_1 \in (\alpha_2, \beta_2) \subset G\)</span>, then <span class="math inline">\(\alpha_1 \in G\)</span>, which is a contradiction. Therefore, different component intervals of <span class="math inline">\(G\)</span> are disjoint. By Theorem 5.1, we can let <span class="math display">\[\widetilde{G}=\bigcup_{\substack{(\alpha, \beta) \subset G \\ \alpha \notin G, \beta \notin G}} (\alpha, \beta)\]</span> be the at most countable union of disjoint intervals.</p>
<p>It is obvious that <span class="math inline">\(\widetilde{G} \subset G\)</span>. Take an arbitrary <span class="math inline">\(x_0 \in G\)</span>, and let <span class="math display">\[A_{x_0}=\{(x_0-\delta, x_0+\delta): (x_0-\delta, x_0+\delta) \subset G\} \neq \varnothing.\]</span> Let <span class="math display">\[\alpha=\inf_{(x_0-\delta, x_0+\delta) \in A_{x_0}} x_0-\delta, \beta=\sup_{(x_0-\delta, x_0+\delta) \in A_{x_0}} x_0+\delta.\]</span> It is obvious that <span class="math inline">\(x_0 \in (\alpha, \beta)\)</span>. Take an arbitrary <span class="math inline">\(y \in (\alpha, \beta)\)</span>. When <span class="math inline">\(\alpha&lt;y \leq x_0\)</span>, since <span class="math inline">\(\alpha\)</span> is an infimum, then <span class="math inline">\(\exists \delta\)</span> s.t. <span class="math inline">\(\alpha \leq x_0-\delta&lt;y\)</span> and <span class="math inline">\((x_0-\delta, x_0+\delta) \in A_{x_0}\)</span>, then <span class="math inline">\(y \in (x_0-\delta, x_0+\delta) \subset G\)</span>. Similarly, when <span class="math inline">\(x_0 \leq y&lt;\beta\)</span>, we can show that <span class="math inline">\(y \in G\)</span>. Hence, <span class="math inline">\((\alpha, \beta) \subset G\)</span>.</p>
Suppose <span class="math inline">\(\alpha \in G\)</span>, then <span class="math inline">\(\exists \delta_{\alpha}&gt;0\)</span> s.t. <span class="math inline">\((\alpha-\delta_\alpha, \alpha+\delta_\alpha) \subset G\)</span>, then <span class="math inline">\((\alpha-\delta_\alpha, \beta) \subset G\)</span> and <span class="math inline">\(\alpha-\delta_\alpha&lt;\alpha\)</span>, which is a contradiction, i.e., <span class="math inline">\(\alpha \notin G\)</span>. Similarly, we can prove that <span class="math inline">\(\beta \notin G\)</span>. Therefore <span class="math inline">\((\alpha, \beta)\)</span> is the component interval of <span class="math inline">\(G\)</span>, and thus <span class="math inline">\(G=\widetilde{G}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><strong>Definition 5.2</strong>    Suppose <span class="math inline">\(A \subset \mathbb{R}\)</span> is closed. The component interval of <span class="math inline">\(A^c\)</span> is the <strong>complementary interval</strong> of <span class="math inline">\(A\)</span>.</p>
<p><strong>Theorem 5.3</strong>    A closed set <span class="math inline">\(F\)</span> on <span class="math inline">\(\mathbb{R}\)</span> is either a full straight line, or deleting at most countable disjoint open intervals (i.e., the complementary interval of <span class="math inline">\(F\)</span>) from the straight line.</p>
<div class="note info">
            <p>In <span class="math inline">\(\mathbb{R}\)</span>, an isolated point is the common endpoint of two complementary intervals. As a consequence, a complete set is a closed set without adjacent complementary intervals.</p>
          </div>
<div class="note info">
            <p>In <span class="math inline">\(\mathbb{R}^n\)</span>, an open set is a union of at most countable semi-open-closed intervals.</p>
          </div>
<h2 id="cantor-ternary-set">5.1. Cantor Ternary Set</h2>
<p>Suppose <span class="math inline">\(E_0=[0, 1]\)</span>. One starts by deleting the open middle third <span class="math inline">\(\displaystyle \left(\frac{1}{3}, \frac{2}{3}\right)\)</span> from <span class="math inline">\(E_0\)</span>, leaving <span class="math inline">\(\displaystyle E_1=\left[0, \frac{1}{3}\right] \cup \left[\frac{2}{3}, 1\right]\)</span>. Next, the open middle third of each of remaining closed intervals is deleted, leaving <span class="math display">\[\displaystyle E_2=\left[0, \frac{1}{9}\right] \cup \left[\frac{2}{9}, \frac{1}{3}\right] \cup \left[\frac{2}{3}, \frac{7}{9}\right] \cup \left[\frac{8}{9}, 1\right].\]</span> This process is continued ad infinitum, and thus the Cantor ternary set <span class="math inline">\(P\)</span> is <span class="math display">\[P=\lim_{n \to \infty} E_n=\bigcap_{n=0}^\infty E_n.\]</span></p>
<p><strong>Property 5.4</strong>    <span class="math inline">\(P\)</span> is complete. Since we delete countable disjoint open intervals, then <span class="math inline">\(P\)</span> is a closed set without adjacent complementary intervals.</p>
<p><strong>Property 5.5</strong>    <span class="math inline">\(P\)</span> does not have interior point. Take an arbitrary <span class="math inline">\(x \in P\)</span>, we know <span class="math inline">\(B_{3^{-n}}(x)\)</span> always includes a point that does not belong to <span class="math inline">\(P\)</span>. Since <span class="math inline">\(3^{-n} \to 0\)</span>, then we cannot find a <span class="math inline">\(B_\delta(x) \subset P\)</span>.</p>
<p><strong>Definition 5.3</strong>    A set is <strong>nowhere dense</strong> or <strong>rare</strong> if its closure has empty interior.</p>
<p><strong>Property 5.6</strong>    <span class="math inline">\(\overset{=}{P}=c\)</span>.</p>
<p><strong>Property 5.7</strong>    The length of <span class="math inline">\(P\)</span> is 0.</p>
<div class="note info">
            <p><font color="#696969" font size="2">不难发现，<span class="math inline">\(P\)</span>是一个非常“古怪”的集合，因此引申出一个<strong>分形（Fractal）</strong>的概念.</font></p>
          </div>
]]></content>
      <categories>
        <category>Mathematics</category>
        <category>Real analysis</category>
      </categories>
      <tags>
        <tag>Set</tag>
      </tags>
  </entry>
</search>
