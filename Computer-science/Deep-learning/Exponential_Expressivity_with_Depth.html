<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1. Superposition of Sigmoidal Functions Consider the feedforward network with one hidden layer.  Input: \(\mathbf{h}_1&#x3D;\mathbf{x} \in \mathbb{R}^n\). Hidden layer: \(\mathbf{h}_2&#x3D;\phi(W^{(1)}\mathbf{h">
<meta property="og:type" content="article">
<meta property="og:title" content="Exponential Expressivity With Depth">
<meta property="og:url" content="https://litianyang0211.github.io/Computer-science/Deep-learning/Exponential_Expressivity_with_Depth">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:description" content="1. Superposition of Sigmoidal Functions Consider the feedforward network with one hidden layer.  Input: \(\mathbf{h}_1&#x3D;\mathbf{x} \in \mathbb{R}^n\). Hidden layer: \(\mathbf{h}_2&#x3D;\phi(W^{(1)}\mathbf{h">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://litianyang0211.github.io/images/Exponential_Expressivity_with_Depth_1.png">
<meta property="article:published_time" content="2021-10-18T23:00:00.000Z">
<meta property="article:modified_time" content="2021-11-16T10:09:56.587Z">
<meta property="article:tag" content="DNN">
<meta property="article:tag" content="Expressivity">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://litianyang0211.github.io/images/Exponential_Expressivity_with_Depth_1.png">

<link rel="canonical" href="https://litianyang0211.github.io/Computer-science/Deep-learning/Exponential_Expressivity_with_Depth.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Exponential Expressivity With Depth | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Computer-science/Deep-learning/Exponential_Expressivity_with_Depth">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Exponential Expressivity With Depth
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-19 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-19T00:00:00+01:00">2021-10-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-16 10:09:56" itemprop="dateModified" datetime="2021-11-16T10:09:56+00:00">2021-11-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-science/" itemprop="url" rel="index"><span itemprop="name">Computer science</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-science/Deep-learning/" itemprop="url" rel="index"><span itemprop="name">Deep learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="superposition-of-sigmoidal-functions">1. Superposition of Sigmoidal Functions</h1>
<p>Consider the feedforward network with one hidden layer.</p>
<ul>
<li><p>Input: <span class="math inline">\(\mathbf{h}_1=\mathbf{x} \in \mathbb{R}^n\)</span>.</p></li>
<li><p>Hidden layer: <span class="math inline">\(\mathbf{h}_2=\phi(W^{(1)}\mathbf{h}_1+\mathbf{b}^{(1)}) \in \mathbb{R}^m\)</span>.</p></li>
<li><p>Output: <span class="math inline">\(\displaystyle H(\mathbf{x}, \theta)=\alpha^T\mathbf{h}_2=\sum_{i=1}^m \alpha_i\phi(W_i^{(1)}\mathbf{x}+b_i)\)</span> with <span class="math inline">\(\phi(t) \in [0, 1]\)</span>.</p></li>
</ul>
<p><span id="thmC89"><strong>Theorem (Cybenko, 1989)</strong></span>    Let <span class="math inline">\(\phi(t)\)</span> be a continuous monotone function with <span class="math inline">\(\displaystyle \lim_{t \to -\infty} \phi(t)=0\)</span> and <span class="math inline">\(\displaystyle \lim_{t \to \infty} \phi(t)=1\)</span>. (For example, we may let <span class="math inline">\(\phi\)</span> be the sigmoid function.) Then the set of functions of the form <span class="math inline">\(\displaystyle H(\mathbf{x}, \theta)=\sum_{i=1}^m \alpha_i\phi(W_i^{(1)}\mathbf{x}+b_i)\)</span> is dense in <span class="math inline">\(C_n([0, 1])\)</span>, where <span class="math inline">\(C_n([0, 1])=C([0, 1]^n)\)</span> is the space of continuous functions from <span class="math inline">\([0, 1]^n\)</span> to <span class="math inline">\([0, 1]\)</span> with the metric <span class="math inline">\(d(f, g)=\sup |f(x)-g(x)|\)</span>.</p>
<div class="note info">
            <p>One (or more) layer fully connected DNN is sufficient to approximate any continuous function, provided <span class="math inline">\(m\)</span> is large enough.</p>
          </div>
<h1 id="approximation-of-multilayer-feedforward-network">2. Approximation of Multilayer Feedforward Network</h1>
<p>Consider the feedforward network with one hidden layer.</p>
<ul>
<li><p>Input: <span class="math inline">\(\mathbf{h}_1=\mathbf{x} \in \mathbb{R}^n\)</span>.</p></li>
<li><p>Hidden layer: <span class="math inline">\(\mathbf{h}_2=\phi(W^{(1)}\mathbf{h}_1+\mathbf{b}^{(1)}) \in \mathbb{R}^m\)</span>.</p></li>
<li><p>Output: <span class="math inline">\(\displaystyle H(\mathbf{x}, \theta)=\alpha^T\mathbf{h}_2=\sum_{i=1}^m \alpha_i\phi(W_i^{(1)}\mathbf{x}+b_i)\)</span> with <span class="math inline">\(\phi(t) \in [0, 1]\)</span> non-constant.</p></li>
</ul>
<p><span id="thmH90"><strong>Theorem (Hornik, 1990)</strong></span>    Let <span class="math inline">\(\phi(t)\)</span> be unbounded, then <span class="math inline">\(\displaystyle H(\mathbf{x}, \theta)=\sum_{i=1}^m \alpha_i\phi(W_i^{(1)}\mathbf{x}+b_i)\)</span> is dense in <span class="math inline">\(L^p(\mu)\)</span> for all finite measures <span class="math inline">\(\mu\)</span> and <span class="math inline">\(1 \leq p&lt;\infty\)</span>, where <span class="math inline">\(L^p(\mu)\)</span> is the space of functions <span class="math inline">\(f\)</span> with <span class="math inline">\(\displaystyle \int |f|^p\text{d}\mu&lt;\infty\)</span> with metric <span class="math inline">\(\displaystyle d(f, g)=\left(\int |f-g|^p\text{d}\mu\right)^{1/p}\)</span>. Moreover, if <span class="math inline">\(\phi(t)\)</span> is continuous and bounded, then <span class="math inline">\(H(\mathbf{x}, \theta)\)</span> is dense in <span class="math inline">\(C_n([0, 1])\)</span>.</p>
<div class="note info">
            <p><a href="#thmH90">Hornik (1990)</a> extended the theorem given by <a href="#thmC89">Cybenko (1989)</a>, where previously we need a Sigmoid-like function. Both theorems show that if we take a nonlinear activation, then if we increase the <strong>width</strong>, which is increasing <span class="math inline">\(m\)</span>, we are able to approximate arbitrarily well.</p>
          </div>
<h1 id="representational-benefit-of-depth">3. Representational Benefit of Depth</h1>
<h2 id="representation-benefits-of-deep-feedforward-networks">3.1. Representation Benefits of Deep Feedforward Networks</h2>
<p>Telgarsky (2015) considered a specific construction of a function from a deep network which requires a shallow network to have exponential width.</p>
<p>Let <span class="math inline">\(\phi(x)=\text{ReLU}(x)=\max(x, 0)\)</span> and consider the two-layer network:</p>
<p><span class="math display">\[h_2(x)=2\phi(x)-4\phi(x-1/2)=\begin{cases}
0, &amp; x&lt;0 \\
2x, &amp; x \in [0, 1/2] \\
2-2x, &amp; x&gt;1/2
\end{cases}\]</span> and <span class="math inline">\(h_3(x)=\phi(h_2(x))\)</span> set the negative portion for <span class="math inline">\(x&gt;1\)</span> to zero.</p>
<p>For <span class="math inline">\(\phi(x)=\max(x, 0)\)</span>, let <span class="math display">\[f(x)=h_3(x)=\phi(2\phi(x)-4\phi(x-1/2))\]</span> and iterate this two-layer network <span class="math inline">\(k\)</span> times to obtain a <span class="math inline">\(2k\)</span>-layer network <span class="math inline">\(f^k(x)\)</span> with the property that it is piecewise linear with change in slope at <span class="math inline">\(x_i=i2^{-k}\)</span> for <span class="math inline">\(i=0, \ldots, 2^k\)</span>; <span class="math inline">\(f^k(x_i)=0\)</span> for <span class="math inline">\(i\)</span> even and <span class="math inline">\(f^k(x_i)=1\)</span> for <span class="math inline">\(i\)</span> odd.</p>
<p><img data-src="/images/Exponential_Expressivity_with_Depth_1.png"></p>
<center>
Figure 1: <span class="math inline">\(f\)</span>, <span class="math inline">\(f^2\)</span>, and <span class="math inline">\(f^3\)</span> <a href="#Tel15">[Tel15]</a>.
</center>
<p>A two-layer network with the same <span class="math inline">\(\phi(x)\)</span> requires <span class="math inline">\(m=2^k\)</span> to exactly express <span class="math inline">\(f^k(x)\)</span>. The deep network can be thought of as having <span class="math inline">\(6k\)</span> parameters, whereas the two-layer network requires <span class="math inline">\(3 \cdot 2^k+1\)</span> parameters.</p>
<p><span id="thmT15"><strong>Theorem (Telgarsky, 2015)</strong></span>    Define the function class <span class="math inline">\(F(\phi; m, L)\)</span> be the space of functions composed of <span class="math inline">\(L\)</span> layers fully connected <span class="math inline">\(m\)</span> widths feed forward network with nonlinear activation function <span class="math inline">\(\phi\)</span>. Let <span class="math display">\[\mathcal{R}(f)=n^{-1}\sum_{i=1}^n \chi[f(x_i) \neq y_i]\]</span> count the number of incorrect labels of the data set <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span>. Consider positive integers <span class="math inline">\(k, L, m\)</span> with <span class="math inline">\(m \leq 2^{(k-3)/L-1}\)</span>, then there exists a collection of <span class="math inline">\(n=2^k\)</span> points <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^n\)</span> with <span class="math inline">\(x_i \in [0, 1]\)</span> and <span class="math inline">\(y_i \in \{0, 1\}\)</span> s.t. <span class="math display">\[\min_{f \in F(\phi; 2, 2k)} \mathcal{R}(f)=0\ \ \text{and}\ \ \min_{g \in F(\phi; m, L)} \mathcal{R}(g) \geq \frac{1}{6}.\]</span></p>
<h2 id="error-bounds-for-approximations-with-deep-relu-networks">3.2. Error Bounds for Approximations with Deep ReLU Networks</h2>
<p>Consider <span class="math inline">\(f(x)=h_3(x)=\phi(2\phi(x)-4\phi(x-1/2))\)</span>, where <span class="math inline">\(\phi(x)=\text{ReLU}(x)=\max(x, 0)\)</span>. Let <span class="math inline">\(h_m(x)\)</span> denote the piecewise linear interpolation of <span class="math inline">\(h(x)=x^2\)</span> at <span class="math inline">\(2^{m+1}\)</span> equispaced points, then <span class="math display">\[h_m(x)=x-\sum_{s=1}^m 2^{-2s}f^s(x).\]</span> Consequently, <span class="math inline">\(h(x)=x^2\)</span> can be approximated on <span class="math inline">\([0, 1]\)</span> to uniform accuracy <span class="math inline">\(\varepsilon\)</span> by a ReLU network having depth and number of weights proportional to <span class="math inline">\(\ln(1/\varepsilon)\)</span>. Now, we can approximate <span class="math inline">\(x\)</span> to any power since <span class="math display">\[xy=\frac{1}{2}((x+y)^2-x^2-y^2).\]</span></p>
<div class="note info">
            <p>The <strong>Sobolev norm</strong> is similar to that of function with <span class="math inline">\(n-1\)</span> derivatives that are Lipschitz continuous <span class="math inline">\(\mathcal{C}^{n-1}([0, 1]^d)\)</span> excluding sets of measure zero: <span class="math display">\[\|f\|_{W^{n, \infty}}([0, 1]^d)=\max_{|s| \leq n} \mathop{\operatorname{ess}\sup}_{x \in [0, 1]^d} |D^sf(x)|.\]</span> Define the <strong>unit ball</strong> of function in <span class="math inline">\(W^{n, \infty}([0, 1]^d)\)</span> as <span class="math display">\[F_{n, d}=\{f \in W^{n, \infty}([0, 1]^d): \|f\|_{W^{n, \infty}}([0, 1]^d) \leq 1\},\]</span> where <span class="math inline">\(d\)</span> is the dimension and <span class="math inline">\(n\)</span> is the degree of smoothness.</p>
          </div>
<p><span id="thmY16"><strong>Theorem (Yarotsky, 2016)</strong></span>    For any <span class="math inline">\(d\)</span>, <span class="math inline">\(n\)</span> and <span class="math inline">\(\varepsilon \in (0, 1)\)</span>, there is a ReLU network with depth at most <span class="math inline">\(c(1+\ln(1/\varepsilon))\)</span> and at most <span class="math inline">\(c\varepsilon^{-d/n}(1+\ln(1/\varepsilon))\)</span> weights (width <span class="math inline">\(\mathcal{O}(\varepsilon^{-d/n})\)</span>, for <span class="math inline">\(c\)</span> a function of <span class="math inline">\(d\)</span> and <span class="math inline">\(n\)</span>, that can approximate any function from <span class="math inline">\(F_{d, n}\)</span> with absolute error <span class="math inline">\(\varepsilon\)</span>.</p>
<div class="note info">
            <p><a href="#thmY16">Yarotsky (2016)</a> showed that the depth grows logarithmically in the accuracy (so that we do not need a very deep network). For the number of weights, we need <span class="math inline">\(n \gg d\)</span>, i.e., we need the local smoothness to be high compared to the dimension.</p>
          </div>
<h2 id="further-work">3.3. Further Work</h2>
<p>There is a growing literature on the ability to express high dimensional data using deep network. The exponential complexity generated by depth allows these remarkable approximation rates. However, one needs to be able to train the network parameters to achieve these rates.</p>
<h1 id="geometric-notion-of-exponential-expressivity">4. Geometric Notion of Exponential Expressivity</h1>
<p>Prior to the approximation rate results from <a href="#thmT15">Telgarsky (2015)</a> and <a href="#thmY16">Yarotsky (2016)</a>, there were qualitative geometric results showing potential for exponential expressivity.</p>
<h2 id="relu-hyperplane-arrangement">4.1. ReLu Hyperplane Arrangement</h2>
<p>The action of ReLu to an affine transform is a linearly increasing function orthogonal to hyperplanes. Let <span class="math inline">\(W \in \mathbb{R}^{n_1 \times n_0}\)</span>, then for all <span class="math inline">\(i \in [n_1]\)</span> <span class="math display">\[H_i:=\{\mathbf{x} \in \mathbb{R}^{n_0}: W_i\mathbf{x}+b_i=0\},\]</span> where <span class="math inline">\(W_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(W\)</span>.</p>
<p>The normals to these hyperplanes partition the input dimension <span class="math inline">\(n_0\)</span>, and if <span class="math inline">\(W\)</span> is in general position (all subsets of rows are maximal rank), then the number of partitions is <span class="math display">\[\sum_{j=0}^{n_0}\binom{n_1}{j}.\]</span></p>
<p>The number of partitions in one layer is lower bounded by <span class="math display">\[\sum_{j=0}^{n_0}\binom{n_1}{j} \geq n_1^{\min\{n_0, n_1/2\}}\]</span> and each hidden layers can further subdivide these regions.</p>
<p><strong>Theorem (Pascanu et al., 2014)</strong>    An <span class="math inline">\(L\)</span> layer DNN with ReLU activation, input <span class="math inline">\(\mathbb{R}^{n_0}\)</span>, and hidden layers of width <span class="math inline">\(n_1, \ldots, n_L\)</span> partitions the input space into at least <span class="math display">\[\prod_{l=0}^L n_\mathscr{l}^{\min\{n_0, n_\mathscr{l}/2\}}.\]</span></p>
<h2 id="trajectory-length-of-random-dnn">4.2. Trajectory Length of Random DNN</h2>
<p>A random network <span class="math inline">\(f_{NN}(\mathbf{x}; \mathcal{P}, \mathcal{Q})\)</span> denotes a DNN: <span class="math display">\[\mathbf{h}^{(d)}=W^{(d)}\mathbf{z}^{(d)}+\mathbf{b}^{(d)}, \mathbf{z}^{(d+1)}=\phi(\mathbf{h}^{(d)}), d=0, \ldots, L-1,\]</span> which takes as input the vector <span class="math inline">\(\mathbf{x}\)</span>, and is parameterized by random weight matrices <span class="math inline">\(W^{(d)}\)</span> with entries sampled i.i.d. from the distribution <span class="math inline">\(\mathcal{P}\)</span>, and bias vectors <span class="math inline">\(\mathbf{b}^{(d)}\)</span> with entries drawn i.i.d. from distribution <span class="math inline">\(\mathcal{Q}\)</span>.</p>
<div class="note info">
            <p>While our goal is to train a network, DNN typically starts as random network which influences the ability to be trained.</p><p>Some popular choices are Gaussian (<span class="math inline">\(\mathcal{P}=\mathcal{N}(0, \sigma_W^2)\)</span>), or uniform (<span class="math inline">\(\mathcal{P}=\mathcal{U}(-C_W, C_W)\)</span>) initializations.</p>
          </div>
<p>Raghu et al. (2016) introduced the notion of <strong>trajectory length</strong> <span class="math display">\[\mathscr{l}(\mathbf{x}(t))=\int_t \left\|\frac{\text{d}\mathbf{x}(t)}{\text{d}t}\right\|\text{d}t\]</span> as a measure of expressivity of a DNN. In particular, they considered passing a simple geometric object <span class="math inline">\(\mathbf{x}(t)\)</span>, such as a line <span class="math inline">\(\mathbf{x}(t)=t\mathbf{x}_0+(1-t)\mathbf{x}_1\)</span> for <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1 \in \mathbb{R}^k\)</span> and measure the expected length of the output of the random DNN at layer <span class="math inline">\(d\)</span>: <span class="math display">\[\frac{\mathbb{E}[\mathscr{l}(\mathbf{z}^{(d)})]}{\mathscr{l}(\mathbf{x}(t))}\]</span></p>
<p><strong>Theorem (Raghu et al., 2016)</strong>    Consider a random DNN of width <span class="math inline">\(n\)</span> and depth <span class="math inline">\(L\)</span> with weights and bias are drawn i.i.d. <span class="math inline">\(W^{(\mathscr{l})}(i, j) \sim \mathcal{N}(0, \sigma_W^2/n)\)</span> and <span class="math inline">\(\mathbf{b}^{(\mathscr{l})}(j) \sim \mathcal{N}(0, \sigma_\mathbf{b}^2)\)</span>. Consider as input a one dimensional trajectory <span class="math inline">\(\mathbf{x}(t)\)</span> with arc-length <span class="math display">\[\mathscr{l}(\mathbf{x}(t))=\int_t \left\|\frac{\text{d}\mathbf{x}(t)}{\text{d}t}\right\|\text{d}t\]</span> and let <span class="math inline">\(\mathbf{z}^{(\mathscr{l})}(t)\)</span> be the output of the Gaussian random feedforward network with ReLu activation, then <span class="math display">\[\frac{\mathbb{E}[\mathscr{l}(\mathbf{z}^{(\mathscr{l})})]}{\mathscr{l}(\mathbf{x}(t))} \geq \mathcal{O}\left(\left(\frac{\sigma_W}{(\sigma_W^2+\sigma_\mathbf{b}^2)^{1/4}}\frac{n^{1/2}}{(n+(\sigma_W^2+\sigma_\mathbf{b}^2)^{1/2})^{1/2}}\right)^L\right).\]</span></p>
<p>Price et al. (2019) also extended the results to have all but <span class="math inline">\(\alpha\)</span> fraction of the entries in <span class="math inline">\(W\)</span> equal to <span class="math inline">\(0\)</span>.</p>
<p><strong>Theorem (Price et al., 2019)</strong>    Let <span class="math inline">\(f_{NN}(\mathbf{x}; \alpha, \mathcal{P}, \mathcal{Q})\)</span> be a random sparse net with layers of width <span class="math inline">\(n\)</span>. Then, if <span class="math inline">\(\mathbb{E}[|\mathbf{u}^TW_i|] \geq M\|\mathbf{u}\|\)</span>, where <span class="math inline">\(W_i\)</span> is the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(W \in \mathcal{P}\)</span>, and <span class="math inline">\(\mathbf{u}\)</span> and <span class="math inline">\(M\)</span> are constants, then <span class="math display">\[\mathbb{E}[\mathscr{l}(\mathbf{z}^{(\mathscr{l})}(t))] \geq \left(\frac{M}{2}\right)^L \cdot \mathscr{l}(\mathbf{x}(t))\]</span> for <span class="math inline">\(\mathbf{x}(t)\)</span> a one dimensional trajectory in input space.</p>
<div class="note info">
            <p>Unless <span class="math inline">\(\sigma_W\)</span> or <span class="math inline">\(\alpha\)</span> small enough at initialization, the pre-activation output is exponentially complex.</p>
          </div>
<h1 id="reference">5. Reference</h1>
<p><font size="2" style="line-height: 1;"></font></p>
<p><span id="Tel15">[Tel15] Matus Telgarsky. Representation Benefits of Deep Feedforward Networks. <em>arXiv Preprint arXiv:1509.08101</em>, 2015.</span></p>
<p></p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/DNN/" rel="tag"><i class="fa fa-tag"></i> DNN</a>
              <a href="/tags/Expressivity/" rel="tag"><i class="fa fa-tag"></i> Expressivity</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Computer-science/Deep-learning/Introduction_to_Deep_Learning" rel="prev" title="Introduction to Deep Learning">
      <i class="fa fa-chevron-left"></i> Introduction to Deep Learning
    </a></div>
      <div class="post-nav-item">
    <a href="/Statistics/Graphical-model/Undirected_Graphical_Model" rel="next" title="Undirected Graphical Model">
      Undirected Graphical Model <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#superposition-of-sigmoidal-functions"><span class="nav-text">1. Superposition of Sigmoidal Functions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#approximation-of-multilayer-feedforward-network"><span class="nav-text">2. Approximation of Multilayer Feedforward Network</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#representational-benefit-of-depth"><span class="nav-text">3. Representational Benefit of Depth</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#representation-benefits-of-deep-feedforward-networks"><span class="nav-text">3.1. Representation Benefits of Deep Feedforward Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#error-bounds-for-approximations-with-deep-relu-networks"><span class="nav-text">3.2. Error Bounds for Approximations with Deep ReLU Networks</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#further-work"><span class="nav-text">3.3. Further Work</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#geometric-notion-of-exponential-expressivity"><span class="nav-text">4. Geometric Notion of Exponential Expressivity</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#relu-hyperplane-arrangement"><span class="nav-text">4.1. ReLu Hyperplane Arrangement</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#trajectory-length-of-random-dnn"><span class="nav-text">4.2. Trajectory Length of Random DNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-text">5. Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt=""
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name"></p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
