<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Transformer in PyTorch">
<meta property="og:url" content="https://litianyang0211.github.io/Compute_Science/PyTorch/Transformer_in_PyTorch">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_1.png">
<meta property="og:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_2.png">
<meta property="article:published_time" content="2021-12-11T00:00:00.000Z">
<meta property="article:modified_time" content="2021-12-29T13:04:35.756Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_1.png">

<link rel="canonical" href="https://litianyang0211.github.io/Compute_Science/PyTorch/Transformer_in_PyTorch.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Transformer in PyTorch | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Compute_Science/PyTorch/Transformer_in_PyTorch">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer in PyTorch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-11 00:00:00" itemprop="dateCreated datePublished" datetime="2021-12-11T00:00:00+00:00">2021-12-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-29 13:04:35" itemprop="dateModified" datetime="2021-12-29T13:04:35+00:00">2021-12-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          
            <div class="post-description"><div></div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="prelim">1. Prelim</h1>
<p>本文使用PyTorch实现<a href="/Computer_Science/Natural_Language_Processing/Transformer">Transformer</a>, 并进一步补充细节。</p>
<p>首先导入所需模块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math, copy, time</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># seaborn用于可视化自注意力</span></span><br><span class="line"><span class="keyword">import</span> seaborn</span><br><span class="line">seaborn.set_context(context=<span class="string">&quot;talk&quot;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="model-architecture">2. Model Architecture</h1>
<p>大部分热门的神经序列转导模型（Neural Sequence Transduction Model）都具有编码—解码架构 <a href="#BCB14">(Bahdanau, Cho, and Bengio, 2014)</a>. Transformer也使用了这一架构：</p>
<span id="f1"><img data-src="/images/Transformer_in_PyTorch_1.png"></span>
<center>
Figure 1: Model architecture of Transformer <a href="#V+17">(Vaswani et al., 2017)</a>.
</center>
<p>整个Transformer是由各种类反复叠加形成的，最上层的为<code>EncoderDecoder</code>类，其中<code>__init__</code>函数初始化五个接口（均由其它类实现）：<code>src_embed</code>, <code>tgt_embed</code>, <code>encoder</code>, <code>decoder</code>和<code>generator</code>.</p>
<ul>
<li><p><code>src_embed</code>和<code>tgt_embed</code>: 均由<code>Embeddings</code>和<code>PositionalEncoding</code>构成（通过<code>torch.nn.Sequential</code>串联），负责输入序列和输出序列的词嵌入和位置编码。</p></li>
<li><p><code>Encoder</code>: 负责<span class="math inline">\(N=6\)</span>层<code>EncoderLayer</code>以及一层<code>LayerNorm</code>.</p></li>
<li><p><code>Decoder</code>: 负责<span class="math inline">\(N=6\)</span>层<code>DecoderLayer</code>以及一层<code>LayerNorm</code>.</p></li>
<li><p><code>Generator</code>: 包括一个全连接层和一个<code>softmax</code>层。</p></li>
</ul>
<h1 id="embeddings-and-positionalencoding">3. Embeddings and PositionalEncoding</h1>
<h2 id="embeddings">3.1. Embeddings</h2>
<p><code>Embeddings</code>负责将输入的Source Sequence进行词嵌入，每个单词嵌入后，映射为维度为<code>d_model</code>的向量。比如说，有10个单词，当<code>d_model=512</code>时，我们得到一个形状为<code>10 * 512</code>的词嵌入矩阵，每一行代表一个单词。在<code>forward</code>函数中乘以<span class="math inline">\(\sqrt{\texttt{d_model}}\)</span>是因为<code>nn.Embedding</code>在初始化时，用的是<code>xavier_uniform</code> (见<code>make_model</code>)，乘法运算是为了让最后分布的方差为<span class="math inline">\(1\)</span>, 使网络在训练时的收敛速度更快。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        d_model是嵌入向量的维度</span></span><br><span class="line"><span class="string">        vocab是当前语言的词典大小（语料库中单词个数）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model) <span class="comment"># 对输入的src进行向量化，得到一个形状为vocab * d_model的待训练的矩阵</span></span><br><span class="line">        self.d_model = d_model <span class="comment"># 假设d_model=512</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><p><code>torch.nn.Embedding</code>有多个参数，我们主要关注<code>num_embeddings</code>与<code>embedding_dim</code>. <code>num_embeddings</code>即词典的大小尺寸，<code>embedding_dim</code>即嵌入向量的维度。</p></li><li><p>在做欧洲语系和英语翻译的时候，很多词是共享词根的，因此他们的源语言和目标语言共享一个权重矩阵 <a href="#PW17">(Press and Wolf, 2017)</a>. 但对于其它语言之间（如中文和英文），则没有共享权重矩阵的必要。</p></li></ul>
          </div>
<p>如果需要共享权重，只需要将以下代码添加到模型中，其中<code>weight</code>是形状为<code>vocab * d_model</code>的词嵌入矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure>
<h2 id="positionalencoding">3.2. PositionalEncoding</h2>
<p><code>PositionalEncoding</code>的作用是添加位置信息。回顾位置编码的公式：</p>
<p><span class="math display">\[\begin{aligned}
\text{PE}(\text{pos}, 2i)&amp;=\sin(\text{pos}/10000^{2i/d_\text{model}}) \\
\text{PE}(\text{pos}, 2i+1)&amp;=\cos(\text{pos}/10000^{2i/d_\text{model}})
\end{aligned}\]</span></p>
<p>其中<span class="math inline">\(\text{pos}\)</span>指的是一句话中某个单词的位置，<span class="math inline">\(i\)</span>指的是词嵌入的维度序号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dropout一般取0.1</span></span><br><span class="line"><span class="string">        max_len=5000代表提前准备好长度为5000的序列的位置编码</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        pe = torch.zeros(max_len, d_model) <span class="comment"># 得到形状为5000 * 512的矩阵，一共5000个位置，每个位置用一个512维度向量来表示其位置编码</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) <span class="comment"># (5000) -&gt; (5000, 1)</span></span><br><span class="line">        <span class="comment"># 为了防止数值过大溢出的问题，一般先用log再用exp</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数下标的位置</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数下标的位置</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment"># (5000, 512) -&gt; (batch_size=1, 5000, 512)</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x为Embeddings的词嵌入结果，然后将位置编码pe封装成torch的Variable，二者相加</span></span><br><span class="line"><span class="string">        x的大小为(batch_size, seq_length, d_model)</span></span><br><span class="line"><span class="string">        pe[:, :x.size(1)]是取了max_len=5000中的x.size(1)=seq_length个</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><p>带<code>nn.Dropout</code>的网络可以防止出现过拟合，<code>nn.Dropout(p=dropout)</code>的意思是指该层的神经元在每次迭代训练时会随机有<span class="math inline">\(10\%\)</span>的可能性不参与训练。</p></li><li><p><code>torch.arange</code>的结果并不包含<code>end</code>. 比如<code>torch.arange(1, 3)</code>输出<code>tensor([1, 2])</code>, 其类型为<code>torch.int64</code>. 又比如<code>torch.arange(0, 512, 2)</code>生成从<span class="math inline">\(0\)</span>到<span class="math inline">\(512\)</span>的偶数。</p></li><li><p><code>.unsqueeze()</code>主要是对数据维度进行扩充。</p></li><li><p><code>register_buffer</code>函数通常用于保存一些模型参数之外的值。</p></li></ul>
          </div>
<h2 id="nn.sequential">3.3. nn.Sequential</h2>
<p>有了<code>Embeddings</code>和<code>PositionalEncoding</code>, 我们只需要通过<code>nn.Sequential</code>串联：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># src_vocab即源语言的词典大小</span></span><br><span class="line">nn.Sequential(Embeddings(d_model, src_vocab), </span><br><span class="line">              PositionalEncoding(d_model, dropout)) </span><br></pre></td></tr></table></figure>
<h1 id="encoder-and-decoder">4. Encoder and Decoder</h1>
<h2 id="multiheadedattention">4.1. MultiHeadedAttention</h2>
<p>Transformer是围绕着注意力（Attention）机制展开的，正如Vaswani等人 (2017) 提到的：</p>
<blockquote>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.</p>
</blockquote>
<p>简而言之，注意力机制将序列中的数据点联系起来。Transformer使用一种特定类型的注意力机制，称为多头注意力（Multi-Head Attention）——这是模型中最重要的部分。多头注意力如下图所示：</p>
<span id="f2"><img data-src="/images/Transformer_in_PyTorch_2.png"></span>
<center>
Figure 2: (Left) Scaled dot-product attention. (Right) Multi-Head attention consists of several attention layers running in parallel <a href="#V+17">(Vaswani et al., 2017)</a>.
</center>
<p>我们可以通过注意力构建多头注意力层，注意力公式输出值（Value）的加权平均，而权重来自查询（Query）和键（Key）的计算：<span class="math display">\[\text{Attention}(Q, K, V)=\text{softmax}\left(\frac{QK^T}{\sqrt{\dim(\mathbf{k})}}\right)V.\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    query, key和value的形状为(batch_size, head_num, seq_length, num_features)</span></span><br><span class="line"><span class="string">    head_num为注意力头的个数</span></span><br><span class="line"><span class="string">    seq_length是源语言传过来的memory中，当前序列的词的个数</span></span><br><span class="line"><span class="string">    num_features是每个词对应的向量表示</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    d_k = query.size(-<span class="number">1</span>) <span class="comment"># Q与K向量的num_features必须相等</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用mask，对已经计算好的scores，按照mask矩阵填-1e9</span></span><br><span class="line">    <span class="comment"># 这样在下一步在计算softmax时，被设置成-1e9的数对应的值可以被忽略</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask==<span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 对scores的最后一个维度执行softmax</span></span><br><span class="line">    p_attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 返回p_attn是为了可视化显示多头注意力机制</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>.matmul</code>用于计算高维矩阵，比如维度为<code>(batch_size, head_num, seq_length, num_features)</code>的矩阵。</li></ul>
          </div>
<p>我们再定义一个<code>clones</code>函数，帮助我们复制相同的结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="comment"># copy.deepcopy是深复制，指一个改变不会影响另一个</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>
<p>对于每一个头（Head），都使用三个矩阵<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>和<span class="math inline">\(W^V\)</span>把输入转换为<span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>和<span class="math inline">\(V\)</span>. 然后分别用每一个头进行自注意力的计算，最后把<span class="math inline">\(h\)</span>个头的输出拼接起来，用一个矩阵<span class="math inline">\(W^O\)</span>把输出压缩，具体的计算过程为：</p>
<p><span class="math display">\[\text{MultiHead}(Q, K, V)=\text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\]</span>其中<span class="math inline">\(\text{head}_i=\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span>, <span class="math inline">\(W_i^Q \in \mathbb{R}^{d_\text{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_\text{model} \times d_k}\)</span>, <span class="math inline">\(W_i^V \in \mathbb{R}^{d_\text{model} \times d_v}\)</span>以及<span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_\text{model}}\)</span>. 我们假设<span class="math inline">\(h=8\)</span>及<span class="math inline">\(d_k=d_v=d_\text{model}/h=64\)</span>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        <span class="comment"># 定义四个全连接网络，输入和输出均为d_model</span></span><br><span class="line">        <span class="comment"># 每个全连接网络有两类可训练参数：</span></span><br><span class="line">        <span class="comment"># weights: d_model * d_model的矩阵</span></span><br><span class="line">        <span class="comment"># biases: d_model的向量</span></span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        query, key和value的形状为(batch_size, seq_length, num_features)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 所有h个head的mask都是相同的</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># （1）使用线性变换，把d_model分配给h个头，每个头为d_k=d_model/h</span></span><br><span class="line">        <span class="comment"># 输出矩阵的形状为(batch_size, head_num, seq_length, num_features=d_k)</span></span><br><span class="line">        query, key, value = [l(x).view(nbathces, -<span class="number">1</span>, self.h, </span><br><span class="line">                                       self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">                             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, </span><br><span class="line">                                             (query, key, value))]</span><br><span class="line">                                     </span><br><span class="line">        <span class="comment">#（2）使用attention函数计算</span></span><br><span class="line">        x, self.attn = attention(query, key, value, </span><br><span class="line">                                 mask=mask, dropout=self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#（3）利用最后一层线性变换把h个头的d_k维向量拼接成h * d_k维的向量</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, </span><br><span class="line">                                                self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.Linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h2 id="sublayerconnection">4.2. SubLayerConnection</h2>
<p>SubLayer指的是<code>MultiHeadedAttention</code>以及<code>PositionwiseFeedForward</code>. 这两个子层的输入和输出都是形状为<code>(batch_size, seq_length, d_model)</code>的张量。<code>SubLayerConnection</code>类主要实现两个功能：残差连接以及层标准化（Layer Normalization）。为了简便起见，我们将层标准化放到了前面。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubLayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    LayerNorm + SubLayer + Dropout + Residual</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SubLayerConnection, self).__init__()</span><br><span class="line">        self.norm = nn.LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="comment"># sublayer是一个具体的MultiHeadedAttention</span></span><br><span class="line">        <span class="comment"># 或PositionwiseFeedForward对象</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
<h2 id="positionwisefeedforward">4.3. PositionwiseFeedForward</h2>
<p><code>PositionwiseFeedForward</code>是一个全连接层，由两个线性变换以及它们之间的ReLU激活组成，即：<span class="math display">\[\text{FFN}(x)=\max(0, xW_1+b_1)W_2+b_2.\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="comment"># d_ff是中间隐单元的个数</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure>
<h2 id="encoderlayer">4.4. EncoderLayer</h2>
<p><code>EncoderLayer</code>是由<code>MultiHeadedAttention</code>, <code>PositionwiseFeedForward</code>及其连接层构成的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        size=d_model</span></span><br><span class="line"><span class="string">        self_attn=MultiHeadedAttention, 第一个子层</span></span><br><span class="line"><span class="string">        feed_forward=PositionwiseFeedForward, 第二个子层</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SubLayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h2 id="encoder">4.5. Encoder</h2>
<p><code>Encoder</code>是由<span class="math inline">\(N\)</span>个相同结构的<code>EncoderLayer</code>堆栈而成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = nn.LayerNorm(layer.size) <span class="comment"># layer.size=d_model</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h2 id="decoderlayer">4.6. DecoderLayer</h2>
<p><code>DecoderLayer</code>中大部分的类是重复使用的，但我们要注意：底层的<code>MultiHeadAttention</code>输入目标语言序列，而上一层的<code>MultiHeadAttention</code>则要考虑源语言和目标语言。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        m = memory <span class="comment"># 来自源语言序列的Encoder之后的输出，供目标语言的序列检索匹配</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h2 id="decoder">4.7. Decoder</h2>
<p><code>Decoder</code>与`<code>Encoder</code>也是类似的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = nn.LayerNorm(layer.size) <span class="comment"># layer.size=d_model</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h1 id="generator">5. Generator</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    根据decoder的隐状态输出一个单词</span></span><br><span class="line"><span class="string">    d_model是decoder输出的大小，vocab是目标语言词典大小</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab) <span class="comment"># 全连接成vocab大小，作为softmax的输入</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h1 id="encoderdecoder">6. EncoderDecoder</h1>
<p><code>EncoderDecoder</code>的结构相对比较简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed <span class="comment"># 源语言序列的编码</span></span><br><span class="line">        self.tgt_embed = tgt_embed <span class="comment"># 目标语言序列的编码</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">                </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)    </span><br></pre></td></tr></table></figure>
<h1 id="mask">7. Mask</h1>
<p><code>Encoder</code>和<code>Decoder</code>有一个关键的不同：<code>Decoder</code>在解码第<span class="math inline">\(t\)</span>个时刻的时候只能使用<span class="math inline">\(1, \ldots, t\)</span>时刻的输入，而不能使用<span class="math inline">\(t+1\)</span>时刻及其之后的输入。因此我们需要一个函数来产生一个Mask矩阵：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span> <span class="comment">#将numpy转换为tensor格式，判断是否为0，输出布尔值</span></span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>np.triu</code>生成一个三角矩阵，<code>k=1</code>表示第<span class="math inline">\(k\)</span>条对角线以下都设置为<code>0</code>.</li></ul>
          </div>
<h1 id="full-model">8. Full Model</h1>
<p>我们现在定义一个函数来生成完整的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="params"><span class="function">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    src_vocab: 源语言词表大小</span></span><br><span class="line"><span class="string">    tgt_vocab: 目标语言词表大小</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    c = copy.deepcopy <span class="comment"># 为了简洁</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    </span><br><span class="line">    model = EncoderDecoder(Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">                           Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">                           nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">                           nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">                           Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 随机初始化参数</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h1 id="reference">9. Reference</h1>
<div id="BCB14" style="line-height: 18px; font-size: 15px;">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. <em>arXiv Preprint arXiv:1409.0473</em>, 2014.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="H18" style="line-height: 18px; font-size: 15px;">
Harvard NLP. The Annotated Transformer. <em>Online: http://nlp.seas.harvard.edu/2018/04/03/attention.html</em>, 2018.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="PW17" style="line-height: 18px; font-size: 15px;">
Ofir Press and Lior Wolf. Using the Output Embedding to Improve Language Models. <em>arXiv Preprint arXiv:1608.05859</em>, 2017.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="V+17" style="line-height: 18px; font-size: 15px;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In <em>31st Conference on Neural Information Processing Systems</em>, 2017.
</div>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
              <a href="/tags/Attention/" rel="tag"><i class="fa fa-tag"></i> Attention</a>
              <a href="/tags/PyTorch/" rel="tag"><i class="fa fa-tag"></i> PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Compute_Science/Natural_Language_Processing/Transformer" rel="prev" title="Transformer">
      <i class="fa fa-chevron-left"></i> Transformer
    </a></div>
      <div class="post-nav-item">
    <a href="/Statistics/Graphical_Model/Causal_Inference" rel="next" title="Causal Inference">
      Causal Inference <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#prelim"><span class="nav-text">1. Prelim</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#model-architecture"><span class="nav-text">2. Model Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#embeddings-and-positionalencoding"><span class="nav-text">3. Embeddings and PositionalEncoding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#embeddings"><span class="nav-text">3.1. Embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positionalencoding"><span class="nav-text">3.2. PositionalEncoding</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#nn.sequential"><span class="nav-text">3.3. nn.Sequential</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#encoder-and-decoder"><span class="nav-text">4. Encoder and Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#multiheadedattention"><span class="nav-text">4.1. MultiHeadedAttention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sublayerconnection"><span class="nav-text">4.2. SubLayerConnection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positionwisefeedforward"><span class="nav-text">4.3. PositionwiseFeedForward</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoderlayer"><span class="nav-text">4.4. EncoderLayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-text">4.5. Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoderlayer"><span class="nav-text">4.6. DecoderLayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-text">4.7. Decoder</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#generator"><span class="nav-text">5. Generator</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#encoderdecoder"><span class="nav-text">6. EncoderDecoder</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mask"><span class="nav-text">7. Mask</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#full-model"><span class="nav-text">8. Full Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-text">9. Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
