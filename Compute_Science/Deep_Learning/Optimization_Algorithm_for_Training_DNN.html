<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1. DNN Loss Function Consider a fully connected \(L\) layer deep network given by \[\mathbf{h}^{(\mathscr{l})}&#x3D;W^{(\mathscr{l})}\mathbf{z}^{(\mathscr{l})}+\mathbf{b}^{(\mathscr{l})}, \mathbf{z}^{(\mat">
<meta property="og:type" content="article">
<meta property="og:title" content="Optimization Algorithm for Training DNN">
<meta property="og:url" content="https://litianyang0211.github.io/Compute_Science/Deep_Learning/Optimization_Algorithm_for_Training_DNN">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:description" content="1. DNN Loss Function Consider a fully connected \(L\) layer deep network given by \[\mathbf{h}^{(\mathscr{l})}&#x3D;W^{(\mathscr{l})}\mathbf{z}^{(\mathscr{l})}+\mathbf{b}^{(\mathscr{l})}, \mathbf{z}^{(\mat">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-11-07T00:00:00.000Z">
<meta property="article:modified_time" content="2021-12-15T11:27:58.646Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="DNN">
<meta property="article:tag" content="Optimization Algorithm">
<meta property="article:tag" content="SGD">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://litianyang0211.github.io/Compute_Science/Deep_Learning/Optimization_Algorithm_for_Training_DNN.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Optimization Algorithm for Training DNN | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Compute_Science/Deep_Learning/Optimization_Algorithm_for_Training_DNN">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Optimization Algorithm for Training DNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-11-07 00:00:00" itemprop="dateCreated datePublished" datetime="2021-11-07T00:00:00+00:00">2021-11-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-12-15 11:27:58" itemprop="dateModified" datetime="2021-12-15T11:27:58+00:00">2021-12-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/Deep-Learning/" itemprop="url" rel="index"><span itemprop="name">Deep Learning</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="dnn-loss-function">1. DNN Loss Function</h1>
<p>Consider a fully connected <span class="math inline">\(L\)</span> layer deep network given by <span class="math display">\[\mathbf{h}^{(\mathscr{l})}=W^{(\mathscr{l})}\mathbf{z}^{(\mathscr{l})}+\mathbf{b}^{(\mathscr{l})}, \mathbf{z}^{(\mathscr{l}+1)}=\phi(\mathbf{h}^{(\mathscr{l})}), \mathscr{l}=0, \ldots, L-1,\]</span> for <span class="math inline">\(\mathscr{l}=1, \ldots, L\)</span> with nonlinear activation <span class="math inline">\(\phi(\cdot)\)</span> and <span class="math inline">\(W^{(\mathscr{l})} \in \mathbb{R}^{n_\mathscr{l} \times n_{\mathscr{l}-1}}\)</span>. The trainable parameters for DNN are <span class="math inline">\(\theta:=\{W^{(\mathscr{l})}, \mathbf{b}^{(\mathscr{l})}\}_{\mathscr{l}=1}^L\)</span>, which are learned by minimizing a high dimensional (<span class="math inline">\(|\theta| \sim n^2L\)</span>) loss function <span class="math inline">\(\mathcal{L}\)</span>.</p>
<p>The shape of <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span> and our knowledge about a good initial minimizer <span class="math inline">\(\theta^{(0)}\)</span> strongly influence our ability to learn the parameters <span class="math inline">\(\theta\)</span> for the DNN.</p>
<p>Let <span class="math inline">\(\displaystyle \delta_\mathscr{l}:=\frac{\partial \mathcal{L}}{\partial \mathbf{h}^{(\mathscr{l})}}\)</span> and <span class="math inline">\(D^{(\mathscr{l})}\)</span> be the diagonal matrix with <span class="math inline">\(D_{ii}^{(\mathscr{l})}=\phi&#39;(h_i^{(\mathscr{l})})\)</span>, we have <span class="math display">\[\delta_\mathscr{l}=D^{(\mathscr{l})}(W^{(\mathscr{l})})^T\delta_{\mathscr{l}+1} \quad \text{and} \quad \delta_L=D^L\nabla_{\mathbf{h}^{(L)}} \mathcal{L}\]</span> which gives the formula for computing the <span class="math inline">\(\delta_\mathscr{l}\)</span> for each layer as <span class="math display">\[\delta_\mathscr{l}=\left(\prod_{k=\mathscr{l}}^{L-1} D^{(k)}(W^{(k)})^T\right)D^{L}\nabla_{\mathbf{h}^{(L)}}\mathcal{L}\]</span> and the resulting gradient <span class="math inline">\(\nabla_\theta \mathcal{L}\)</span> with entries as <span class="math display">\[\frac{\partial \mathcal{L}}{\partial W^{(\mathscr{l})}} = \delta_{\mathscr{l}+1} \cdot \mathbf{h}_\mathscr{l}^T \quad \text{and} \quad \frac{\partial \mathcal{L}}{\partial \mathbf{b}^{(\mathscr{l})}} = \delta_{\mathscr{l}+1}.\]</span></p>
<h1 id="stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</h1>
<p>Given a loss function <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span>, <strong>gradient descent</strong> is given by <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \alpha \cdot \nabla_\theta \mathcal{L}(\theta; X, Y)\]</span> with <span class="math inline">\(\alpha\)</span> is called the <strong>learning rate</strong>. In deep learning, <span class="math inline">\(\mathcal{L}(\theta; X, Y)\)</span> is the sum of <span class="math inline">\(m\)</span> individual loss functions for <span class="math inline">\(m\)</span> data point <span class="math display">\[\mathcal{L}(\theta; X, Y)=m^{-1}\sum_{\mu=1}^m l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu).\]</span> For <span class="math inline">\(m \gg 1\)</span>, gradient descent is computationally too costly and instead one can break apart the <span class="math inline">\(m\)</span> loss functions into mini-batches, and repeatedly solve <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \alpha |\mathcal{S}_k|^{-1} \nabla_\theta \sum_{\mu \in \mathcal{S}_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu),\]</span> which is called <strong>stochastic gradient descent</strong> as typically <span class="math inline">\(\mathcal{S}_k\)</span> is chosen in some randomized method, usually as a partition of <span class="math inline">\([m]\)</span> and a sequence of <span class="math inline">\(\mathcal{S}_k\)</span> which cover <span class="math inline">\([m]\)</span> is called an <strong>epoch</strong>.</p>
<h2 id="challenge-and-benefit">2.1. Challenge and Benefit</h2>
<ul>
<li><p>SGD is preferable for large <span class="math inline">\(m\)</span> since it reduces the per iteration computational cost depending on <span class="math inline">\(m\)</span> to instead depending on <span class="math inline">\(|\mathcal{S}_k|\)</span>.</p></li>
<li><p>SGD and gradient descent require selection of a learning rage which in deep learning is typically selected using some costly trial and error heuristics.</p></li>
<li><p>The learning rate is typically chosen adaptively in a way that satisfies <span class="math display">\[\sum_{k=1}^\infty \alpha_k=\infty \quad \text{and} \quad \sum_{k=1}^\infty \alpha_k^2&lt;\infty.\]</span> In particular, <span class="math inline">\(\alpha_k \sim k^{-1}\)</span>.</p></li>
<li><p>The optimal selection of learning weight and selection of <span class="math inline">\(\mathcal{S}_k\)</span> depend on the unknown local Lipschitz constant <span class="math display">\[\| \nabla l(\theta_1; \mathbf{x}_\mu, \mathbf{y}_\mu) - \nabla l(\theta_2; \mathbf{x}_\mu, \mathbf{y}_\mu) \| \leq L_\mu \| \theta_1 - \theta_2 \|.\]</span></p></li>
</ul>
<h2 id="global-convergence-of-gradient-descent">2.2. Global Convergence of Gradient Descent</h2>
<p><span id="thm2.1"><strong>Lemma 2.1(Overestimation Property)</strong></span>    Let <span class="math inline">\(\mathcal{L}(\theta) \in \mathcal{C}^1(\mathbb{R}^n)\)</span> with <span class="math inline">\(\nabla \mathcal{L}\)</span> Lipschitz continuous with constant <span class="math inline">\(L\)</span>. Then for any <span class="math inline">\(\theta\)</span>, <span class="math inline">\(\mathbf{d} \in \mathbb{R}^n\)</span> and <span class="math inline">\(\alpha \in \mathbb{R}\)</span>: <span class="math display">\[\mathcal{L}(\theta + \alpha \mathbf{d}) \leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T \mathbf{d} + \alpha^2 \frac{L}{2} \|\mathbf{d}\|^2.\]</span> In particular, if <span class="math inline">\(\mathbf{d}=-\nabla \mathcal{L}(\theta)\)</span>, then <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta - \alpha \nabla \mathcal{L}(\theta)) &amp;\leq \mathcal{L}(\theta) - \alpha \| \nabla \mathcal{L}(\theta) \|^2 + \frac{L}{2} \alpha^2 \| \nabla \mathcal{L}(\theta) \|^2 \\
&amp;=\mathcal{L}(\theta) - \alpha \left(1 - \frac{L}{2}\alpha \right) \| \nabla \mathcal{L}(\theta) \|^2.
\end{aligned}\]</span></p>
<strong><em>Proof.</em></strong> By Taylor's theorem in integral form, we have <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta+\alpha\mathbf{d}) &amp;= \mathcal{L}(\theta)+\int_0^1 \nabla \mathcal{L}(\theta+\alpha t \mathbf{d})^T (\alpha\mathbf{d}) \text{d}t
\\&amp;=\mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha \int_0^1 [\nabla \mathcal{L}(\theta+\alpha t \mathbf{d})-\nabla \mathcal{L}(\theta)]^T\mathbf{d} \text{d}t 
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha \int_0^1 \| \nabla \mathcal{L}(\theta+\alpha t \mathbf{d})-\nabla \mathcal{L}(\theta) \| \cdot \| \mathbf{d} \| \text{d}t 
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha L \|\mathbf{d}\| \int_0^1 \|\theta+\alpha t \mathbf{d}-\theta \|\text{d}t
\\&amp;\leq \mathcal{L}(\theta) + \alpha \nabla \mathcal{L}(\theta)^T\mathbf{d} + \alpha^2 L \|\mathbf{d}\|^2 \int_0^1 t\text{d}t.
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h2 id="global-convergence-of-sgd">2.3. Global Convergence of SGD</h2>
<p>Suppose for convenience that <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, the expected gradient w.r.t. data point <span class="math inline">\(\displaystyle G^k:=\nabla_\theta \sum_{\mu \in \mathcal{S}_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu)\)</span> is <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[G^k]=\mathbb{E}[G^k \mid \mathcal{S}_k]=\sum_{i=1}^m \mathbb{E}[G^k \mid \mathcal{S}_k=i]P(\mathcal{S}_k=i)=\sum_{i=1}^m \nabla l_i(\theta^k) \cdot \frac{1}{m}=\nabla \mathcal{L}(\theta^k).\]</span></p>
<p>We also assume (<span class="math inline">\(|\mathcal{S}_k|=1\)</span>):</p>
<p><span id="a1">    (1) for all <span class="math inline">\(i \leq m\)</span>, <span class="math inline">\(\nabla l_i\)</span> is Lipschitz continuous with constant <span class="math inline">\(L\)</span>;</span></p>
<p><span id="a2">    (2) there exists <span class="math inline">\(M&gt;0\)</span> s.t. <span class="math inline">\(\text{Var}[G^k \mid \mathcal{S}_k]:=\mathbb{E}[(G^k-\nabla l(\theta^k))^T(G^k- \nabla l(\theta^k)) \mid \mathcal{S}_k] \leq M\)</span> for all <span class="math inline">\(k\)</span>.</span></p>
<div class="note warning">
            <p>Bounded total variance can usually be guaranteed in a neighborhood of <span class="math inline">\(\theta^*\)</span>, but not globally for strongly convex <span class="math inline">\(\mathcal{L}(\cdot)\)</span>.</p><p>Recall that <span class="math inline">\(G^k\)</span> conditioned on current batch is an unbiased estimator of the true gradient, which is true (and for <span class="math inline">\(|\mathcal{S}_k|&gt;1\)</span>), but it would have to be assumed in a more general stochastic framework.</p>
          </div>
<p><span id="thm2.2"><strong>Lemma 2.2 (Overestimation Property in Expectation)</strong></span>    Assume <a href="#a1">assumption (1)</a> holds. When applying SGD to <span class="math inline">\(\mathcal{L}(\theta)\)</span> with <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, we have <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha \nabla\mathcal{L}(\theta^k)^T \mathbb{E}_{\mathcal{S}_k}[G^k] + \frac{L\alpha^2}{2}\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2].\]</span> If <a href="#a2">assumption (2)</a> also holds, then <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha^k\left(\frac{L\alpha^k}{2}-1\right) \|\nabla \mathcal{L}(\theta^k)\|^2+\frac{ML(\alpha^k)^2}{2}.\]</span></p>
<p><strong><em>Proof.</em></strong> Apply <a href="#thm2.1">lemma 2.1</a> to <span class="math inline">\(\mathcal{L}\)</span> with <span class="math inline">\(\theta=\theta^k\)</span>, <span class="math inline">\(\mathbf{d}=G^k\)</span> and <span class="math inline">\(\alpha=\alpha^k\)</span>. Using <span class="math inline">\(\theta^{k+1}=\theta^k-\alpha^kG^k\)</span>, we have <span class="math display">\[\mathcal{L}(\theta^{k+1}) \leq \mathcal{L}(\theta^k)-\alpha^k \nabla\mathcal{L}(\theta^k)^TG^k+\frac{L}{2}(\alpha^k)^2\|G^k\|^2.\]</span></p>
Applying expectation on both sides w.r.t. <span class="math inline">\(\mathcal{S}_k\)</span>: <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\alpha^k \nabla\mathcal{L}(\theta^k)^T\mathbb{E}_{\mathcal{S}_k}[G^k]+\frac{L}{2}(\alpha^k)^2 \mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2]\]</span> where <span class="math inline">\(\mathcal{L}(\theta^k)\)</span> and <span class="math inline">\(\nabla \mathcal{L}(\theta^k)\)</span> do not depend on <span class="math inline">\(\mathcal{S}_k\)</span>. Since <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[G^k]=\nabla \mathcal{L}(\theta^k)\)</span>, then <span class="math display">\[\begin{aligned}
\text{Var}[G^k \mid \mathcal{S}_k]&amp;=\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2] - 2\nabla \mathcal{L}(\theta^k)^T \mathbb{E}_{\mathcal{S}_k}[G^k] + \|\nabla \mathcal{L}(\theta^k)\|^2
\\&amp;=\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2]-\|\nabla \mathcal{L}(\theta^k)\|^2,
\end{aligned}\]</span> and thus <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[\|G^k\|^2] \leq M + \|\nabla \mathcal{L}(\theta^k)\|^2\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm2.3"><strong>Theorem 2.3</strong></span>    Let <span class="math inline">\(\mathcal{L}\)</span> be smooth, <strong>strongly convex</strong>, i.e., for <span class="math inline">\(\mu&gt;0\)</span>, <span class="math inline">\(\displaystyle \mathcal{L}(\mathbf{x}+\mathbf{s}) \geq \mathcal{L}(\mathbf{x})+\mathbf{s}^T \nabla\mathcal{L}(\mathbf{x})+\frac{\mu}{2}\|\mathbf{s}\|^2\)</span> for all <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{s}\)</span>, and satisfying <a href="#a1">assumption (1)</a> and <a href="#a2">(2)</a>. Let SGD with fixed learning rate be applied to minimize <span class="math inline">\(\mathcal{L}\)</span>, where <span class="math inline">\(\displaystyle \alpha^k=\alpha=\frac{\eta}{L}\)</span> and <span class="math inline">\(\eta \in (0, 1]\)</span>. Then SGD converges linearly to a residual error: for all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu} \leq \left(1-\frac{\eta\mu}{L}\right)^k \cdot \left[\mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)-\frac{\eta M}{2 \mu}\right].\]</span></p>
<p><strong><em>Proof.</em></strong> <a href="#thm2.2">Lemma 2.2</a> and <span class="math inline">\(\displaystyle \frac{L\alpha}{2}-1=\frac{\eta}{2}-1&lt;-\frac{1}{2}\)</span> give <span class="math display">\[\mathbb{E}_{\mathcal{S}_k}[\mathcal{L}(\theta^{k+1})] \leq \mathcal{L}(\theta^k)-\frac{\alpha}{2} \|\nabla \mathcal{L}(\theta^k)\|^2+\frac{ML\alpha^2}{2}.\]</span> Taking expectation w.r.t the past, i.e., <span class="math inline">\(\mathcal{S}_0, \ldots, \mathcal{S}_{k-1}\)</span>, we note that we have a memoryless property and so current iterate only depends on previous sample size (<span class="math inline">\(\mathbb{E}=\mathbb{E}_k:=\mathbb{E}[\cdot \mid \mathcal{S}_0, \ldots, \mathcal{S}_k]=E_{\mathcal{S}_k}\)</span>):</p>
<p><span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*) \leq \mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha}{2}\mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]+\frac{ML\alpha^2}{2}.\]</span> A consequence of the strong convexity property is that global minimizer <span class="math inline">\(\theta^*\)</span> is unique and <span class="math display">\[\mathcal{L}(\theta^k)-\mathcal{L}(\theta^*) \geq \frac{1}{2\mu} \|\nabla \mathcal{L}(\theta^k)\|^2\]</span> and thus <span class="math display">\[\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)-\mathcal{L}(\theta^*)] \geq \frac{1}{2\mu} \mathbb{E}_{k-1}[\| \nabla \mathcal{L}(\theta^k)\|^2].\]</span></p>
<p>We deduce <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*) \leq (1-\mu\alpha)(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*))+\frac{ML\alpha^2}{2}\]</span> or equivalently <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha ML}{2\mu} \leq (1-\mu\alpha)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha ML}{2\mu}\right).\]</span></p>
Note that <span class="math inline">\(\displaystyle \alpha=\frac{\eta}{L} \leq \frac{1}{L} \leq \frac{1}{\mu}\)</span>. Replacing <span class="math inline">\(\alpha\)</span> gives <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{M\eta}{2\mu} \leq \left(1-\frac{\eta\mu}{L}\right)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{M\eta}{2\mu}\right).\]</span> By induction, the claim holds.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>Thus <span class="math display">\[\lim_{k \to \infty} (\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)) \leq \frac{\alpha ML}{2\mu}=\frac{\eta M}{2 \mu}.\]</span> Convergence is obtained, in expectation, up to the level <span class="math inline">\(\displaystyle \frac{\eta M}{2\mu}\)</span> (noise level), which can be decreased in various ways. The ratio <span class="math inline">\(\displaystyle \frac{L}{\mu}\)</span> is a condition number of <span class="math inline">\(\mathcal{L}\)</span>.</p>
          </div>
<h2 id="decreasing-the-sgd-noise-level">2.4. Decreasing the SGD Noise Level</h2>
<p>Though not always desirable (due to the needs for small generalization error), the SGD noise level of <span class="math inline">\(\displaystyle \frac{\eta M}{2\mu}\)</span> can be removed so that <span class="math display">\[\lim_{k \to \infty} \mathbb{E}[\mathcal{L}(\theta^k)]=\mathcal{L}(\theta^*).\]</span></p>
<h3 id="dynamic-stepsize-reduction">2.4.1. Dynamic Stepsize Reduction</h3>
<p>We dynamically reduce <span class="math inline">\(\displaystyle \alpha^k=\frac{\eta_k}{L}\)</span>. Note that <span class="math inline">\(\eta_k \to 0\)</span> makes the residual <span class="math inline">\(\displaystyle \frac{\eta_kM}{2\mu} \to 0\)</span>, but it also means that <span class="math inline">\(\displaystyle \left(1-\frac{\eta_k}{L}\right) \to 1\)</span>, so the price is that we lose linear convergence.</p>
<p><strong>Theorem 2.4 (Dynamic Stepsize Stochastic Gradient Descent, DS-SGD)</strong>    Let <span class="math inline">\(\displaystyle \alpha^k=\frac{2}{2L+k\mu}\)</span> for all <span class="math inline">\(k \geq 0\)</span>. Then SGD satisfies <span class="math display">\[0 \leq \mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*) \leq \frac{v}{2L/\mu+k}\]</span> for all <span class="math inline">\(k \geq 0\)</span>, where <span class="math inline">\(\displaystyle v:=\frac{2L}{\mu}\max\left\{\frac{M}{\mu}, \mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)\right\}\)</span>. Thus <span class="math display">\[\lim_{k \to \infty} \mathbb{E}[\mathcal{L}(\theta^k)]=\mathcal{L}(\theta^*).\]</span></p>
<div class="note warning">
            <p>The rate is sublinear, <span class="math inline">\(\displaystyle \mathcal{O}\left(\frac{1}{k}\right)\)</span>.</p>
          </div>
<p><strong><em>Proof.</em></strong> Note that <span class="math inline">\(\displaystyle \alpha^k \leq \frac{1}{L} \leq \frac{1}{\mu}\)</span> and all arguments continue to hold in the proof of <a href="#thm2.3">theorem 2.3</a> until and including, and so for all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu} \leq (1-\mu\alpha^k)\left(\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu}\right).\]</span></p>
<p>We prove the conclusion by induction. Clearly at <span class="math inline">\(k=0\)</span>, the conclusion holds. Assume the conclusion holds at <span class="math inline">\(k&gt;0\)</span>, and substitute the conclusion into the above inequality, we obtain <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]-\mathcal{L}(\theta^*)-\frac{\alpha^kML}{2\mu} \leq (1-\mu\alpha^k)\left(\frac{v}{2L/\mu+k}-\frac{\alpha^kML}{2\mu}\right).\]</span></p>
Using the expression of <span class="math inline">\(\alpha^k\)</span> in the above and simplifying the expressions provides the conclusion with <span class="math inline">\(k\)</span> replaced by <span class="math inline">\((k+1)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h3 id="increasing-mini-batch-size">2.4.2. Increasing Mini-batch Size</h3>
<p>We increase mini-batch size from <span class="math inline">\(|\mathcal{S}_k|=1\)</span> to <span class="math inline">\(|\mathcal{S}_k|=p \geq 1\)</span>. Let <span class="math display">\[G^k=\frac{1}{p}\sum_{j \in \mathcal{S}_k} \nabla l_j(\theta^k)\]</span> where <span class="math inline">\(j \in \mathcal{S}_k \overset{\text{i.i.d.}}{\sim} \mathcal{U}(\{1, \ldots, m\})\)</span>. Then <span class="math display">\[\begin{aligned}
\text{Var}[G^k \mid \mathcal{S}_k]&amp;=\sum_{j \in \mathcal{S}_k} \frac{1}{p^2} \mathbb{E}_{\mathcal{S}_k} [\| \nabla l_j(\theta^k)-\nabla \mathcal{L}(\theta^k)\|^2]
\\&amp;\ \ \ \ +2\sum_{j&lt;i} \frac{1}{p^2} \mathbb{E}_{\mathcal{S}_k}[\nabla l_j(\theta^k)-\nabla \mathcal{L}(\theta^k)]^T \mathbb{E}_{\mathcal{S}_k}[\nabla l_i(\theta^k)-\nabla \mathcal{L}(\theta^k)]
\\&amp;=\frac{1}{p^2}\sum_{j \in \mathcal{S}_k} \text{Var}[\nabla l_j(\theta^k)]+0 \leq \frac{M}{p},
\end{aligned}\]</span> where <span class="math inline">\(|\mathcal{S}_k|=p\)</span> and the independence of <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> indices in <span class="math inline">\(\mathcal{S}_k\)</span> in the first equality as well as the lack of bias <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[\nabla l_j(\theta^k)]=\nabla \mathcal{L}(\theta^k)\)</span>. We also have <span class="math inline">\(\mathbb{E}_{\mathcal{S}_k}[G_k]=\nabla \mathcal{L}(\theta^k)\)</span>, i.e., unbiased batch gradient. Then as in <a href="#thm2.3">theorem 2.3</a>, we deduce, under the same assumptions, <span class="math display">\[\mathbb{E}[\mathcal{L}(\theta^k)]-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu p} \leq \left(1-\frac{\eta\mu}{L}\right)^k \cdot \left( \mathcal{L}(\theta^0)-\mathcal{L}(\theta^*)-\frac{\eta M}{2\mu p}\right).\]</span> Thus the noise level is decreased by batch size <span class="math inline">\(p\)</span>, without impacting the convergence factor.</p>
<h3 id="momentum-for-gradient-variance-reduction">2.4.3. Momentum for Gradient Variance Reduction</h3>
<p>We use acceleration by momentum to reduce <span class="math inline">\(\text{Var}[G^k \mid \mathcal{S}_k]\)</span>, which yields <span class="math inline">\(\mathbb{E}[\mathcal{L}(\theta^k)] \to \mathcal{L}(\theta^*)\)</span> with linear convergence rate, with a much smaller cost per iteration than mini-batching.</p>
<h3 id="other-technique">2.4.4. Other Technique</h3>
<ul>
<li><p>Variance reduction (SVRG)</p></li>
<li><p>SAG</p></li>
<li><p>SAGA</p></li>
</ul>
<h3 id="conclusion">2.4.5. Conclusion</h3>
<p>Each of the three approaches (2.4.1 to 2.4.3) for accelerating SGD have merit and are often all used at once. In particular, once SGD appears to stagnate, one both reduces the stepsize and increases the batch-size; though this is stopped once validation error begins to increase.</p>
<h2 id="global-convergence-of-sgd-non-convex">2.5. Global Convergence of SGD: Non-convex</h2>
<p><strong>Theorem 2.5 (SGD with Fixed Stepsize)</strong>    Let <span class="math inline">\(\mathcal{L} \in \mathcal{C}^1(\mathbb{R}^n)\)</span> be bounded below by <span class="math inline">\(\mathcal{L}_\text{low}\)</span>, with <span class="math inline">\(\nabla \mathcal{L}\)</span> Lipschitz continuous with constant <span class="math inline">\(L\)</span> (<a href="#a1">assumption (1)</a>). Assume <a href="#a2">assumption (2)</a> holds. Apply the SGD method with fixed stepsize <span class="math inline">\(\displaystyle \alpha=\frac{\eta}{L}\)</span>, where <span class="math inline">\(\eta \in (0, 1]\)</span>, and <span class="math inline">\(|\mathcal{S}_k|=1\)</span>, to minimize <span class="math inline">\(\mathcal{L}\)</span>. Then <span class="math display">\[\min_{0 \leq i \leq k} \mathbb{E}[\|\nabla \mathcal{L}(\theta^i)\|^2] \leq \alpha LM + \frac{2(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{k\alpha}=\eta M + \frac{2L(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{k\eta}\]</span> and therefore the SGD method takes at most <span class="math inline">\(\displaystyle k \leq \frac{2L(\mathcal{L}(\theta^0)-\mathcal{L}_\text{low})}{\eta\varepsilon}\)</span> iterations/evaluations to generate <span class="math inline">\(\mathbb{E}[\|\nabla \mathcal{L}(\theta^k)\|^2] \leq \varepsilon+\eta M\)</span>.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\mathbb{E}_k[\mathcal{L}(\theta^{k+1})] \leq \mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\frac{\alpha}{2}\mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]+\frac{ML\alpha^2}{2}.\]</span> We need to connect the per iteration decrease with the gradient. For all <span class="math inline">\(k \geq 0\)</span>, <span class="math display">\[\mathbb{E}_{k-1}[\mathcal{L}(\theta^k)]-\mathbb{E}_k[\mathcal{L}(\theta^{k+1})] \geq \frac{\alpha}{2} \mathbb{E}_{k-1}[\|\nabla \mathcal{L}(\theta^k)\|^2]-\frac{ML\alpha^2}{2}.\]</span> Summing up the above bound from <span class="math inline">\(i=0\)</span> to <span class="math inline">\(k\)</span>, we deduce <span class="math display">\[\begin{aligned}
\mathcal{L}(\theta^0)-\mathcal{L}_\text{low} &amp;\geq \mathcal{L}(\theta^0)-\mathbb{E}_k[\mathcal{L}(\theta^{k+1})]
\\&amp;\geq \frac{\alpha}{2}\sum_{i=0}^k \mathbb{E}_{i-1}[\|\nabla \mathcal{L}(\theta^i)\|^2]-(k+1)\frac{ML\alpha^2}{2}
\\&amp;\geq \frac{\alpha}{2}(k+1)\left[\min_{0 \leq i \leq k} \mathbb{E}[\|\nabla \mathcal{L}(\theta^i)\|^2]-ML\alpha\right].
\end{aligned}\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The noise floor that limits the accuracy can be obtained. The acceleration momentum method is difficult in the nonconvex case.</p>
          </div>
<h1 id="sgd-improvement">3. SGD Improvement</h1>
<p>There are many improvements of SGD typically used in practice for deep learning.</p>
<h2 id="momentum">3.1. Momentum</h2>
<ul>
<li><p><strong>Polyak momentum</strong>: <span class="math display">\[\theta^{(k+1)}=\theta^{(k)} + \beta(\theta^{(k)} - \theta^{(k-1)}) - \alpha \nabla_\theta \mathcal{L}(\theta^{(k)}).\]</span></p></li>
<li><p><strong>Nesterov's accelerated gradient</strong>: <span class="math display">\[\begin{aligned}
\widehat{\theta}^k &amp;= \theta^{(k)}+\beta(\theta^{(k)}-\theta^{(k-1)}) \\
\theta^{(k+1)} &amp;= \widehat{\theta}^{(k)} - \alpha\nabla_\theta\mathcal{L}(\widehat{\theta}^{(k)})
\end{aligned}\]</span></p></li>
</ul>
<p>These acceleration methods give substantial improvements in the linear convergence rate for convex problems. Note that the linear convergence rates are: normal GD <span class="math inline">\(\displaystyle \frac{\kappa-1}{\kappa+1}\)</span>, Polyak <span class="math inline">\(\displaystyle \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}\)</span>, and NAG <span class="math inline">\(\displaystyle \sqrt{\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}}}\)</span>.</p>
<h2 id="adaptive-sub-gradient">3.2. Adaptive Sub-Gradient</h2>
<p><strong>Preconditioning</strong> improves convergence rate of line-search methods.</p>
<p>Let <span class="math inline">\(\mathbf{g}^{(k)}(\theta^{(k)})=:\nabla_\theta \mathcal{L}(\theta^{(k)})\)</span> be the gradient of the training loss function at iteration <span class="math inline">\(k\)</span> and <span class="math display">\[B_k(i, i)=\left(\sum_{j=1}^k (\mathbf{g}^{(j)}(\theta^{(j)})(i))^2\right)^{1/2}\]</span> be the diagonal of the square root of the sum of prior gradient outer products. <strong>Adaptive sub-gradient (AdaGrad)</strong> is preconditioned GD via the diagonal matrix <span class="math inline">\(B\)</span>: <span class="math display">\[\theta^{(k+1)} = \theta^{(k)} - \eta|\Lambda_k|^{-1}(B^{(k)}+\varepsilon I)^{-1}\nabla_\theta\sum_{\mu \in \Lambda_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu),\]</span> where <span class="math inline">\(\eta\)</span> is the learning rate, <span class="math inline">\(\Lambda_k\)</span> is chosen in some randomized method, and <span class="math inline">\(\varepsilon I&gt;0\)</span> is added to avoid poor scaling of small values of <span class="math inline">\(B^{(k)}(i, i)\)</span>.</p>
<h3 id="adagrad-improvement-rmsprop-and-adadelta">3.2.1. AdaGrad Improvement: RMSProp and AdaDelta</h3>
<p>RMSProp gives more weight to the current gradient: <span class="math display">\[B_k^\text{RMS}(i, i)=\gamma B_{k-1}^\text{RMS}(i, i) + (1-\gamma)(\mathbf{g}^{(k)}(\theta^{(k)})(i))^2\]</span> for some <span class="math inline">\(\gamma \in [0, 1]\)</span> and updates as <span class="math display">\[\theta^{(k+1)}=\theta^{(k)} - \eta|\Lambda_k|^{-1}(B^{(k)}+\varepsilon I)^{-1/2}\nabla_\theta\sum_{\mu \in \Lambda_k} l(\theta; \mathbf{x}_\mu, \mathbf{y}_\mu).\]</span></p>
<p>AdaDelta (Zeiler, 2012) extended AdaGrad using a similar preconditioned as <span class="math inline">\(B_k^\text{RMS}\)</span>, but also estimated the stepsize using an average difference in <span class="math inline">\(\theta^{(k)}-\theta^{(k-1)}\)</span>.</p>
<h3 id="scalar-adagrad">3.2.2. Scalar AdaGrad</h3>
<h4 id="iteration-complexity">3.2.2.1. Iteration Complexity</h4>
<p>Initialize with <span class="math inline">\(\theta^{(0)}\)</span> and <span class="math inline">\(b_0&gt;0\)</span>, <span class="math display">\[\begin{aligned}
b_k^2 &amp;= b_{k-1}^2+\|\nabla_\theta\mathcal{L}(\theta^{(k)})\|^2 \\
\theta^{(k)} &amp;= \theta^{(k-1)} - b_k^{-1}\nabla_\theta\mathcal{L}(\theta^{(k)}).
\end{aligned}\]</span></p>
<p>For <span class="math inline">\(\mathcal{L}(\theta) \in \mathcal{C}_L^1\)</span>, that is <span class="math inline">\(L\)</span> minimal for which <span class="math inline">\(\|\nabla_\theta\mathcal{L}(\theta_1)-\nabla_\theta\mathcal{L}(\theta_2)\| \leq L\|\theta_1-\theta_2\|\)</span> for all <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, then scalar batch AdaGrad satisfies <span class="math display">\[\min_{k=1, \ldots, T-1} \|\nabla_\theta\mathcal{L}(\theta^{(k)})\|^2 \leq \varepsilon\]</span> for either <span class="math display">\[T=1+\lceil 2\varepsilon^{-1}\mathcal{L}(\theta^{(0)})(b_0+2\mathcal{L}(\theta^{(0)})) \rceil\ \ \text{if}\ \ b_0 \geq L\]</span> or <span class="math display">\[T=1+\lceil \varepsilon^{-1}(L^2-b_0^2+4(\mathcal{L}(\theta^{(0)})+(3/4+\ln(L/b_0))L)^2) \rceil\ \ \text{if}\ \ b_0&lt;L.\]</span> In contrast, if <span class="math inline">\(b_k\)</span> is a fixed constant <span class="math inline">\(b\)</span>, then if <span class="math inline">\(b&lt;L/2\)</span>, GD can diverge, while if <span class="math inline">\(b \geq L\)</span>, then <span class="math inline">\(T=2b\varepsilon^{-1}\mathcal{L}(\theta^{(0)})\)</span>.</p>
<p>Iteration complexity for scalar batch AdaGrad following properties that for any non-negative values <span class="math inline">\(a_1, \ldots, a_T\)</span> with <span class="math inline">\(a_1&gt;0\)</span> <span class="math display">\[\sum_{\mathscr{l}=1}^T \frac{a_\mathscr{l}}{\sum_{i=1}^\mathscr{l} a_i} \leq \ln\left(\sum_{i=1}^T a_i\right)+1 \quad \text{and} \quad \sum_{\mathscr{l}=1}^T \frac{a_\mathscr{l}}{\sqrt{\sum_{i=1}^\mathscr{l}}a_i} \leq 2\sqrt{\sum_{i=1}^T a_i}.\]</span></p>
<p>In addition, for any fixed <span class="math inline">\(\varepsilon \in (0, 1]\)</span> and <span class="math inline">\(L, b_0&gt;0\)</span>, the iterates <span class="math inline">\(b_{k+1}^2=b_k^2+a_k\)</span> has the property that after <span class="math inline">\(N=\lceil \varepsilon^{-1}(L^2-b_0^2) \rceil+1\)</span> iterations, either <span class="math inline">\(\displaystyle \min_{0 \leq k \leq N-1} a_k \leq \varepsilon\)</span> or <span class="math inline">\(b_N \geq L\)</span>.</p>
<p>Let <span class="math inline">\(k_0\)</span> be the first iterate s.t. <span class="math inline">\(b_{k_0} \geq L\)</span>, then for all <span class="math inline">\(k \geq k_0\)</span>, <span class="math display">\[b_k \leq b_{k_0-1}+2\mathcal{L}(\theta^{(k_0-1)}) \quad \text{(bounded above)}\]</span> and <span class="math display">\[\mathcal{L}(\theta^{(k_0-1)}) \leq \frac{L}{2}(1+2\ln(b_{k_0-1} / b_0)) \quad \text{(not diverged)}.\]</span></p>
<h4 id="influence-of-mini-batch-or-other-gradient-approximation">3.2.2.2. Influence of Mini-batch or Other Gradient Approximation</h4>
<p>Let <span class="math inline">\(\mathbf{g}^{(k)}\)</span> be an unbiased estimator of the gradient <span class="math inline">\(\nabla_\theta \mathcal{L}(\theta^{(k)})\)</span> of the training loss function at iteration <span class="math inline">\(k\)</span>, i.e., <span class="math inline">\(\mathbb{E}[\mathbf{g}^{(k)}]=\nabla_\theta\mathcal{L}(\theta^{(k)})\)</span>. Moreover, suppose there is a uniform bound <span class="math inline">\(\mathbb{E}[\|\mathbf{g}^{(k)}\|^2] \leq c_\mathbf{g}^2\)</span>. Then consider the stochastic scalar AdaGrad update as <span class="math display">\[\begin{aligned}
b_k^2 &amp;= b_{k-1}^2+\|\mathbf{g}^{(k)}\|^2 \\
\theta^{(k)} &amp;= \theta^{(k-1)}-b_k^{-1}\mathbf{g}^{(k)}.
\end{aligned}\]</span></p>
<p>Unlike in the batch version of AdaGrad where <span class="math inline">\(b_k\)</span> converges to a fixed stepsize, stochastic AdaGrad converges roughly at the rate <span class="math inline">\(b_k \approx c_\mathbf{g}k^{1/2}\)</span>. Moreover, Ward et al. (2018) showed that <span class="math display">\[\min_{\mathscr{l}=0, \ldots, N-1}(\mathbb{E} \|\nabla_\theta\mathcal{L}(\theta^{(\mathscr{l})})\|^{4/3})^{3/2} \leq \mathcal{O}\left(\frac{b_0+c_\mathbf{g}}{N}+\frac{c_\mathbf{g}}{N^{1/2}}\right)\ln(Nc_\mathbf{g}^2/b_0^2).\]</span></p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/DNN/" rel="tag"><i class="fa fa-tag"></i> DNN</a>
              <a href="/tags/Optimization-Algorithm/" rel="tag"><i class="fa fa-tag"></i> Optimization Algorithm</a>
              <a href="/tags/SGD/" rel="tag"><i class="fa fa-tag"></i> SGD</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Compute_Science/Deep_Learning/DNN_Initialization" rel="prev" title="DNN Initialization">
      <i class="fa fa-chevron-left"></i> DNN Initialization
    </a></div>
      <div class="post-nav-item">
    <a href="/Statistics/Graphical_Model/Directed_Graphical_Model" rel="next" title="Directed Graphical Model">
      Directed Graphical Model <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#dnn-loss-function"><span class="nav-text">1. DNN Loss Function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#stochastic-gradient-descent-sgd"><span class="nav-text">2. Stochastic Gradient Descent (SGD)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#challenge-and-benefit"><span class="nav-text">2.1. Challenge and Benefit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#global-convergence-of-gradient-descent"><span class="nav-text">2.2. Global Convergence of Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#global-convergence-of-sgd"><span class="nav-text">2.3. Global Convergence of SGD</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decreasing-the-sgd-noise-level"><span class="nav-text">2.4. Decreasing the SGD Noise Level</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#dynamic-stepsize-reduction"><span class="nav-text">2.4.1. Dynamic Stepsize Reduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#increasing-mini-batch-size"><span class="nav-text">2.4.2. Increasing Mini-batch Size</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#momentum-for-gradient-variance-reduction"><span class="nav-text">2.4.3. Momentum for Gradient Variance Reduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#other-technique"><span class="nav-text">2.4.4. Other Technique</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conclusion"><span class="nav-text">2.4.5. Conclusion</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#global-convergence-of-sgd-non-convex"><span class="nav-text">2.5. Global Convergence of SGD: Non-convex</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sgd-improvement"><span class="nav-text">3. SGD Improvement</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#momentum"><span class="nav-text">3.1. Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#adaptive-sub-gradient"><span class="nav-text">3.2. Adaptive Sub-Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#adagrad-improvement-rmsprop-and-adadelta"><span class="nav-text">3.2.1. AdaGrad Improvement: RMSProp and AdaDelta</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#scalar-adagrad"><span class="nav-text">3.2.2. Scalar AdaGrad</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#iteration-complexity"><span class="nav-text">3.2.2.1. Iteration Complexity</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#influence-of-mini-batch-or-other-gradient-approximation"><span class="nav-text">3.2.2.2. Influence of Mini-batch or Other Gradient Approximation</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
