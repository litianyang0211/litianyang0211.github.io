<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Introduction to Network">
<meta property="og:url" content="https://litianyang0211.github.io/Mathematics/Network_Science/Introduction_to_Network">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-10-10T23:00:00.000Z">
<meta property="article:modified_time" content="2022-01-14T22:37:48.734Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="Network">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://litianyang0211.github.io/Mathematics/Network_Science/Introduction_to_Network.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Introduction to Network | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Mathematics/Network_Science/Introduction_to_Network">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Introduction to Network
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-10-11 00:00:00" itemprop="dateCreated datePublished" datetime="2021-10-11T00:00:00+01:00">2021-10-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-14 22:37:48" itemprop="dateModified" datetime="2022-01-14T22:37:48+00:00">2022-01-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Mathematics/" itemprop="url" rel="index"><span itemprop="name">Mathematics</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Mathematics/Network-Science/" itemprop="url" rel="index"><span itemprop="name">Network Science</span></a>
                </span>
            </span>

          
            <div class="post-description"><div></div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="mathematical-preliminary">1. Mathematical Preliminary</h1>
<h2 id="probability">1.1. Probability</h2>
<h3 id="discrete-variable">1.1.1. Discrete Variable</h3>
<p>We use <span class="math inline">\(\langle \cdot \rangle\)</span> to denote the expected value, i.e. <span class="math display">\[\langle x \rangle=\sum_x xp(x).\]</span> The <span class="math inline">\(n\)</span>th moment of r.v. <span class="math inline">\(X\)</span> is defined by <span class="math display">\[\langle x^n \rangle=\sum_x x^np(x)\]</span> where <span class="math inline">\(n\)</span> is a positive integer. Hence, the variance is <span class="math display">\[\sigma^2=\langle(x-\langle x \rangle)^2\rangle=\langle x^2 \rangle-\langle x \rangle^2.\]</span> Moments can be generalized to the case of multiple r.v.s., often to evaluate correlation between them. A familiar measure of linear dependence between two variables is the Pearson correlation coefficient defined by <span class="math display">\[\rho_{X, Y}=\frac{\langle (x-\langle x \rangle)(y-\langle y \rangle) \rangle}{\sigma_X\sigma_Y}\]</span> where <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are the standard deviations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively.</p>
<p>A list of frequently used discrete distributions is given:</p>
<ol type="1">
<li><p>The Bernoulli distribution takes only two possible values, <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> (i.e., failure or success), w.p. <span class="math inline">\(1-p\)</span> and <span class="math inline">\(p\)</span>, respectively. We obtain <span class="math inline">\(\langle x \rangle=p\)</span> and <span class="math inline">\(\sigma^2=p(1-p)\)</span>.</p></li>
<li><p>The binomial distribution describes the outcome of <span class="math inline">\(n\)</span> i.i.d. r.v.s. generated by the Bernoulli distribution with parameter <span class="math inline">\(p\)</span>. The probability that exactly <span class="math inline">\(m\)</span> successes are observed is given by <span class="math display">\[p(m)=\binom{n}{m}p^m(1-p)^{n-m}\]</span> where <span class="math inline">\(0 \leq m \leq n\)</span>. We obtain <span class="math inline">\(\langle m \rangle=np\)</span> and <span class="math inline">\(\sigma^2=np(1-p)\)</span>.</p></li>
<li><p>The geometric distribution is defined via the <strong>waiting time</strong> before a success is observed, in a sequence of i.i.d. r.v.s. obeying the Bernoulli distribution. The geometric distribution is defined as <span class="math display">\[p(m)=(1-p)^mp\]</span> where <span class="math inline">\(m=0, 1, \ldots\)</span>. We obtain <span class="math inline">\(\langle m \rangle=(1-p)/p\)</span> and <span class="math inline">\(\sigma^2=(1-p)/p^2\)</span>.</p></li>
<li><p>The Poisson distribution is given as the limit of the binomial distribution as <span class="math inline">\(n \to \infty\)</span> while the mean <span class="math inline">\(np\)</span> tends to a constant <span class="math inline">\(\lambda\)</span> (therefore <span class="math inline">\(p \to 0\)</span>). The Poisson distribution is defined as <span class="math display">\[p(m)=\frac{m^\lambda e^{-\lambda}}{m!}\]</span> where <span class="math inline">\(m=0, 1, \ldots\)</span>. We obtain <span class="math inline">\(\langle m \rangle=\sigma^2=\lambda\)</span>.</p></li>
</ol>
<h3 id="continuous-variable">1.1.2. Continuous Variable</h3>
<p>Similarly, the <span class="math inline">\(n\)</span>th moment of r.v. <span class="math inline">\(X\)</span> is defined by <span class="math display">\[\langle x^n \rangle=\int_{-\infty}^\infty x^nf(x)\text{d}x.\]</span> It is often practical to use the cumulative probability <span class="math inline">\(F(x)\)</span> defined by <span class="math display">\[F(x)=\int_{-\infty}^x f(x&#39;)\text{d}x&#39;,\]</span> and the <strong>survival probability</strong> or <strong>survival function</strong> defined by <span class="math display">\[\widetilde{F}(x)=\int_x^\infty f(x&#39;)\text{d}x&#39;=1-F(x).\]</span></p>
<p>A list of frequently used continuous distribution is given:</p>
<ol type="1">
<li><p>The uniform distribution takes a constant probability on interval <span class="math inline">\([a, b]\)</span>, i.e., <span class="math display">\[f(x)=\frac{1}{b-a}\]</span> where <span class="math inline">\(a \leq x \leq b\)</span>. We obtain <span class="math inline">\(\langle x \rangle=(b-a)/2\)</span> and <span class="math inline">\(\sigma^2=(b-a)^2/12\)</span>.</p></li>
<li><p>The exponential distribution is defined by <span class="math display">\[f(x)=\lambda e^{-\lambda x}\]</span> where <span class="math inline">\(x \geq 0\)</span>. Its cumulative distribution is given by <span class="math display">\[F(x)=1-e^{-\lambda x}\]</span> where <span class="math inline">\(x \geq 0\)</span>. We obtain <span class="math inline">\(\langle x \rangle=1/\lambda\)</span> and <span class="math inline">\(\sigma^2=1/\lambda^2\)</span>.</p></li>
<li><p>The Gaussian distribution is defined by <span class="math display">\[f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)\]</span> where <span class="math inline">\(x \in \mathbb{R}\)</span>, <span class="math inline">\(\mu\)</span> is the mean, and <span class="math inline">\(\sigma^2\)</span> is the variance. The Gaussian distribution can be considered as a continuous limit of the binomial distribution. The binomial distribution with <span class="math inline">\(n\)</span> trials, each w.p. <span class="math inline">\(p\)</span>, converges to the Gaussian distribution with mean <span class="math inline">\(np\)</span> and variance <span class="math inline">\(np(1-p)\)</span> owing to the central limit theorem. As a consequence, the Gaussian distribution is frequently observed in empirical data.</p></li>
</ol>
<h2 id="poisson-process">1.2. Poisson Process</h2>
<p>A system, where events take place in a discrete and random fashion, is often modeled, as a first order approximation, by a <strong>Poisson process</strong>, a.k.a. the <strong>homogeneous Poisson process</strong>. The Poisson process assumes that the events are independent of each other, that the rate at which the events take place is constant over time, and that time is continuous. These assumptions are often violated in empirical data, but the Poisson process is advantageous in its simplicity.</p>
<p><strong>Definition 1.1</strong>    Consider a time window of duration <span class="math inline">\(\Delta t\)</span> and the probability <span class="math inline">\(q\)</span> that an event takes place within time <span class="math inline">\(\Delta t\)</span>. The <strong>event rate</strong> is given by <span class="math inline">\(\lambda=q/\Delta t\)</span>. A <strong>Poisson process</strong> is specified by the rate <span class="math inline">\(\lambda\)</span> for infinitesimally small <span class="math inline">\(\Delta t\)</span>. For <span class="math inline">\(\lambda\)</span> to be well-defined, <span class="math inline">\(q \to 0\)</span> must be satisfied as <span class="math inline">\(\Delta t \to 0\)</span>. Consistent with the requirement, multiple events are not allowed to occur in a time window when <span class="math inline">\(\Delta t\)</span> is sufficiently small.</p>
<h3 id="property">1.2.1. Property</h3>
<h4 id="distribution-of-inter-event-time">1.2.1.1. Distribution of Inter-Event Time</h4>
<p>Let <span class="math inline">\(p(n, t)\)</span> be the probability of observing <span class="math inline">\(n\)</span> events in time window <span class="math inline">\([0, t]\)</span>. By definition, <span class="math display">\[\begin{aligned}
q&amp;=p(1, \Delta t)=\lambda \Delta t, \\
1-q&amp;=p(0, \Delta t)=1-\lambda \Delta t,
\end{aligned}\]</span> when <span class="math inline">\(\Delta t\)</span> (and hence <span class="math inline">\(q\)</span>) is small. For any <span class="math inline">\(n \geq 1\)</span>, we have <span id="eq1"><span class="math display">\[\begin{align}\begin{aligned}
p(n, t+\Delta t)&amp;=p(n, t)p(0, \Delta t)+p(n-1, t)p(1, \Delta t)
\\&amp;=p(n, t)(1-\lambda \Delta t)+p(n-1, t)\lambda \Delta t.
\end{aligned}\end{align}\]</span></span></p>
<p>Eq. <a href="#eq1">(1)</a> relates the probability of a system at a certain time to that at a previous time, and hence is an example of <strong>master equation</strong>. In the limit <span class="math inline">\(\Delta t \to 0\)</span>, Eq. <a href="#eq1">(1)</a> is reduced to <span id="eq2"><span class="math display">\[\begin{align}\begin{aligned}
\frac{\text{d}p(n, t)}{\text{d}t}=\lim_{\Delta t \to 0}\frac{p(n, t+\Delta t)-p(n, t)}{\Delta t}=\lambda p(n-1, t)-\lambda p(n, t).
\end{aligned}\end{align}\]</span></span> For <span class="math inline">\(n=0\)</span>, we obtain <span class="math display">\[\frac{\text{d}p(0, t)}{\text{d}t}=-\lambda p(0, t).\]</span> Since <span class="math inline">\(p(0, 0)=1\)</span>, i.e. no event has occurred at <span class="math inline">\(t=0\)</span>, then <span id="eq3"><span class="math display">\[\begin{align}\begin{aligned}
p(0, t)=e^{-\lambda t}.
\end{aligned}\end{align}\]</span></span> Recall that the first event occurs in <span class="math inline">\([t, t+\Delta t]\)</span> is given by <span class="math inline">\(p(0, t)-p(0, t+\Delta t)\)</span>. Eq. <a href="#eq3">(3)</a> implies that the inter-event time between two consecutive events, denoted by <span class="math inline">\(\tau\)</span>, is distributed according to <span id="eq4"><span class="math display">\[\begin{align}\begin{aligned}
\psi(\tau)=-\frac{\text{d}p(0, \tau)}{d\tau}=\lambda e^{-\lambda \tau}.
\end{aligned}\end{align}\]</span></span></p>
<div class="note warning">
            <p>Not Done</p>
          </div>
<p>The inter-event time of a Poisson process is distributed according to the exponential distribution, and the mean inter-event time is given by <span class="math display">\[\langle \tau \rangle=\int_0^\infty \tau\psi(\tau)\text{d}\tau=\frac{1}{\lambda}.\]</span></p>
<p>In Poisson process, different inter-event times <span class="math inline">\(\tau\)</span> are independent of each other because event times before the last event time <span class="math inline">\(t\)</span> do not affect the time <span class="math inline">\(\tau\)</span> to the next event since <span class="math inline">\(t\)</span>, which is known as the <strong>renewal property</strong> of a Poisson process. Poisson process satisfies a stronger property, i.e., <strong>memoryless property</strong> <span id="eq5"><span class="math display">\[\begin{align}\begin{aligned}
p(\tau&gt;t_1+t_2 \mid \tau&gt;t_2)=p(\tau&gt;t_1).
\end{aligned}\end{align}\]</span></span> Eq. <a href="#eq5">(5)</a> indicates that the length of time <span class="math inline">\(t_2\)</span>, for which we have waited, actually without an event, does not affect the time of the next event. The time to the next event starting from <span class="math inline">\(t=t_2\)</span>, <span class="math inline">\(t_1\)</span>, is independent of <span class="math inline">\(t_2\)</span> and obeys <span class="math inline">\(\psi(t_1)\)</span>.</p>
<h4 id="distribution-of-the-number-of-events-observed-within-given-time-window">1.2.1.2. Distribution of the Number of Events Observed within Given Time Window</h4>
<p>Using Eq. <a href="#eq2">(2)</a> recursively, we obtain <span id="eq6"><span class="math display">\[\begin{align}\begin{aligned}
p(n, t)=\frac{(\lambda t)^n}{n!}e^{-\lambda t}
\end{aligned}\end{align}\]</span></span> for any <span class="math inline">\(n \geq 0\)</span>. Therefore, the probability of observing <span class="math inline">\(n\)</span> events in <span class="math inline">\([0, t]\)</span> obeys the Poisson distribution with mean and variance <span class="math inline">\(\lambda t\)</span>.</p>
<h3 id="poisson-process-generation">1.2.2. Poisson Process Generation</h3>
<p>A method to generate an event sequence obeying a Poisson process is to generate events one by one by independently drawing the inter-event time <span class="math inline">\(\tau\)</span> according to Eq. <a href="#eq4">(4)</a>.</p>
<p>An alternative method, exploiting the memoryless property of Poisson process, is to first draw the number of events <span class="math inline">\(n\)</span> in <span class="math inline">\([0, t_\text{max}]\)</span> according to the Poisson distribution with parameter <span class="math inline">\(\lambda t_\text{max}\)</span> (suppose the final time <span class="math inline">\(t_\text{max}\)</span> is specified), and then to distribute each of the <span class="math inline">\(n\)</span> events independently and uniformly on <span class="math inline">\([0, t_\text{max}]\)</span>.</p>
<h3 id="extension-of-poisson-process">1.2.3. Extension of Poisson Process</h3>
<h4 id="non-homogeneous-poisson-process">1.2.3.1. Non-Homogeneous Poisson Process</h4>
<p>For <strong>non-homogeneous Poisson process</strong>, the event rate <span class="math inline">\(\lambda(t)\)</span> is time-dependent, i.e., an event occurs in <span class="math inline">\([t, t+\Delta t]\)</span> w.p. <span class="math inline">\(\lambda(t)\Delta t\)</span>. For a non-homogeneous Poisson process, Eq. <a href="#eq6">(6)</a> is extended as <span class="math display">\[p(n, t)=\frac{\Lambda(t)^n}{n!}e^{-\Lambda(t)}\]</span> where <span class="math inline">\(\displaystyle \Lambda(t)=\int_0^t \lambda(t&#39;)\text{d}t&#39;\)</span>. The distribution of inter-event times, conditioned by the last event at <span class="math inline">\(t=0\)</span>, is given by <span class="math display">\[\psi(\tau)=\lambda(\tau)e^{-\Lambda(\tau)},\]</span> which extends Eq. <a href="#eq4">(4)</a> and is properly normalized, i.e., <span class="math inline">\(\displaystyle \int_0^\infty \psi(\tau)\text{d}\tau=1\)</span>.</p>
<h4 id="renewal-process">1.2.3.2. Renewal Process</h4>
<p><strong>Renewal process</strong> considers a general distribution of inter-event times <span class="math inline">\(\psi(\tau)\)</span>. When <span class="math inline">\(\psi(\tau)=\delta(\tau-1)\)</span>, events periodically happen at all integer times. To obtain the time of the <span class="math inline">\(n\)</span>th event or the number of events in a given time period, we sum independent r.v.s. generated according to <span class="math inline">\(\psi(\tau)\)</span>.</p>
<h2 id="random-walk-and-diffusion">1.3. Random Walk and Diffusion</h2>
<p>The Poisson process provides a basic model for modeling temporal events, while <strong>random walk process</strong> is for modeling trajectories in space, i.e., when and where random event takes place. Random walk process is a standard tool to emulate diffusion on network and to extract information from the structure of network.</p>
<p>In each discrete time step, a walker performs a jump whose length and direction are r.v.s.. The probability density of transition is denoted by <span class="math inline">\(f(r)\)</span>, i.e., the probability that the walker located at <span class="math inline">\(x\)</span> arrives in the interval <span class="math inline">\([x+r, x+r+\Delta r]\)</span> in one jump is equal to <span class="math inline">\(f(r)\Delta r\)</span>. Under the assumption that jumps are independent events, we obtain the following master equation, where <span class="math inline">\(p(x; t)\)</span> is the probability density that the walker is located at <span class="math inline">\(x\)</span> after <span class="math inline">\(t\)</span> steps, <span id="eq7"><span class="math display">\[\begin{align}\begin{aligned}
p(x; t)=\int_{-\infty}^\infty f(x-x&#39;)p(x&#39;; t-1)\text{d}x&#39;
\end{aligned}\end{align}\]</span></span> because the probability of visiting <span class="math inline">\(x\)</span> at time <span class="math inline">\(t\)</span> is the probability of having visited <span class="math inline">\(x&#39;\)</span> at time <span class="math inline">\(t-1\)</span> and performing a jump of displacement <span class="math inline">\(x-x&#39;\)</span>. Eq. <a href="#eq7">(7)</a> for the entire range of <span class="math inline">\(x\)</span> is more easily solved in the Fourier domain, where the <strong>Fourier transform</strong> is defined by <span class="math display">\[\widehat{p}(k; t)=\int_{-\infty}^\infty p(x; t)e^{-ikx}\text{d}x.\]</span> The original function is recovered through the <strong>inverse Fourier transform</strong> given by <span class="math display">\[p(x; t)=\frac{1}{2\pi}\int_{-\infty}^\infty \widehat{p}(k; t)e^{ikx}\text{d}k.\]</span> Therefore, <span class="math inline">\(p(x; t)\)</span> is a combination of the <strong>oscillatory functions</strong> <span class="math inline">\(e^{ikx}\)</span>, which form a base in the space of functions. The Fourier mode <span class="math inline">\(\widehat{p}(k; t)\)</span> is the projection of <span class="math inline">\(p(x; t)\)</span> onto the base. The Fourier transform of <span class="math inline">\(f(x)\)</span>, <span class="math inline">\(\widehat{f}(k)\)</span>, is called the <strong>structure function of the random walk</strong>. The Taylor expansion around <span class="math inline">\(k=0\)</span> yields <span class="math display">\[\widehat{p}(k; t)=\langle e^{-ikx} \rangle=1-ik\langle x \rangle-\frac{1}{2}k^2\langle x^2 \rangle+\mathcal{O}(k^3),\]</span> which implies that the moments of <span class="math inline">\(p(x; t)\)</span> are obtained from the derivatives of <span class="math inline">\(\widehat{p}(k; t)\)</span> at <span class="math inline">\(k=0\)</span>.</p>
<p>The Fourier transform transfers a convolution to a product, and thus working in the Fourier domain is often recommended when dealing with problems involving summations of r.v.s.. Eq. <a href="#eq7">(7)</a> is equivalent to <span class="math display">\[\widehat{p}(k; t)=\widehat{f}(k)\widehat{p}(k; t-1).\]</span> If the walker is initially located at <span class="math inline">\(x=0\)</span> s.t. <span class="math inline">\(p(x; 0)=\delta(x)\)</span>, which translates to <span class="math inline">\(\wideaht{k; 0)=1\)</span>, we obtain <span class="math display">\[\widehat{p}(k; t)=[\widehat{f}(k)]^t.\]</span> Using the inverse Fourier transform, the formal solution of the random walk in the time domain is given by <span class="math display">\[p(x; t)=\frac{1}{2\pi}\int_{-\infty}^\infty [\widehat{f}(k)]^te^{ikx}\text{d}k.\]</span> When the first two moments of the structure function <span class="math inline">\(\widehat{f}(k)\)</span> are finite, the solution converges to the Gaussian profile <span class="math display">\[p(x; t)=\frac{1}{(2\pi Dt)^{1/2}}\exp\left(-\frac{(x-vt)^2}{4Dt}\right)\]</span> with a variance growing linearly with time.</p>
<div class="note warning">
            <p>To be continued.</p>
          </div>
<h2 id="power-law-distribution">1.4. Power-Law Distribution</h2>
<h2 id="maximum-likelihood">1.5. Maximum Likelihood</h2>
<h2 id="entropy-information-and-similarity-measure">1.6. Entropy, Information and Similarity Measure</h2>
<h2 id="matrix-algebra">1.7. Matrix Algebra</h2>
<h2 id="markov-chain">1.8. Markov Chain</h2>
<h2 id="branching-process">1.9. Branching Process</h2>
<h1 id="definition">2. Definition</h1>
<p><strong>Definition 1.1</strong>    We abbreviate a <strong>graph</strong> <span class="math inline">\(\mathcal{G}\)</span> as <span class="math inline">\(\mathcal{G}=(\mathcal{V}, \mathcal{E})\)</span>, where <span class="math inline">\(\mathcal{V}\)</span> is the set of <strong>vertices (nodes)</strong> and <span class="math inline">\(\mathcal{E}\)</span> is the set of <strong>edges (links)</strong>. Let <span class="math inline">\(|\mathcal{V}|\)</span> be the number of vertices, and <span class="math inline">\(|\mathcal{E}|\)</span> be the number of edges in the graph <span class="math inline">\(\mathcal{G}\)</span>. If <span class="math inline">\(u\)</span> and <span class="math inline">\(b\)</span> are two vertices and there is an edge from <span class="math inline">\(u\)</span> to <span class="math inline">\(v\)</span>, then we say <span class="math inline">\(v\)</span> is a <strong>neighbor</strong> of <span class="math inline">\(u\)</span>, denoted <span class="math inline">\((u, v) \in \mathcal{E}\)</span>.</p>
<p><strong>Definition 1.2</strong>    A <strong>directed graph</strong> or <strong>digraph</strong> is a graph where <em>all</em> edges are directed. The <strong>underlying</strong> graph of a digraph is the graph that results from turning all directed edges into <strong>undirected</strong> edges.</p>
<p><strong>Definition 1.3</strong>    If both endpoints of an edge are the same, then the edge is a <strong>loop</strong>.</p>
<p><strong>Example 1.1</strong>    The graph below is a loop.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 1
          </pre>
<p><strong>Definition 1.4</strong>    Two vertices are called <strong>adjacent</strong> if they are joined by an edge. A graph can be described by its <span class="math inline">\(|\mathcal{V}| \times |\mathcal{V}|\)</span> <strong>adjacency matrix</strong> <span class="math inline">\(A=(a_{u, v})\)</span>, where <span class="math inline">\(a_{u, v}=1\)</span> iff <span class="math inline">\((u, v) \in \mathcal{E}\)</span>.</p>
<div class="note info">
            <p>If there are no self-loops, all elements on the diagonal of the adjacency matrix are <span class="math inline">\(0\)</span>. If the edges of the graph are undirected, then the adjacency matrix will be symmetric.</p><p>The adjacency matrix entries tell us for every vertex <span class="math inline">\(v\)</span> which vertices are within (graph) distance <span class="math inline">\(1\)</span> of <span class="math inline">\(v\)</span>.</p><p>If we take the matrix product <span class="math inline">\(A^2=AA\)</span>, the entry for <span class="math inline">\((u, v)\)</span> with <span class="math inline">\(u \neq v\)</span> would be <span class="math display">\[a^{(2)}(u, v)=\sum_{w \in \mathcal{V}} a_{u, w}a_{w, v}.\]</span> If <span class="math inline">\(a^{(2)}(u, v) \neq 0\)</span>, then <span class="math inline">\(u\)</span> can be reached from <span class="math inline">\(v\)</span> within two steps, i.e., <span class="math inline">\(u\)</span> is within distance <span class="math inline">\(2\)</span> of <span class="math inline">\(v\)</span>. Higher powers can be interpreted similarly.</p>
          </div>
<p><strong>Definition 1.5</strong>    A <strong>complete</strong> graph is a graph, without self-loops, s.t. every pair of vertices is joined by an edge. The adjacency matrix has entry <span class="math inline">\(0\)</span> on the diagonal, and <span class="math inline">\(1\)</span> everywhere else.</p>
<p><strong>Example 1.2</strong>    The graph below is complete.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 2((2)) --- 3((3))
1 --- 3
          </pre>
<p><strong>Definition 1.6</strong>    A <strong>bipartite</strong> graph is a graph where the vertex set <span class="math inline">\(\mathcal{V}\)</span> is decomposed into two disjoint subsets, <span class="math inline">\(\mathcal{U}\)</span> and <span class="math inline">\(\mathcal{W}\)</span>, s.t. there are no edges between any two vertices in <span class="math inline">\(\mathcal{U}\)</span>, and there are no edges between any two vertices in <span class="math inline">\(\mathcal{W}\)</span>; all edges have one endpoint in <span class="math inline">\(\mathcal{U}\)</span> and the other endpoint in <span class="math inline">\(\mathcal{W}\)</span>. The adjacency matrix <span class="math inline">\(A\)</span> can then be arranged s.t. <span class="math display">\[A=\begin{bmatrix}
0 &amp; A_1 \\
A_2 &amp; 0
\end{bmatrix}.\]</span></p>
<p><strong>Example 1.3</strong>    The graph below is bipartite.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 3((3)) & 4((4))
2((2)) --- 3((3)) & 4((4))
subgraph U
1((1))
2((2))
end
subgraph W
3((3))
4((4))
end
          </pre>
<h1 id="network-summary">3. Network Summary</h1>
<h2 id="density-degree-and-degree-distribution">3.1. Density, Degree and Degree Distribution</h2>
<p><strong>Definition 2.1</strong>    The <strong>density</strong> of a network with <span class="math inline">\(e\)</span>edges and <span class="math inline">\(n\)</span> vertices is <span class="math display">\[\frac{e}{\displaystyle \binom{n}{2}}=\frac{2e}{n(n-1)}\]</span> where <span class="math inline">\(\displaystyle \binom{n}{2}\)</span> is the potential number of edges.</p>
<p><strong>Definition 2.2</strong>    The <strong>degree</strong> <span class="math inline">\(d(v)\)</span> of a vertex <span class="math inline">\(v\)</span> is the number of edges which involve <span class="math inline">\(v\)</span> as an endpoint. The degree can be calculated from the adjacency matrix <span class="math inline">\(A\)</span> <span class="math display">\[d(v)=\sum_{u \in \mathcal{V}} a_{u, v}.\]</span></p>
<p><strong>Definition 2.3</strong>    The <strong>average degree</strong> of a graph is the average of its vertex degrees <span class="math display">\[\overline{d}=\frac{1}{n}\sum_{v \in \mathcal{V}} d(v).\]</span></p>
<p><strong>Definition 2.4</strong>    The <strong>degree sequence</strong> of a given network on a vertex set <span class="math inline">\(\mathcal{V}\)</span> with <span class="math inline">\(n\)</span> elements is the unordered <span class="math inline">\(n\)</span>-element set of degrees <span class="math inline">\(\{d(v), v \in \mathcal{V}\}\)</span>.</p>
<p><strong>Definition 2.5</strong>    The <strong>degree distribution</strong> <span class="math inline">\((d_0, d_1, \ldots)\)</span> of a graph on <span class="math inline">\(n\)</span> vertices is the vector of fraction of vertices with given degree <span class="math display">\[d_k=\frac{1}{n} \times \text{Number of vertices of degree}\ k.\]</span></p>
<div class="note info">
            <p>For directed graphs, we define the <strong>in-degree</strong> as the number of edges directed at the vertex, and the <strong>out-degree</strong> as the number of edges taht go out from that vertex.</p>
          </div>
<h2 id="local-clustering-coefficient">3.2. Local Clustering Coefficient</h2>
<p><strong>Definition 2.6</strong>    The <strong>local clustering coefficient</strong> of a vertex <span class="math inline">\(v\)</span> is the proportion of neighbors of <span class="math inline">\(v\)</span> which are neighbors themselves. In adjacency matrix notation <span class="math display">\[C(v)=\frac{\displaystyle \sum_{u, w \in \mathcal{V}} a_{u, v}a_{w, v}a_{u,w}}{\displaystyle \sum_{u \neq w \in \mathcal{V}} a_{u, v}a_{w, v}},\]</span> where <span class="math inline">\(\displaystyle \frac{0}{0}:=0\)</span>.</p>
<div class="note info">
            <p>To compute <span class="math inline">\(C(v)\)</span> more efficiently, we should note that <span class="math inline">\(\displaystyle \frac{1}{2}\sum_{\substack{u \neq v \in \mathcal{V} \\ w \neq u, v \in \mathcal{V}}} a_{u, v}a_{w, v}a_{u, w}\)</span> is the number of <strong>triangles</strong> involving <span class="math inline">\(v\)</span> in the graph, and <span class="math inline">\(\displaystyle \frac{1}{2}\sum_{u \neq w \in \mathcal{V}} a_{u, v}a_{w, v}\)</span> is the number of <strong><span class="math inline">\(2\)</span>-stars</strong> centered around <span class="math inline">\(v\)</span> in the graph.</p>
          </div>
<p><strong>Definition 2.7</strong>    The <strong>average clustering coefficient</strong> is defined as <span class="math display">\[\overline{C}=\frac{1}{|\mathcal{V}|}\sum_{v \in \mathcal{V}} C(v).\]</span></p>
<div class="note info">
            <p>The local clustering coefficient describes how <em>locally dense</em> a graph is.</p>
          </div>
<h2 id="global-clustering-coefficient">3.3. Global Clustering Coefficient</h2>
<p><strong>Definition 2.8</strong>    The <strong>global clustering coefficient</strong> or <strong>transitivity</strong> is defined as <span class="math display">\[C=\frac{3 \times \text{Number of triangles}}{\text{Number of connected triples}}=\frac{6 \times \text{Number of triangles}}{\text{Number of paths of length two}}\]</span></p>
<div class="note warning">
            <p><span class="math inline">\(\overline{C} \neq C\)</span> in general. Indeed <span class="math inline">\(\overline{C}\)</span> tends to be dominated by vertices with low degree, since they tend to have small denominators in the local clustering coefficient.</p>
          </div>
<h2 id="expected-clustering-coefficient">3.4. Expected Clustering Coefficient</h2>
<p><strong>Definition 2.9</strong>    For models of random networks, we consider <strong>expected clustering coefficient</strong> <span class="math display">\[E(C)=\frac{3\mathbb{E}[\text{Number of triangles}]}{\mathbb{E}[\text{Number of connected triples}]}\]</span></p>
<h2 id="average-shortest-path">3.5. Average Shortest Path</h2>
<p><strong>Definition 2.10</strong>    In a graph, a <strong>path</strong> from vertex <span class="math inline">\(v_0\)</span> to vertex <span class="math inline">\(v_n\)</span> is an alternating sequence of vertices and edges <span class="math inline">\((v_0, e_1, v_1, e_2, \ldots, v_{n-1}, e_n, v_n)\)</span> s.t. the endpoints of <span class="math inline">\(e_i\)</span> are <span class="math inline">\(v_{i-1}\)</span> and <span class="math inline">\(v_i\)</span> for <span class="math inline">\(i=1, \ldots, n\)</span>. The <strong>distance</strong> <span class="math inline">\(\mathscr{l}(u, v)\)</span> between two vertices <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> is the length of a <strong>shortest</strong> path (not necessarily unique) joining them.</p>
<div class="note info">
            <p>We can calculate <span class="math inline">\(\mathscr{l}(u, v)\)</span> from the adjacency matrix <span class="math inline">\(A\)</span> as the smallest power <span class="math inline">\(p\)</span> of <span class="math inline">\(A\)</span> s.t. <span class="math inline">\(A^{(p)}(u, v) \neq 0\)</span>.</p>
          </div>
<p><strong>Definition 2.11</strong>    A graph is called <strong>connected</strong> if there is a path between any pair of vertices in the graph, otherwise it is <strong>disconnected</strong>. In a connected graph, the <strong>average shortest path length</strong> is defined as <span class="math display">\[\mathscr{l}=\frac{1}{|\mathcal{V}|(|\mathcal{V}|-1)}\sum_{u \neq v \in \mathcal{V}} \mathscr{l}(u, v),\]</span> where <span class="math inline">\(|\mathcal{V}|(|\mathcal{V}|-1)\)</span> means the possible number of shortest path length.</p>
<div class="note info">
            <p>The average shortest path length describes how <em>globally connected</em> a graph is.</p>
          </div>
<p><strong>Example 2.1</strong>    Consider a graph.</p>
<pre class="mermaid" style="text-align: center;">
            graph LR
            1((1)) --- 5((5)) --- 3((3)) --- 2((2))
5((5)) --- 2((2))
5((5)) --- 4((4)) --- 2((2))
          </pre>
<p>We know <span class="math inline">\(\mathcal{V}=\{1, 2, 3, 4, 5\}\)</span>, <span class="math inline">\(\mathcal{E}=\{(1, 5), (2, 3), (2, 4), (2, 5), (3, 5), (4, 5)\}\)</span>, <span class="math inline">\(|\mathcal{V}|=5\)</span>, and <span class="math inline">\(|\mathcal{E}|=6\)</span>.</p>
<p>The adjacency matrix is <span class="math display">\[\begin{bmatrix}
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 0
\end{bmatrix}.\]</span></p>
<p>The density is <span class="math inline">\(\displaystyle \frac{2e}{n(n-1)}=0.6\)</span>.</p>
<p>The degrees of vertices are <span class="math inline">\(d(1)=1\)</span>, <span class="math inline">\(d(2)=3\)</span>, <span class="math inline">\(d(3)=2\)</span>, <span class="math inline">\(d(4)=2\)</span> and <span class="math inline">\(d(5)=4\)</span>. The average degree is <span class="math inline">\(\overline{d}=2.4\)</span>. The degree sequence is <span class="math inline">\(\{1, 3, 2, 2, 4\}\)</span>. The degree distribution is <span class="math inline">\(d_0=0\)</span>, <span class="math inline">\(d_1=0.2\)</span>, <span class="math inline">\(d_2=0.4\)</span>, <span class="math inline">\(d_3=0.2\)</span> and <span class="math inline">\(d_4=0.2\)</span>.</p>
<p>The local clustering coefficients of vertices are <span class="math inline">\(C(1)=0\)</span>, <span class="math inline">\(\displaystyle C(2)=\frac{2}{3}\)</span>, <span class="math inline">\(C(3)=1\)</span>, <span class="math inline">\(C(4)=1\)</span> and <span class="math inline">\(\displaystyle C(5)=\frac{1}{3}\)</span>. The average clustering coefficient is <span class="math inline">\(\overline{C}=0.6\)</span>.</p>
<p>The global clustering coefficient is <span class="math inline">\(\displaystyle C=\frac{6}{11}\)</span>.</p>
<p>The distance matrix is <span class="math display">\[\begin{bmatrix}
0 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\
2 &amp; 0 &amp; 1 &amp; 1 &amp; 1 \\
2 &amp; 1 &amp; 0 &amp; 2 &amp; 1 \\
2 &amp; 1 &amp; 2 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 1 &amp; 1 &amp; 0
\end{bmatrix}.\]</span> The graph is connected, and the average shortest path length is <span class="math inline">\(\mathscr{l}=1.4\)</span>.</p>
<h2 id="small-graph-and-motif">3.6. Small Graph and Motif</h2>
<p><strong>Definition 2.12</strong>    <strong>Small subgraph</strong> can be viewed as building-block pattern of networks. By small we mean graph on a small number (three to five) of vertices. Often a small graph is called a <strong>motif</strong> when it is <strong>over-represented</strong> in the network, where over-representation is judged using a probabilistic model for the network.</p>
<div class="note info">
            <p>We think of a motif as a small graph with a fixed number of vertices and with a given topology, and we use the term interchangeably with small graph.</p>
          </div>
<h2 id="spectral-summary">3.7. Spectral Summary</h2>
<p><strong>Definition 2.13</strong>    We call a graph <strong>simple</strong> if it has no self-loops or multiple edges.</p>
<p><strong>Definition 2.14</strong>     The <strong>eigenvector centrality</strong> <strong><em>(Not done. To be continued...)</em></strong></p>
<p><strong>Definition 2.15</strong>     Let <span class="math inline">\(D\)</span> be the diagonal matrix with diagonal entries <span class="math inline">\(D_{ii}\)</span>, the degree of vertex <span class="math inline">\(i\)</span>. The <strong>graph Laplacian</strong> is <span class="math inline">\(L=D-A\)</span>, where <span class="math display">\[L(i, j)=\begin{cases}
d(i), &amp;i=j \\
-1, &amp;i \neq j\ \text{and}\ (i, j) \in \mathcal{E} \\
0, &amp;\text{otherwise}
\end{cases}.\]</span> The second smallest (smallest non-zero) eigenvalue of <span class="math inline">\(L\)</span> is called <strong>algebraic connectivity</strong> or <strong>Fiedler value</strong> or <strong>spectral gap</strong> of <span class="math inline">\(\mathcal{G}\)</span>.</p>
<div class="note info">
            <p><span class="math inline">\(L\)</span> is symmetric, with real eigenvalues <span class="math inline">\(\lambda_0 \leq \cdots \leq \lambda_{n-1}\)</span> and eigenvectors <span class="math inline">\(\phi_1, \ldots, \phi_{n-1}\)</span>.</p><p>Since every row sum and column sum of <span class="math inline">\(L\)</span> is zero, then <span class="math inline">\(L\mathbf{1}=0\mathbf{1}\)</span>, and thus <span class="math inline">\(\lambda_0=0\)</span> and <span class="math inline">\(\phi_0=\mathbf{1}\)</span>.</p>
          </div>
<h2 id="other-summary">3.8. Other Summary</h2>
<p>Specific network may require specific summary. For example, when there is a spatial structure in the network, then geometrical consideration may enter.</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Network/" rel="tag"><i class="fa fa-tag"></i> Network</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Statistics/Graphical_Model/Conditional_Independence" rel="prev" title="Conditional Independence">
      <i class="fa fa-chevron-left"></i> Conditional Independence
    </a></div>
      <div class="post-nav-item">
    <a href="/Statistics/Graphical_Model/Exponential_Family_and_Contingency_Table" rel="next" title="Exponential Family and Contingency Table">
      Exponential Family and Contingency Table <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#mathematical-preliminary"><span class="nav-text">1. Mathematical Preliminary</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#probability"><span class="nav-text">1.1. Probability</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#discrete-variable"><span class="nav-text">1.1.1. Discrete Variable</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#continuous-variable"><span class="nav-text">1.1.2. Continuous Variable</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#poisson-process"><span class="nav-text">1.2. Poisson Process</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#property"><span class="nav-text">1.2.1. Property</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#distribution-of-inter-event-time"><span class="nav-text">1.2.1.1. Distribution of Inter-Event Time</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#distribution-of-the-number-of-events-observed-within-given-time-window"><span class="nav-text">1.2.1.2. Distribution of the Number of Events Observed within Given Time Window</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#poisson-process-generation"><span class="nav-text">1.2.2. Poisson Process Generation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#extension-of-poisson-process"><span class="nav-text">1.2.3. Extension of Poisson Process</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#non-homogeneous-poisson-process"><span class="nav-text">1.2.3.1. Non-Homogeneous Poisson Process</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#renewal-process"><span class="nav-text">1.2.3.2. Renewal Process</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-walk-and-diffusion"><span class="nav-text">1.3. Random Walk and Diffusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#power-law-distribution"><span class="nav-text">1.4. Power-Law Distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#maximum-likelihood"><span class="nav-text">1.5. Maximum Likelihood</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#entropy-information-and-similarity-measure"><span class="nav-text">1.6. Entropy, Information and Similarity Measure</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#matrix-algebra"><span class="nav-text">1.7. Matrix Algebra</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#markov-chain"><span class="nav-text">1.8. Markov Chain</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#branching-process"><span class="nav-text">1.9. Branching Process</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#definition"><span class="nav-text">2. Definition</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#network-summary"><span class="nav-text">3. Network Summary</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#density-degree-and-degree-distribution"><span class="nav-text">3.1. Density, Degree and Degree Distribution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#local-clustering-coefficient"><span class="nav-text">3.2. Local Clustering Coefficient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#global-clustering-coefficient"><span class="nav-text">3.3. Global Clustering Coefficient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#expected-clustering-coefficient"><span class="nav-text">3.4. Expected Clustering Coefficient</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#average-shortest-path"><span class="nav-text">3.5. Average Shortest Path</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#small-graph-and-motif"><span class="nav-text">3.6. Small Graph and Motif</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#spectral-summary"><span class="nav-text">3.7. Spectral Summary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#other-summary"><span class="nav-text">3.8. Other Summary</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
