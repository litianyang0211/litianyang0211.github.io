<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1. Least Squares  Four ways to solve least squares:           1. The SVD of \(A\) leads to its pseudo-inverse \(A^+\), then \(\mathbf{x}^+&#x3D;A^+\mathbf{b}\).          2. When \(A\) has independent colum">
<meta property="og:type" content="article">
<meta property="og:title" content="Computation With Large Matrix">
<meta property="og:url" content="https://litianyang0211.github.io/Mathematics/Matrix-method/Computation_with_Large_Matrix">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:description" content="1. Least Squares  Four ways to solve least squares:           1. The SVD of \(A\) leads to its pseudo-inverse \(A^+\), then \(\mathbf{x}^+&#x3D;A^+\mathbf{b}\).          2. When \(A\) has independent colum">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-09-27T23:00:00.000Z">
<meta property="article:modified_time" content="2021-11-16T10:10:52.792Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="Matrix">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://litianyang0211.github.io/Mathematics/Matrix-method/Computation_with_Large_Matrix.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Computation With Large Matrix | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Mathematics/Matrix-method/Computation_with_Large_Matrix">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Computation With Large Matrix
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-09-28 00:00:00" itemprop="dateCreated datePublished" datetime="2021-09-28T00:00:00+01:00">2021-09-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-11-16 10:10:52" itemprop="dateModified" datetime="2021-11-16T10:10:52+00:00">2021-11-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Mathematics/" itemprop="url" rel="index"><span itemprop="name">Mathematics</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Mathematics/Matrix-method/" itemprop="url" rel="index"><span itemprop="name">Matrix method</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="least-squares">1. Least Squares</h1>
<ul>
<li>Four ways to solve least squares:</li>
</ul>
<p>         1. The SVD of <span class="math inline">\(A\)</span> leads to its <strong>pseudo-inverse</strong> <span class="math inline">\(A^+\)</span>, then <span class="math inline">\(\mathbf{x}^+=A^+\mathbf{b}\)</span>.</p>
<p>         2. When <span class="math inline">\(A\)</span> has independent columns, solving <span class="math inline">\(A^TA\widehat{\mathbf{x}}=A^T\mathbf{b}\)</span> to minimize <span class="math inline">\(\|A\mathbf{x}-\mathbf{b}\|^2\)</span>.</p>
<p>         3. When <span class="math inline">\(A\)</span> has independent columns, Gram-Schmidt <span class="math inline">\(A=QR\)</span> leads to <span class="math inline">\(\widehat{\mathbf{x}}=R^{-1}Q^T\mathbf{b}\)</span>.</p>
<p>         4. Minimize <span class="math inline">\(\|A\mathbf{x}-\mathbf{B}\|^2+\delta^2\|\mathbf{x}\|^2\)</span>, and the best <span class="math inline">\(\mathbf{x}\)</span> is the limit of <span class="math inline">\((A^TA+\delta I)^{-1}A^T\mathbf{b}\)</span> as <span class="math inline">\(\delta \to 0\)</span>.</p>
<h2 id="pseudo-inverse-method">1.1. Pseudo-Inverse Method</h2>
<ul>
<li><p>If <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(A^+=A^{-1}\)</span>.</p></li>
<li><p>The pseudo-inverse of <span class="math inline">\(A=U\Sigma V^T\)</span> is <span class="math inline">\(A^+=V\Sigma^+U^T\)</span>.</p></li>
<li><p>Sometimes we denote <span class="math inline">\(A^+\)</span> to be <span class="math inline">\(A^\dagger\)</span>.</p></li>
</ul>
<p><strong>Example 1.1</strong>    Consider <span class="math inline">\(\Sigma=\begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>, then <span class="math inline">\(\Sigma^+=\begin{bmatrix} 1/\sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; 1/\sigma_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\)</span>.</p>
<ul>
<li><span class="math inline">\(\mathbf{x}^+=A^+\mathbf{b}\)</span> is the <strong>minimum norm least squares</strong> solution.</li>
</ul>
<p><strong>Example 1.2</strong>    Consider <span class="math inline">\(\begin{bmatrix} 3 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}=\begin{bmatrix} 6 \\ 8 \end{bmatrix}\)</span>, then <span class="math inline">\(x^+=A^+\mathbf{b}=\begin{bmatrix} 1/3 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}\begin{bmatrix} 6 \\ 8 \end{bmatrix}=\begin{bmatrix} 2 \\ 0 \end{bmatrix}\)</span>.</p>
<h2 id="normal-equation-method">1.2. Normal Equation Method</h2>
<ul>
<li><p>Suppose <span class="math inline">\(A\)</span> has independent columns, then <span class="math inline">\(\widehat{\mathbf{x}}=(A^TA)^{-1}A^T\mathbf{b}\)</span>.</p></li>
<li><p><strong>Projection</strong> of <span class="math inline">\(\mathbf{b}\)</span> onto the column space of <span class="math inline">\(A\)</span> is <span class="math inline">\(\mathbf{p}=A\widehat{\mathbf{x}}=A(A^TA)^{-1}A^T\mathbf{b}\)</span>.</p></li>
</ul>
<h2 id="gram-schmidt-method">1.3. Gram-Schmidt Method</h2>
<div class="note info">
            <p>Suppose <span class="math inline">\(A\)</span> has independent columns <span class="math inline">\(\mathbf{a}_1, \ldots, \mathbf{a}_n\)</span>.</p><p>Let <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{a}_1}{\|\mathbf{a}_1\|}\)</span>, <span class="math inline">\(\displaystyle A_k=\mathbf{a}_k-\sum_{i=1}^{k-1} (\mathbf{a}_k^T\mathbf{q}_i)\mathbf{q}_i\)</span>, and <span class="math inline">\(\displaystyle \mathbf{q}_k=\frac{A_k}{\|A_k\|}\)</span>. Then each <span class="math inline">\(\mathbf{a}_k\)</span> is a combination of <span class="math inline">\(\mathbf{q}_1\)</span> to <span class="math inline">\(\mathbf{q}_k\)</span>: <span class="math inline">\(\mathbf{a}_1=\|\mathbf{a}_1\|\mathbf{q}_1\)</span> and <span class="math inline">\(\displaystyle \mathbf{a}_k=\sum_{i=1}^{k-1} (\mathbf{a}_k^T\mathbf{q}_i)\mathbf{q}_i+\|A_k\|\mathbf{q}_k\)</span>.</p><p>Hence, the matrix <span class="math inline">\(R=Q^TA\)</span> with <span class="math inline">\(r_{ij}=\mathbf{q}_i^T\mathbf{a}_j\)</span> is upper triangular, i.e., <span class="math display">\[\begin{bmatrix}&amp; \\ \mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_n \\ &amp; \end{bmatrix}=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \cdots &amp; \mathbf{q}_n \\ &amp;\end{bmatrix}\begin{bmatrix}r_{11} &amp; \cdots &amp; r_{1n} \\ 0 &amp; \ddots &amp; \vdots \\0 &amp; 0 &amp; r_{nn}\end{bmatrix}\]</span> is <span class="math inline">\(A=QR\)</span>.</p><p>The MATLAB command is <code>[Q, R]=qr(A)</code>.</p>
          </div>
<ul>
<li><p>Suppose <span class="math inline">\(A\)</span> has independent columns, then by Gram-Schmidt, we have <span class="math inline">\(A^TA=R^TQ^TQR=R^TR\)</span>, and thus <span class="math inline">\(\widehat{\mathbf{x}}=(R^TR)^{-1}R^TQ^T\mathbf{b}=R^{-1}Q^T\mathbf{b}\)</span>.</p></li>
<li><p>We can modify Gram-Schmidt with <strong>column pivoting</strong>.</p></li>
</ul>
<h2 id="penalty-method">1.4. Penalty Method</h2>
<p>Penalty method regularizes a singular problem.</p>
<ul>
<li><p>The new normal equation is <span class="math inline">\((A^*)^TA^*\widehat{\mathbf{x}}=(A^*)^T\mathbf{b}^*\)</span>, where <span class="math inline">\(A^*=\begin{bmatrix} A \\ \delta I \end{bmatrix}\)</span> and <span class="math inline">\(b^*=\begin{bmatrix} \mathbf{b} \\ \mathbf{0} \end{bmatrix}\)</span>, i.e., <span class="math display">\[(A^TA+\delta^2I)\widehat{\mathbf{x}}=A^T\mathbf{b}.\]</span></p></li>
<li><p>For any <span class="math inline">\(A\)</span>, <span class="math inline">\((A^TA+\delta^2I)^{-1}A^T \to A^+\)</span> as <span class="math inline">\(\delta \to 0\)</span>.</p></li>
</ul>
<p><strong><em>Proof.</em></strong> Suppose <span class="math inline">\(A=U\Sigma V^T\)</span>, then we have <span class="math display">\[A^TA+\delta^2I=V\Sigma^TU^TU\Sigma V^T+\delta^2I=V\Sigma^T\Sigma V^T+\sigma^2I=V(\Sigma^T\Sigma+\delta^2I)V^T\]</span> Therefore, <span class="math display">\[(A^TA+\delta^2I)^{-1}A^T=V(\Sigma^T\Sigma+\delta^2I)^{-1}V^TV\Sigma^TU^T=V[(\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T]U^T.\]</span></p>
Since <span class="math inline">\((\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T\)</span> has positive diagonal entries <span class="math inline">\(\displaystyle \frac{\sigma_i}{\sigma_i^2+\delta^2}\)</span> and otherwise all zeros, then as <span class="math inline">\(\delta \to 0\)</span>, <span class="math inline">\((\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T \to \Sigma^+\)</span>. Hence, as <span class="math inline">\(\delta \to 0\)</span>, <span class="math display">\[V[(\Sigma^T\Sigma+\delta^2I)^{-1}\Sigma^T]U^T \to V\Sigma^+U^T=A^+.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="numerical-linear-algebra">2. Numerical Linear Algebra</h1>
<h2 id="krylov-subspace-and-arnoldi-iteration">2.1. Krylov Subspace and Arnoldi Iteration</h2>
<ul>
<li><p>Matrix-vector multiplication is fast, especially if <span class="math inline">\(A\)</span> is sparse. If we start with <span class="math inline">\(A\)</span> and <span class="math inline">\(\mathbf{b}\)</span>, we can quickly compute each of the vectors <span class="math inline">\(\mathbf{b}, A\mathbf{b}, A^2\mathbf{b}=A(A\mathbf{b}), \ldots, A^{j-1}\mathbf{b}\)</span>. The combinations of vectors above give <strong>Krylov subspace</strong> <span class="math inline">\(\mathcal{K}_j\)</span>. We want to find a closest approximation <span class="math inline">\(\mathbf{x}_j \in K_j\)</span> to the desired solution <span class="math inline">\(\mathbf{x}\)</span>.</p></li>
<li><p><strong>(Arnoldi Iteration)</strong> An orthogonal basis is better than the original basis <span class="math inline">\(\mathbf{b}, A\mathbf{b}, \ldots, A^{j-1}\mathbf{b}\)</span>.</p>
<ul>
<li>Let <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{b}}{\|\mathbf{b}\|}\)</span>, and <span class="math inline">\(\mathbf{q}_2, \ldots, \mathbf{q}_k\)</span> are known;</li>
<li>Let <span class="math inline">\(\mathbf{v}=A\mathbf{q}_k\)</span>;</li>
<li>For <span class="math inline">\(j=1\)</span> to <span class="math inline">\(k\)</span>:
<ul>
<li><span class="math inline">\(h_{jk}=\mathbf{q}_j^T\mathbf{v}\)</span>;</li>
<li><span class="math inline">\(\mathbf{v}=\mathbf{v}-h_{jk}\mathbf{q}_j\)</span>;</li>
</ul></li>
<li>Let <span class="math inline">\(h_{k+1, k}=\|\mathbf{v}\|\)</span>;</li>
<li>Let <span class="math inline">\(\displaystyle \mathbf{q}_{k+1}=\frac{\mathbf{v}}{h_{k+1, k}}\)</span>.</li>
</ul></li>
</ul>
<div class="note info">
            <p>The Arnoldi iteration follows the Gram-Schmidt idea.</p><p>Assume we use the standard Gram-Schmidt to attain an orthogonal basis for <span class="math inline">\(\mathcal{K}_{j+1}\)</span>, then <span class="math display">\[\mathbf{y}_j=A^j\mathbf{b}-\sum_{i=1}^j (\mathbf{q}_i^TA^j\mathbf{b})\mathbf{q}_i\]</span> and <span class="math display">\[\mathbf{q}_{j+1}=\frac{\mathbf{y}_j}{\|\mathbf{y}_j\|}.\]</span> Hence, <span class="math inline">\(\{\mathbf{q}_1, \ldots, \mathbf{q}_{j+1}\}\)</span> is an orthogonal basis for <span class="math inline">\(\mathcal{K}_{j+1}\)</span>.</p><p>Since <span class="math inline">\(\displaystyle \mathbf{q}_1=\frac{\mathbf{b}}{\|\mathbf{b}\|}\)</span> and <span class="math inline">\(A\mathbf{q}_1=c_1\mathbf{q}_1+c_2\mathbf{q}_2\)</span>, then <span class="math display">\[\begin{aligned}\mathcal{K}_{j+1}&amp;=\text{span}\{\mathbf{b}, A\mathbf{b}, A^2\mathbf{b}, \ldots, A^j\mathbf{b}\}\\&amp;=\text{span}\{\mathbf{q}_1, A\mathbf{q}_1, A^2\mathbf{q}_1, \ldots, A^j\mathbf{q}_1\}\\&amp;=\text{span}\{\mathbf{q}_1, c_1\mathbf{q}_1+c_2\mathbf{q}_2, A(c_1\mathbf{q}_1+c_2\mathbf{q}_2), \ldots, A^{j-1}(c_1\mathbf{q}_1+c_2\mathbf{q}_2)\}\\&amp;=\text{span}\{\mathbf{q}_1, \mathbf{q}_2, A\mathbf{q}_2, \ldots, A^{j-1}\mathbf{q}_2\}\\&amp;\ \ \vdots\\&amp;=\text{span}\{\mathbf{q}_1, \mathbf{q}_2, \ldots, \mathbf{q}_j, A\mathbf{q}_j\}.\end{aligned}\]</span></p><p>Apply Gram-Schmidt again, we have <span class="math display">\[\mathbf{v}_j=A\mathbf{q}_j-\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\]</span> and replace <span class="math display">\[\mathbf{q}_{j+1}=\frac{\mathbf{v}_j}{\|\mathbf{v}_j\|}.\]</span></p><p>Since <span class="math display">\[\mathbf{q}_{j+1}^T\mathbf{v}_j=\mathbf{q}_{j+1}^TA\mathbf{q}_j-\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_{j+1}^T\mathbf{q}_i=\mathbf{q}_{j+1}^TA\mathbf{q}_j\]</span> and <span class="math display">\[\mathbf{q}_{j+1}^T\mathbf{v}_j=\frac{\mathbf{v}_j^T\mathbf{v}_j}{\|\mathbf{v}_j\|}=\|\mathbf{v}_j\|,\]</span> then <span class="math inline">\(\|\mathbf{v}_j\|=\mathbf{q}_{j+1}^TA\mathbf{q}_j\)</span>.</p><p>Therefore, <span class="math display">\[\begin{aligned}A\mathbf{q}_j&amp;=\mathbf{v}_j+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\|\mathbf{v}_j\|\mathbf{q}_{j+1}+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\mathbf{q}_{j+1}^TA\mathbf{q}_j\mathbf{q}_{j+1}+\sum_{i=1}^j (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\sum_{i=1}^{j+1} (\mathbf{q}_i^TA\mathbf{q}_j)\mathbf{q}_i\\&amp;=\sum_{i=1}^{j+1} h_{ij}\mathbf{q}_i\\&amp;=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \cdots &amp; \mathbf{q}_j \\ &amp;\end{bmatrix}\begin{bmatrix}h_{11} \\ \vdots \\ h_{jj}\end{bmatrix}+\mathbf{q}_{j+1}h_{j+1, j}.\end{aligned}\]</span></p><p>Hence, <span class="math display">\[\begin{aligned}AQ_j&amp;=\begin{bmatrix}&amp; \\ A\mathbf{q}_1 &amp; A\mathbf{q}_2 &amp; \cdots &amp; A\mathbf{q}_{j-1} &amp; A\mathbf{q}_j \\ &amp;\end{bmatrix}\\&amp;=\begin{bmatrix}&amp; \\ \mathbf{q}_1 &amp; \mathbf{q}_2 &amp; \cdots &amp; \mathbf{q}_j &amp; \mathbf{q}_{j+1} \\ &amp;\end{bmatrix}\begin{bmatrix}h_{11} &amp; h_{12} &amp; h_{13} &amp; \cdots &amp; h_{1j} \\h_{21} &amp; h_{22} &amp; h_{23} &amp; \cdots &amp; h_{2j} \\ 0 &amp; h_{32} &amp; h_{33} &amp; \cdots &amp; h_{3j} \\ \vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; 0 &amp; h_{j, j-1} &amp; h_{jj} \\ 0 &amp; \cdots &amp; \cdots &amp; 0 &amp; h_{j+1, j}\end{bmatrix}\\&amp;=Q_{j+1}H_{j+1, j}\end{aligned}.\]</span></p><p>Besides, we have <span class="math display">\[Q^T_jAQ_j=Q_j^TQ_{j+1}H_{j+1, j}=\begin{bmatrix}&amp; \\ I_{j \times j} &amp; \mathbf{0}_{j \times 1} \\ &amp;\end{bmatrix}\begin{bmatrix}H_j \\ (k+1)^\text{th}\ \text{row} \end{bmatrix}=H_j,\]</span> where <span class="math inline">\(H_j\)</span> is the <strong>Hessenberg matrix</strong> (an upper triangular matrix with one nonzero sub-diagonal). In addition, <span class="math inline">\(H_j=Q_j^TAQ_j\)</span> is the projection of <span class="math inline">\(A\)</span> onto the Krylov space, using the basis of <span class="math inline">\(\mathbf{q}\)</span>'s.</p>
          </div>
<h2 id="computation-of-eigenvalue-and-singular-value">2.2. Computation of Eigenvalue and Singular Value</h2>
<h3 id="qr-algorithm-for-eigenvalue">2.2.1. QR Algorithm for Eigenvalue</h3>
<ul>
<li>QR algorithm with shifts:</li>
</ul>
<p>         Step 1: Reduce <span class="math inline">\(A\)</span> to a similar Hessenberg form <span class="math inline">\(A_0\)</span>;</p>
<p>         Step 2: QR with shift:</p>
<p>             (1) Choose a shift <span class="math inline">\(s_k\)</span> at step <span class="math inline">\(k\)</span>, where <span class="math inline">\(k=0, \ldots\)</span>;</p>
<p>             (2) Factor <span class="math inline">\(A_k-s_kI=Q_kR_k\)</span>;</p>
<p>             (3) Reverse factors and shift back: <span class="math inline">\(A_{k+1}=R_kQ_k+s_kI\)</span>.</p>
<ul>
<li><p>Well-chosen shifts <span class="math inline">\(s_k\)</span> will greatly speed up the approach of the <span class="math inline">\(A\)</span>'s to a diagonal matrix <span class="math inline">\(\Lambda\)</span>.</p></li>
<li><p>If the matrix <span class="math inline">\(S\)</span> is symmetric, then we can reduce <span class="math inline">\(S\)</span> to a symmetric Hessenberg matrix, i.e., <strong>tridiagonal matrix</strong>.</p></li>
</ul>
<h3 id="golub-kahan-algorithm-for-singular-value">2.2.2. Golub-Kahan Algorithm for Singular Value</h3>
<ul>
<li><p>Eigenvalues are the same for <span class="math inline">\(S\)</span> and <span class="math inline">\(Q^{-1}SQ=Q^TSQ\)</span>, so we have <em>limited freedom</em> to create zeros in <span class="math inline">\(Q^{-1}SQ\)</span>. The good <span class="math inline">\(Q^{-1}SQ\)</span> will be tridiagonal matrix.</p></li>
<li><p>Singular values are the same for <span class="math inline">\(A\)</span> and <span class="math inline">\(Q_1AQ_2^T\)</span> even if <span class="math inline">\(Q_1\)</span> is different from <span class="math inline">\(Q_2\)</span>. We have <em>more freedom</em> to create zeros in <span class="math inline">\(Q_1AQ_2^T\)</span>. With the right <span class="math inline">\(Q\)</span>'s, the good <span class="math inline">\(Q_1AQ_2^T\)</span> will be <strong>bidiagonal matrix</strong>.</p></li>
<li><p>Golub-Kahan algorithm:</p></li>
</ul>
<p>         Step 1: Find <span class="math inline">\(Q_1\)</span> and <span class="math inline">\(Q_2\)</span> so that <span class="math inline">\(Q_1AQ_2^T\)</span> is bidiagonal;</p>
<p>         Step 2: Apply the shifted QR algorithm to find the eigenvalues of <span class="math display">\[(Q_1AQ_2^T)^T(Q_1AQ_2^T)=Q_2A^TAQ_2^T,\]</span> which is tridiagonal.</p>
<h1 id="random-matrix-multiplication">3. Random Matrix Multiplication</h1>
<ul>
<li><p>Let <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{C}\)</span>, where <span class="math inline">\(\mathbf{a}_j\)</span> is the column <span class="math inline">\(j\)</span> of <span class="math inline">\(A\)</span>, <span class="math inline">\(\mathbf{b}_j^T\)</span> is the row <span class="math inline">\(j\)</span> of <span class="math inline">\(B\)</span>, and <span class="math inline">\(\displaystyle C=\sum_{j=1}^n \|\mathbf{a}_j\|\|\mathbf{b}_j^T\|\)</span>.</p></li>
<li><p>Take <span class="math inline">\(\mathbf{a}_j\)</span> and <span class="math inline">\(\mathbf{b}_j^T\)</span> w.p. <span class="math inline">\(p_j\)</span>, and each of the <span class="math inline">\(s\)</span> trials produces a matrix <span class="math inline">\(\displaystyle X_j=\frac{\mathbf{a}_j\mathbf{b}_j^T}{sp_j}\)</span>. The mean of each trial is <span class="math display">\[\mathbb{E}[X]=\sum_{j=1}^n p_jX_j=\frac{1}{s}\sum_{j=1}^n \mathbf{a}_j\mathbf{b}_j^T=\frac{1}{s}AB.\]</span> The approximation of <span class="math inline">\(AB\)</span> is the sum of <span class="math inline">\(s\)</span> trials, and we have <span class="math inline">\(s\mathbb{E}[X]=AB\)</span>.</p></li>
<li><p>The variance of each trial is <span class="math display">\[\begin{aligned}
\text{Var}[X]&amp;=\mathbb{E}[X^2]-(\mathbb{E}[X])^2
\\&amp;=\sum_{j=1}^n p_j\frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{s^2p_j^2}-\frac{1}{s^2}\|AB\|_F^2
\\&amp;=\frac{1}{s^2}(C^2-\|AB\|_F^2).
\end{aligned}\]</span> The variance of samples is <span class="math inline">\(\displaystyle s\text{Var}[X]=\frac{1}{s}(C^2-\|AB\|_F^2)\)</span>.</p></li>
<li><p>Norm-squared sampling uses the optimal probabilities <span class="math inline">\(p_j\)</span> for minimum variance.</p></li>
</ul>
<p><strong><em>Proof.</em></strong> We want to solve <span class="math display">\[\min \sum_{j=1}^n \frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j}-\frac{1}{s}\|AB\|_F^2\ \ \text{s.t.}\ \ \sum_{j=1}^n p_j=1.\]</span> By Lagrange, <span class="math display">\[L(p_1, \ldots, p_n, \lambda)=\sum_{j=1}^n \frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j}+\lambda\left(\sum_{j=1}^n p_j-1\right).\]</span></p>
<p>Take the partial derivatives w.r.t. <span class="math inline">\(p_j\)</span> and <span class="math inline">\(\lambda\)</span>, let <span class="math display">\[\frac{\partial L}{\partial p_j}=-\frac{\|\mathbf{a}_j\|^2\|\mathbf{b}_j^T\|^2}{sp_j^2}+\lambda=0\]</span> and <span class="math display">\[\frac{\partial L}{\partial \lambda}=\sum_{j=1}^n p_j-1=0,\]</span> then <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{\sqrt{s\lambda}}\)</span> and <span class="math inline">\(\displaystyle \sum_{j=1}^n p_j=1\)</span>.</p>
Therefore, <span class="math inline">\(\displaystyle \sum_{j=1}^n \frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{\sqrt{s\lambda}}=1\)</span> gives <span class="math inline">\(\sqrt{s\lambda}=C\)</span>. Hence, <span class="math inline">\(\displaystyle p_j=\frac{\|\mathbf{a}_j\|\|\mathbf{b}_j^T\|}{C}\)</span> solves the optimization problem, i.e., norm-squared sampling uses the optimal probabilities <span class="math inline">\(p_j\)</span> for minimum variance.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Matrix/" rel="tag"><i class="fa fa-tag"></i> Matrix</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Mathematics/Matrix-method/Linear_Algebra" rel="prev" title="Linear Algebra">
      <i class="fa fa-chevron-left"></i> Linear Algebra
    </a></div>
      <div class="post-nav-item">
    <a href="/Mathematics/Matrix-method/Low_Rank_and_Compressed_Sensing" rel="next" title="Low Rank and Compressed Sensing">
      Low Rank and Compressed Sensing <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#least-squares"><span class="nav-text">1. Least Squares</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#pseudo-inverse-method"><span class="nav-text">1.1. Pseudo-Inverse Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#normal-equation-method"><span class="nav-text">1.2. Normal Equation Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#gram-schmidt-method"><span class="nav-text">1.3. Gram-Schmidt Method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#penalty-method"><span class="nav-text">1.4. Penalty Method</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#numerical-linear-algebra"><span class="nav-text">2. Numerical Linear Algebra</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#krylov-subspace-and-arnoldi-iteration"><span class="nav-text">2.1. Krylov Subspace and Arnoldi Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#computation-of-eigenvalue-and-singular-value"><span class="nav-text">2.2. Computation of Eigenvalue and Singular Value</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#qr-algorithm-for-eigenvalue"><span class="nav-text">2.2.1. QR Algorithm for Eigenvalue</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#golub-kahan-algorithm-for-singular-value"><span class="nav-text">2.2.2. Golub-Kahan Algorithm for Singular Value</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#random-matrix-multiplication"><span class="nav-text">3. Random Matrix Multiplication</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
