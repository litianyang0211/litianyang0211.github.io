<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Kernel Method">
<meta property="og:url" content="https://litianyang0211.github.io/Computer_Science/Machine_Learning/Kernel_Method">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2022-02-06T16:00:00.000Z">
<meta property="article:modified_time" content="2022-02-12T22:53:13.328Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="Kernel">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://litianyang0211.github.io/Computer_Science/Machine_Learning/Kernel_Method.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Kernel Method | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Computer_Science/Machine_Learning/Kernel_Method">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Kernel Method
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-07 00:00:00" itemprop="dateCreated datePublished" datetime="2022-02-07T00:00:00+08:00">2022-02-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-02-13 06:53:13" itemprop="dateModified" datetime="2022-02-13T06:53:13+08:00">2022-02-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          
            <div class="post-description"><div></div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="kernel-method">1. Kernel Method</h1>
<ul>
<li>Kernel method is a powerful class of non-linear machine learning algorithm, which is based around employing linear method in nonlinearly transformed feature space.</li>
</ul>
<h1 id="feature-map-and-feature-space">2. Feature Map and Feature Space</h1>
<p><strong>Definition 2.1</strong>    Let <span class="math inline">\(\mathcal{H}\)</span> be a vector space over <span class="math inline">\(\mathbb{R}\)</span>. A function <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}: \mathcal{H} \times \mathcal{H} \to \mathbb{R}\)</span> is said to be an <strong>inner product</strong> on <span class="math inline">\(\mathcal{H}\)</span> if</p>
<p>    (1) <span class="math inline">\(\langle \alpha_1f_1+\alpha_2f_2, g \rangle_\mathcal{H}=\alpha_1 \langle f_1, g \rangle_\mathcal{H}+\alpha_2 \langle f_2, g \rangle_\mathcal{H}\)</span>;</p>
<p>    (2) <span class="math inline">\(\langle f, g \rangle_\mathcal{H}=\langle g, f \rangle_\mathcal{H}\)</span>;</p>
<p>    (3) <span class="math inline">\(\langle f, f \rangle_\mathcal{H} \geq 0\)</span>;</p>
<p>    (4) <span class="math inline">\(\langle f, f \rangle_\mathcal{H}=0\)</span> iff <span class="math inline">\(f=0\)</span>.</p>
<p>The <strong>norm</strong> is <span class="math inline">\(\|f\|_\mathcal{H}=\sqrt{\langle f, f \rangle_\mathcal{H}}\)</span>.</p>
<p><strong>Definition 2.2</strong>    A <strong>Hilbert space</strong> is a vector space on which we define an inner product, <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}\)</span>, and which is complete w.r.t. <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}\)</span>.</p>
<p><strong>Definition 2.3</strong>    A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> is called a <strong>kernel</strong> if there exists a Hilbert space <span class="math inline">\(\mathcal{H}\)</span> and a map <span class="math inline">\(\varphi: \mathcal{X} \to \mathcal{H}\)</span> s.t. for all <span class="math inline">\(x, x&#39; \in \mathcal{X}\)</span>, <span class="math display">\[k(x, x&#39;):=\langle \varphi(x), \varphi(x&#39;) \rangle_\mathcal{H}.\]</span> <span class="math inline">\(\mathcal{H}\)</span> is known as a <strong>feature space</strong> and <span class="math inline">\(\varphi\)</span> is known as a <strong>feature map</strong> of <span class="math inline">\(k\)</span>.</p>
<div class="note info">
            <p>There are no conditions on <span class="math inline">\(\mathcal{X}\)</span>: it does not need to be numeric. For example, we can define features and kernels for text.</p>
          </div>
<p><strong>Definition 2.4</strong>    A symmetric function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> is <strong>positive definite</strong> iff for all <span class="math inline">\(n \geq 1\)</span>, <span class="math inline">\(a_i \in \mathbb{R}\)</span>, <span class="math inline">\(x_i \in \mathcal{X}\)</span>, <span class="math display">\[\sum_{i=1}^n \sum_{j=1}^n a_ia_jk(x_i, x_j) \geq 0.\]</span> The function <span class="math inline">\(k(\cdot, \cdot)\)</span> is <strong>strictly positive definite</strong> if for mutually distinct <span class="math inline">\(x_i\)</span>, the equality holds only when all the <span class="math inline">\(a_i\)</span> are zero.</p>
<p><span id="thm2.1"><strong>Theorem 2.1</strong></span>    Let <span class="math inline">\(\mathcal{H}\)</span> be any Hilbert space, <span class="math inline">\(\mathcal{X}\)</span> a non-empty set, and <span class="math inline">\(\varphi: \mathcal{X} \to \mathcal{H}\)</span>. Then <span class="math display">\[k(x, x&#39;)=\langle \varphi(x), \varphi(x&#39;) \rangle_\mathcal{H}\]</span> is a positive definite function.</p>
<strong><em>Proof.</em></strong> We have <span class="math display">\[\sum_{i=1}^n \sum_{j=1}^n a_ia_jk(x_i, x_j)=\sum_{i=1}^n \sum_{j=1}^n \langle a_i\varphi(x_i), a_j\varphi(x_j) \rangle_\mathcal{H}=\left\langle \sum_{i=1}^n a_i\varphi(x_i), \sum_{j=1}^n a_j\varphi(x_j) \right\rangle_\mathcal{H}=\left\| \sum_{i=1}^n a_i\varphi(x_i) \right\|_\mathcal{H}^2 \geq 0.\]</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>We can consider infinite dimensional Hilbert space as a space of functions. By definition, <span class="math inline">\(\varphi(x) \in \mathcal{H}\)</span>, and we can think of <span class="math inline">\(\varphi\)</span> as mapping from <span class="math inline">\(x\)</span> to functions, such that <span class="math inline">\(\varphi: \mathcal{X} \to (\mathcal{X} \to \mathbb{R})\)</span>.</p>
          </div>
<h1 id="reproducing-kernel-hilbert-space">3. Reproducing Kernel Hilbert Space</h1>
<p>A particularly important class of Hilbert spaces is <strong>reproducing kernel Hilbert space</strong> (<strong>RKHS</strong>).</p>
<p><strong>Definition 3.1</strong>    Let <span class="math inline">\(\mathcal{H}\)</span> be a Hilbert space of functions <span class="math inline">\(f: \mathcal{X} \to \mathbb{R}\)</span>. A function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> is called a <strong>reproducing kernel</strong> of <span class="math inline">\(\mathcal{H}\)</span> if it satisfies:</p>
<p>    (1) for all <span class="math inline">\(x \in \mathcal{X}\)</span>, <span class="math inline">\(k_x:=k(\cdot, x) \in \mathcal{H}\)</span>;</p>
<p>    (2) for all <span class="math inline">\(x \in \mathcal{X}\)</span> and <span class="math inline">\(f \in \mathcal{H}\)</span>, <span class="math inline">\(\langle f, k(\cdot, x) \rangle_\mathcal{H}=f(x)\)</span> (<strong>reproducing property</strong>).</p>
<p>If <span class="math inline">\(\mathcal{H}\)</span> has a reproducing kernel, it is called a <strong>reproducing kernel Hilbert space</strong> (<strong>RKHS</strong>).</p>
<p><strong>Definition 3.1'</strong>    <span class="math inline">\(\mathcal{H}\)</span> is an RKHS if the evaluation functional <span class="math inline">\(\delta_x: \mathcal{H} \to \mathbb{R}\)</span>, <span class="math inline">\(\delta_xf=f(x)\)</span> is continuous for all <span class="math inline">\(x \in \mathcal{X}\)</span>. Equivalently, <span class="math inline">\(\mathcal{H}\)</span> is an RKHS if <span class="math inline">\(\delta_x\)</span> is a bounded operator, i.e. there exists a <span class="math inline">\(C&gt;0\)</span> s.t. <span class="math display">\[|\delta_xf|=|f(x)| \leq C\|f\|_\mathcal{H}\]</span> for all <span class="math inline">\(x \in \mathcal{X}\)</span> and <span class="math inline">\(f \in \mathcal{H}\)</span>.</p>
<div class="note info">
            <p>If <span class="math inline">\(\|f-g\|_\mathcal{H}=0\)</span>, then <span class="math inline">\(f(x)=g(x)\)</span> for all <span class="math inline">\(x \in \mathcal{X}\)</span>.</p>
          </div>
<p><span id="thm3.1"><strong>Theorem 3.1</strong></span>    A reproducing kernel is a valid kernel with the feature map <span class="math inline">\(\varphi: x \mapsto k(\cdot, x)\)</span>, known as the <strong>canonical feature map</strong>.</p>
<strong><em>Proof.</em></strong> Let <span class="math inline">\(k\)</span> be the reproducing kernel, then <span class="math inline">\(k(\cdot, x&#39;) \in \mathcal{H}\)</span> and <span class="math inline">\(\langle k(\cdot, x&#39;), k(\cdot, x) \rangle_\mathcal{H}=k(\cdot, x&#39;)(x)=k(x, x&#39;)\)</span>. Hence, we have <span class="math display">\[k(x, x&#39;)=\langle k(\cdot, x&#39;), k(\cdot, x) \rangle_\mathcal{H}=\langle \varphi(x&#39;), \varphi(x) \rangle_\mathcal{H},\]</span> i.e. <span class="math inline">\(k\)</span> is a kernel.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm3.2"><strong>Theorem 3.2 (Moore-Aronszajn Theorem)</strong></span>    Evert positive definite function <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> is also a reproducing kernel with a unique corresponding RKHS.</p>
<p><strong><em>Proof.</em></strong> Let <span class="math inline">\(k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}\)</span> be a (symmetric) positive definite function.</p>
<p>Define a function space <span class="math inline">\(\mathcal{H}_0\)</span> as <span class="math inline">\(\text{span}\{k(\cdot, x): x \in \mathcal{X}\}\)</span>, i.e. <span class="math display">\[\mathcal{H}_0:=\left\{\sum_{i=1}^r \alpha_ik(\cdot, x_i)\right\}_{r \in \mathbb{N}, \alpha_i \in \mathbb{R}, x_i \in \mathcal{X}}\]</span> where typically <span class="math inline">\(r=\infty\)</span>.</p>
<p>Define a functional <span class="math inline">\(h: (\mathcal{X} \to \mathbb{R}) \times (\mathcal{X} \to \mathbb{R}) \to \mathbb{R}\)</span>: <span class="math display">\[h(k(\cdot, x_i), k(\cdot, x_j))=k(x_i, x_j)\]</span> where <span class="math inline">\(k(\cdot, x_i) \in \mathcal{H}_0\)</span>. Since <span class="math inline">\(k(\cdot, x_i)\)</span> dictates all possible values of <span class="math inline">\(k(x, x_i)\)</span> for <span class="math inline">\(x \in \mathcal{X}\)</span> and <span class="math inline">\(k(\cdot, x_j)\)</span> dictates all possible values of <span class="math inline">\(k(x, x_j)\)</span> for <span class="math inline">\(x \in \mathcal{X}\)</span>, then there is only one <span class="math inline">\(k(x_i, x_j)\)</span> compatible with both <span class="math inline">\(k(\cdot, x_i)\)</span> and <span class="math inline">\(k(\cdot, x_j)\)</span>.</p>
<p>Define an inner product for <span class="math inline">\(\mathcal{H}_0\)</span>, <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}: \mathcal{H}_0 \times \mathcal{H}_0 \to \mathbb{R}\)</span>. Consider an arbitrary pair <span class="math inline">\(f, g \in \mathcal{H}_0\)</span>, then <span class="math display">\[f=\sum_{i=1}^r \alpha_ik(\cdot, x_i) \quad \text{and} \quad g=\sum_{j=1}^s \beta_jk(\cdot, x_j).\]</span></p>
<p>Define <span class="math display">\[\langle f, g \rangle_\mathcal{H}:=\sum_{i=1}^r \sum_{j=1}^s \alpha_i\beta_j h(k(\cdot, x_i), k(\cdot, x_j))=\sum_{i=1}^r \sum_{j=1}^s \alpha_i\beta_jk(x_i, x_j).\]</span></p>
<p>Define <span class="math inline">\(\mathcal{H}\)</span> to be the completion of <span class="math inline">\(\mathcal{H}_0\)</span> w.r.t. the norm <span class="math inline">\(\|f\|_\mathcal{H}=\sqrt{\langle f, f \rangle_\mathcal{H}}\)</span>.</p>
<p>We now need to show that <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}\)</span> is a valid inner product.</p>
<ul>
<li><p>Linearity: Let <span class="math inline">\(f_1, f_2, g \in \mathcal{H}\)</span>. We have <span class="math display">\[af_1+bf_2=a\sum_{i=1}^{r_1} \alpha_{i, 1}k(\cdot, x_{i, 1})+b\sum_{i=1}^{r_2} \alpha_{i, 2}k(\cdot, x_{i, 2})=\sum_{i=1}^{r_1+r_2} \alpha_ik(\cdot, x_i)\]</span> where <span class="math inline">\(\alpha_i=\mathbb{I}(i \leq r_1)a\alpha_{i, 1}+\mathbb{I}(i&gt;r_1)b\alpha_{i, 2}\)</span> and <span class="math inline">\(x_i=\mathbb{I}(i \leq r_1)x_{i, 1}+\mathbb{I}(i&gt;r_1)x_{i, 2}\)</span>. Then <span class="math display">\[\begin{aligned}
\langle af_1+bf_2, g \rangle_\mathcal{H}&amp;=\sum_{i=1}^{r_1+r_2} \sum_{j=1}^s \alpha_i\beta_jk(x_i, x_j&#39;)
\\&amp;=\sum_{j=1}^s\beta_j \left(a\sum_{i=1}^{r_1} \alpha_{i, 1}k(x_{i, 1}, x_j&#39;)+b\sum_{i=1}^{r_2} \alpha_{i, 2}k(x_{i, 2}, x_j&#39;)\right)
\\&amp;=a\sum_{i=1}^{r_1}\alpha_{i, 1}\beta_jk(x_{i, 1}, x_j&#39;)+b\sum_{i=1}^{r_2}\alpha_{i, 2}\beta_jk(x_{i, 2}, x_j&#39;)
\\&amp;=a\langle f_1, g \rangle_\mathcal{H}+b\langle f_2, g \rangle_\mathcal{H}.
\end{aligned}\]</span></p></li>
<li><p>Symmetry: Let <span class="math inline">\(f \in \mathcal{H}\)</span>. We have <span class="math display">\[\langle f, g \rangle_\mathcal{H}=\sum_{i=1}^r\sum_{j=1}^s \alpha_i\beta_jk(x_i, x_j)=\sum_{j=1}^s\sum_{i=1}^r \beta_j\alpha_ik(x_j, x_i)=\langle g, f \rangle_\mathcal{H}.\]</span></p></li>
<li><p>Positivity: We have <span class="math display">\[\langle f, f \rangle_\mathcal{H}=\sum_{i=1}^r\sum_{j=1}^r \alpha_i\alpha_jk(x_i, x_j) \geq 0\]</span> since <span class="math inline">\(k\)</span> is positive definite.</p></li>
<li><p>Positive definiteness: Suppose <span class="math inline">\(f=0\)</span>, then <span class="math display">\[\langle f, f \rangle_\mathcal{H}=\sum_{i=1}^r \alpha_i\sum_{j=1}^r \alpha_jk(x_i, x_j)=\sum_{i=1}^r \alpha_if(x_i)=0.\]</span> Suppose <span class="math inline">\(\langle f, f \rangle_\mathcal{H}=0\)</span>, then <span class="math display">\[\sum_{i=1}^r \sum_{j=1}^r \alpha_i\alpha_jk(x_i, x_j)=0\]</span> for such <span class="math inline">\(\{\alpha_i\}_{i=1}^r\)</span> and <span class="math inline">\(\{x_i\}_{i=1}^r\)</span>. Suppose for some <span class="math inline">\(x_{r+1} \in \mathcal{X}\)</span> s.t. <span class="math display">\[f(x_{r+1})=\sum_{i=1}^r \alpha_i k(x_i, x_{r+1}) \neq 0.\]</span> then <span class="math display">\[\begin{aligned}
\sum_{i=1}^{r+1} \sum_{j=1}^{r+1} \alpha_i\alpha_jk(x_i, x_j)&amp;=\sum_{i=1}^r \sum_{j=1}^r \alpha_i\alpha_jk(x_i, x_j)+2\alpha_{r+1}f(x_{r+1})+\alpha_{r+1}^2k(x_{r+1}, x_{r+1})
\\&amp;=2\alpha_{r+1}f(x_{r+1})+\alpha_{r+1}^2k(x_{r+1}, x_{r+1}) \geq 0
\end{aligned}\]</span> holds for any <span class="math inline">\(\alpha_{r+1}\)</span>. If <span class="math inline">\(k(x_{r+1}, x_{r+1})=0\)</span>, then let <span class="math inline">\(\alpha_{r+1}=-f(x_{r+1})\)</span>, and <span class="math inline">\(-2f^2(x_{r+1}) \geq 0\)</span>, implying <span class="math inline">\(f(x_{r+1})=0\)</span>, which is a contradiction. If <span class="math inline">\(k(x_{r+1}, x_{r+1})&gt;0\)</span>, then let <span class="math display">\[\alpha_{r+1}=\frac{-f(x_{r+1})}{k(x_{r+1}, x_{r+1})},\]</span> and <span class="math display">\[\frac{-f^2(x_{r+1})}{k(x_{r+1}, x_{r+1})} \geq 0,\]</span> implying <span class="math inline">\(f(x_{r+1})=0\)</span>, which is a contradiction. Therefore, <span class="math inline">\(f(x)=0\)</span> for all <span class="math inline">\(x \in \mathcal{X}\)</span>. In conclusion, <span class="math inline">\(\langle f, f \rangle_\mathcal{H}=0\)</span> iff <span class="math inline">\(f=0\)</span>.</p></li>
</ul>
<p>Therefore, <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}\)</span> is an inner product, and thus <span class="math inline">\(\mathcal{H}\)</span> is a Hilbert space. Since <span class="math inline">\(\langle k(\cdot, x), k(\cdot, x&#39;) \rangle_\mathcal{H}=k(x, x&#39;)\)</span> by construction, then <span class="math inline">\(k(x, x&#39;)=\langle \varphi(x), \varphi(x&#39;) \rangle_\mathcal{H}\)</span> where <span class="math inline">\(\varphi(x)=k(\cdot, x)\)</span>. Hence, <span class="math inline">\(k(x, x&#39;)\)</span> is a kernel.</p>
<p>We now need to show that <span class="math inline">\(k\)</span> is a reproducing kernel.</p>
<ul>
<li><p><span class="math inline">\(k(\cdot, x) \in \mathcal{H}\)</span> by definition.</p></li>
<li><p>For any <span class="math inline">\(f \in \mathcal{H}\)</span>, we have <span class="math display">\[\begin{aligned}
\langle f, k(\cdot, x) \rangle_\mathcal{H}&amp;=\left\langle \sum_{i=1}^r \alpha_ik(\cdot, x_i), k(\cdot, x) \right\rangle_\mathcal{H}
\\&amp;=\sum_{i=1}^r \alpha_i\langle k(\cdot, x_i), k(\cdot, x) \rangle_\mathcal{H}
\\&amp;=\sum_{i=1}^r \alpha_ik(x_i, x)
\\&amp;=f(x).
\end{aligned}\]</span></p></li>
</ul>
Hence <span class="math inline">\(k\)</span> is a reproducing kernel of <span class="math inline">\(\mathcal{H}\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The following statements are proven:</p><ul><li><p>(<a href="#thm2.1">Theorem 2.1</a>)Any kernel is positive definite.</p></li><li><p>(<a href="#thm3.2">Moore-Aronszajn theorem</a>) Any positive definite function is a reproducing kernel with feature map <span class="math inline">\(\varphi(x)=k(\cdot, x)\)</span>.</p></li><li><p>(<a href="#thm3.1">Theorem 3.1</a>) A reproducing kernel is a valid kernel.</p></li></ul><p>Then we know the following concepts are equivalent: (1) kernel, (2) positive definite function, and (3) reproducing kernel.</p>
          </div>
<h1 id="rkhs-as-hypothesis-class">4. RKHS as Hypothesis Class</h1>
<h2 id="uniqueness-of-rkhs">4.1. Uniqueness of RKHS</h2>
<p><strong>Theorem 4.1</strong>    Each RKHS has a unique corresponding reproducing kernel.</p>
<strong><em>Proof.</em></strong> Assume that an RKHS <span class="math inline">\(\mathcal{H}\)</span> has two unique reproducing kernels <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>. Using a combination of linearity and the reproducing property, we have for all <span class="math inline">\(f \in \mathcal{H}\)</span> and <span class="math inline">\(x \in \mathcal{X}\)</span>, <span class="math display">\[\langle f, k_1(\cdot, x)-k_2(\cdot, x) \rangle_\mathcal{H}=\langle f, k_1(\cdot, x) \rangle_\mathcal{H}-\langle f, k_2(\cdot, x) \rangle_\mathcal{H}=f(x)-f(x)=0.\]</span> Let <span class="math inline">\(f=k_1(\cdot, x)-f_2(\cdot, x)\)</span>, then <span class="math inline">\(\|k_1(\cdot, x)-k_2(\cdot, x)\|_\mathcal{H}^2=0\)</span> for all <span class="math inline">\(x \in \mathcal{X}\)</span>, which implies <span class="math inline">\(k_1=k_2\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note info">
            <p>The inverse result is also true, i.e. the RKHS for any kernel is unique, and thus we can denote the RKHS for kernel <span class="math inline">\(k\)</span> as <span class="math inline">\(\mathcal{H}_k\)</span>.</p>
          </div>
<h2 id="representer-theorem">4.2. Representer Theorem</h2>
<p>We can consider a RKHS as a hypothesis class for ERM. A typical and general setup would be that we are looking for the function <span class="math inline">\(f^*\)</span> in the RKHS <span class="math inline">\(\mathcal{H}_k\)</span> which solves the regularized ERM problem <span class="math display">\[\mathop{\arg\min}_{f \in \mathcal{H}_k} \widehat{R}(f)+\Omega(\|f\|_{\mathcal{H}_k}^2)\]</span> for empirical risk <span class="math inline">\(\widehat{R}(f)\)</span> and any non-decreasing function <span class="math inline">\(\Omega\)</span>.</p>
<p><strong>Theorem 4.2 (Representer Theorem)</strong>    There is always a solution to <span class="math display">\[f^*=\mathop{\arg\min}_{f \in \mathcal{H}_k} \widehat{R}(f)+\Omega(\|f\|_{\mathcal{H}_k}^2)\]</span> that takes the form <span class="math display">\[f^*=\sum_{i=1}^n a_ik(\cdot, x_i)\]</span> where <span class="math inline">\(a_i \in \mathbb{R}\)</span> and <span class="math inline">\(x_i\)</span> is input datapoint. If <span class="math inline">\(\Omega\)</span> is strictly increasing, all solutions have the form above.</p>
<p><strong><em>Proof.</em></strong> Let <span class="math inline">\(f\)</span> be a minimizer of <span class="math inline">\(g(f)=\widehat{R}(f)+\Omega(\|f\|_{\mathcal{H}_k}^2)\)</span>. Let <span class="math display">\[f_s=\sum_{i=1}^n \alpha_i k(\cdot, x_i),\]</span> i.e. <span class="math inline">\(f_s\)</span> is the projection of <span class="math inline">\(f\)</span> to the subspace <span class="math inline">\(\text{span}\{k(\cdot, x_i): i=1, \ldots, n\}\)</span>. Let <span class="math inline">\(f_\perp\)</span> be orthogonal component of <span class="math inline">\(f\)</span> s.t. <span class="math inline">\(f=f_s+f_\perp\)</span>. Then <span class="math inline">\(\|f\|_{\mathcal{H}_k}^2=\|f_s\|_{\mathcal{H}_k}^2+\|f_\perp\|_{\mathcal{H}_k}^2 \geq \|f_s\|_{\mathcal{H}_k}^2\)</span> and thus <span class="math inline">\(\Omega(\|f\|_{\mathcal{H}_k}^2) \geq \Omega(\|f_s\|_{\mathcal{H}_k}^2)\)</span>.</p>
<p>Besides, <span class="math display">\[f(x_i)=\langle f, k(\cdot, x_i) \rangle_{\mathcal{H}_k}=\langle f_s+f_\perp, k(\cdot, x_i) \rangle_{\mathcal{H}_k}=\langle f_s, k(\cdot, x_i) \rangle_{\mathcal{H}_k}=f_s(x_i).\]</span> Then <span class="math inline">\(L(y_i, f(x_i), x_i)=L(y_i, f_s(x_i), x_i)\)</span> and thus <span class="math inline">\(\widehat{R}(f)=\widehat{R}(f_s)\)</span>. Therefore, <span class="math inline">\(g(f_s) \leq g(f)\)</span> and <span class="math inline">\(f_s\)</span> is also a minimizer.</p>
If <span class="math inline">\(\Omega\)</span> is strictly increasing, then <span class="math inline">\(\Omega(\|f\|_{\mathcal{H}_k}^2)&gt;\Omega(\|f_s\|_{\mathcal{H}_k}^2)\)</span>. Suppose <span class="math inline">\(\|f_\perp\|_{\mathcal{H}_k}^2&gt;0\)</span>, then <span class="math inline">\(\|f\|_{\mathcal{H}_k}^2&gt;\|f_s\|_{\mathcal{H}_k}^2\)</span>, then <span class="math inline">\(f\)</span> is not a minimizer, which is a contradiction. Then <span class="math inline">\(\|f_\perp\|_{\mathcal{H}_k}^2=0\)</span>, i.e. <span class="math inline">\(\|f\|_{\mathcal{H}_k}^2=\|f_s\|_{\mathcal{H}_k}^2\)</span>, i.e. all solutions have the form <span class="math inline">\(\displaystyle \sum_{i=1}^n \alpha_ik(\cdot, x_i)\)</span>.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<h1 id="kernel">5. Kernel</h1>
<h2 id="operation-with-kernel">5.1. Operation with Kernel</h2>
<p><strong>Theorem 5.1</strong>    Given kernels <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span> on <span class="math inline">\(\mathcal{X}\)</span> and constants <span class="math inline">\(\alpha_1, \alpha_2&gt;0\)</span>, then <span class="math inline">\(k=\alpha_1k_1+\alpha_2k_2\)</span> is a kernel on <span class="math inline">\(\mathcal{X}\)</span>.</p>
<strong><em>Proof.</em></strong> For all <span class="math inline">\(n \geq 1\)</span>, <span class="math inline">\(a_i \in \mathbb{R}\)</span> and <span class="math inline">\(x_i \in \mathcal{X}\)</span>, we have <span class="math display">\[\sum_{i=1}^n \sum_{j=1}^n a_ia_jk(x_i, x_j)=\alpha_1\sum_{i=1}^n \sum_{j=1}^n a_ia_j k_1(x_i, x_j)+\alpha_2\sum_{i=1}^n \sum_{j=1}^n a_ia_j k_2(x_i, x_j) \geq 0\]</span> and thus <span class="math inline">\(k\)</span> is positive definite.
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<div class="note warning">
            <p><span class="math inline">\(k_1-k_2\)</span> need not be a kernel.</p>
          </div>
<p><strong>Theorem 5.2</strong>    Given a map <span class="math inline">\(A: \mathcal{X} \to \widetilde{\mathcal{X}}\)</span> and kernel <span class="math inline">\(k\)</span> on <span class="math inline">\(\widetilde{\mathcal{X}}\)</span>, then <span class="math inline">\(k(A(x), A(x&#39;))\)</span> is a kernel on <span class="math inline">\(\mathcal{X}\)</span>.</p>
<strong><em>Proof.</em></strong> If <span class="math inline">\(k\)</span> is a kernel, than <span class="math inline">\(k(A(x), A(x&#39;))=\langle \varphi(A(x)), \varphi(A(x&#39;)) \rangle_\mathcal{H}\)</span> which is a kernel with feature <span class="math inline">\(\varphi(A(x))\)</span>
<p align="right">
<span class="math inline">\(\square\)</span>
</p>
<p><span id="thm5.3"><strong>Theorem 5.3</strong></span>    Given <span class="math inline">\(k_1\)</span> on <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(k_2\)</span> on <span class="math inline">\(\mathcal{Y}\)</span>, then <span class="math inline">\(k((x, y), (x&#39;, y&#39;))=k_1(x, x&#39;)k_2(y, y&#39;)\)</span> is a kernel on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>. Moreover, if <span class="math inline">\(\mathcal{X}=\mathcal{Y}\)</span>, then <span class="math inline">\(k(x, x&#39;)=k_1(x, x&#39;)k_2(x, x&#39;)\)</span> is a kernel on <span class="math inline">\(\mathcal{X}\)</span>.</p>
<h2 id="matérn-kernel">5.2. Matérn Kernel</h2>
<p>Using operation with kernel, we can show that <strong>Gaussian kernel</strong> (<strong>RBF kernel</strong>, <strong>squared exponential kernel</strong> or <strong>exponentiated quadratic kernel</strong>) is valid on <span class="math inline">\(\mathbb{R}^p\)</span>: <span class="math display">\[k(x, x&#39;):=\exp\left(-\frac{1}{2\gamma^2}\|x-x&#39;\|^2\right).\]</span> The RKHS of Gaussian kernel is infinite-dimensional. Moreover, if the domain <span class="math inline">\(\mathcal{X}\)</span> is a compact subset of <span class="math inline">\(\mathbb{R}^p\)</span>, its RKHS is dense in the space of all bounded continuous functions w.r.t. the uniform norm. All functions in its RKHS are infinitely differentiable, and Gaussian kernel is often considered to be excessively smooth.</p>
<p>A less smooth alternative is the <strong>Matérn kernel</strong>: <span class="math display">\[k(x, x&#39;)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}}{\gamma}\|x-x&#39;\|\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}}{\gamma}\|x-x&#39;\|\right)\]</span> where <span class="math inline">\(\nu&gt;0\)</span>, <span class="math inline">\(\gamma&gt;0\)</span>, <span class="math inline">\(K_\nu\)</span> is the modified Bessel function of the second kind of order <span class="math inline">\(\nu\)</span>. The Matérn kernel corresponding to the value <span class="math inline">\(\displaystyle \nu=s+\frac{1}{2}\)</span> for non-negative integer <span class="math inline">\(s\)</span> takes a simpler form, where <span class="math inline">\(s\)</span> means <span class="math inline">\(s\)</span> times differentiable iff <span class="math inline">\(\nu&gt;s\)</span>. For example:</p>
<ul>
<li><p>when <span class="math inline">\(\displaystyle \nu=\frac{1}{2}\)</span>, <span class="math inline">\(\displaystyle k(x, x&#39;)=\exp\left(-\frac{1}{\gamma}\|x-x&#39;\|\right)\)</span>;</p></li>
<li><p>when <span class="math inline">\(\displaystyle \nu=\frac{3}{2}\)</span>, <span class="math inline">\(\displaystyle k(x, x&#39;)=\left(1+\frac{\sqrt{3}}{\gamma}\|x-x&#39;\|\right)\exp\left(-\frac{\sqrt{3}}{\gamma}\|x-x&#39;\|\right)\)</span>.</p></li>
</ul>
<p>Moreover, the RKHS norm <span class="math inline">\(\|f\|_{\mathcal{H}_k}^2\)</span> directly penalizes the derivatives of <span class="math inline">\(f\)</span>. For example, when <span class="math inline">\(\displaystyle \nu=\frac{3}{2}\)</span>, <span class="math display">\[\|f\|_{\mathcal{H}_k}^2 \propto \int_\mathcal{X} f&#39;&#39;(x)^2\text{d}x+\frac{6}{\gamma^2}\int_\mathcal{X} f&#39;(x)^2\text{d}x+\frac{9}{\gamma^4}\int_\mathcal{X} f(x)^2\text{d}x.\]</span></p>
<p>As <span class="math inline">\(\nu \to \infty\)</span>, Matérn kernel converges to the RBF kernel.</p>
<h2 id="other-kernels">5.3. Other Kernels</h2>
<ul>
<li><p><strong>Constant</strong>: <span class="math inline">\(k(x, x&#39;)=c\)</span>.</p></li>
<li><p><strong>Linear</strong>: <span class="math inline">\(k(x, x&#39;)=x^\top x&#39;\)</span>.</p></li>
<li><p><strong>Polynomial</strong>: <span class="math inline">\(k(x, x&#39;)=(c+x^\top x&#39;)^m\)</span>, where <span class="math inline">\(c \in \mathbb{R}\)</span> and <span class="math inline">\(m \in \mathbb{N}\)</span>. If <span class="math inline">\(m=1\)</span>, then <span class="math inline">\(k(x, x&#39;)\)</span> gives <strong>affine kernel</strong>.</p></li>
<li><p><strong>Periodic (1d)</strong>: <span class="math inline">\(\displaystyle k(x, x&#39;)=\exp\left(-\frac{2\sin^2(\pi|x-x&#39;|/p)}{\gamma^2}\right)\)</span>, where <span class="math inline">\(p\)</span> is the period and <span class="math inline">\(\gamma&gt;0\)</span>.</p></li>
<li><p><strong>Laplace</strong>: <span class="math inline">\(\displaystyle k(x, x&#39;)=\exp\left(-\frac{1}{\gamma}\|x-x&#39;\|\right)\)</span>, where <span class="math inline">\(\gamma&gt;0\)</span>.</p></li>
<li><p><strong>Rational quadratic</strong>: <span class="math inline">\(\displaystyle k(x, x&#39;)=\left(1+\frac{\|x-x&#39;\|^2}{2\alpha\gamma^2}\right)^{-\alpha}\)</span>, where <span class="math inline">\(\alpha&gt;0\)</span> and <span class="math inline">\(\gamma&gt;0\)</span>.</p></li>
</ul>
<div class="note info">
            <p>Rational quadratic kernel is a scale mixture of Gaussian kernels. In particular, consider Gaussian parametrization <span class="math display">\[k_\theta(x, x&#39;)=\exp(-\theta\|x-x&#39;\|^2)\]</span> and a Gamma density placed on <span class="math inline">\(\theta\)</span>, i.e. <span class="math display">\[p(\theta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{\alpha-1}\exp(-\beta\theta)\]</span> with shape <span class="math inline">\(\alpha\)</span> and rate <span class="math inline">\(\beta\)</span>. We then define <span class="math display">\[\begin{aligned}\kappa(x, x&#39;)&amp;=\int_0^\infty k_\theta(x, x&#39;)p(\theta)\text{d}\theta\\&amp;=\frac{\beta^\alpha}{\Gamma(\alpha)}\int_0^\infty \exp(-\theta(\|x-x&#39;\|^2+\beta))\theta^{\alpha-1}\text{d}\theta\\&amp;=\frac{\beta^\alpha}{\Gamma(\alpha)} \cdot \frac{\Gamma(\alpha)}{(\|x-x&#39;\|^2+\beta)^\alpha}\\&amp;=\left(1+\frac{\|x-x&#39;\|^2}{\beta}\right)^{-\alpha}.\end{aligned}\]</span></p><p>If <span class="math inline">\(\beta=2\alpha\gamma^2\)</span> and let <span class="math inline">\(\alpha \to \infty\)</span>, we again recover Gaussian kernel.</p>
          </div>
<h1 id="representation-of-probability-in-rkhs">6. Representation of Probability in RKHS</h1>
<p>We can represent probability distribution <span class="math inline">\(P\)</span> in the RKHS by considering the <strong>kernel mean embedding</strong> <span class="math display">\[\mu_k(P)=\mathbb{E}_{X \sim P}k(\cdot, X) \in \mathcal{H}_k.\]</span> Using the reproducing property, a kernel mean embedding converts a function <span class="math inline">\(f \in \mathcal{H}_k\)</span> to its mean w.r.t. <span class="math inline">\(P\)</span> when we take an inner product <span class="math display">\[\begin{aligned}
\langle \mu_k(P), f \rangle_{\mathcal{H}_k}&amp;=\langle \mathbb{E}_{X \sim P}k(\cdot, X), f \rangle_{\mathcal{H}_k}
\\&amp;=\mathbb{E}_{X \sim P}\langle k(\cdot, X), f \rangle_{\mathcal{H}_k}
\\&amp;=\mathbb{E}_{X \sim P}f(X)
\end{aligned}\]</span> for all <span class="math inline">\(f \in \mathcal{H}_k\)</span>.</p>
<p>Inner product between kernel mean embeddings can be computed as <span class="math display">\[\langle \mu_k(P), \mu_k(Q) \rangle_{\mathcal{H}_k}=\langle \mathbb{E}_{X \sim P}k(\cdot, X), \mathbb{E}_{Y \sim Q}k(\cdot, Y) \rangle_{\mathcal{H}_k}=\mathbb{E}_{X \sim P, Y \sim Q}k(X, Y).\]</span></p>
<div class="note warning">
            <p>The samples are from different margin distributions, not the joint distribution.</p>
          </div>
<h2 id="maximum-mean-dsicrepancy">6.1. Maximum Mean Dsicrepancy</h2>
<p>One of the most powerful uses of kernel mean embedding is to measure discrepancy between distributions by considering their distance in RKHS norm, which is called <strong>maximum mean discrepancy</strong> (<strong>MMD</strong>): <span class="math display">\[\begin{aligned}
\text{MMD}_k(P, Q)&amp;=\|\mu_k(P)-\mu_k(Q)\|_{\mathcal{H}_k}
\\&amp;=\sqrt{\langle \mu_k(P), \mu_k(P) \rangle_{\mathcal{H}_k}+\langle \mu_k(Q), \mu_k(Q) \rangle_{\mathcal{H}_k}-2\langle \mu_k(P), \mu_k(Q) \rangle_{\mathcal{H}_k}}
\\&amp;=\sqrt{\mathbb{E}_{X \sim P, X&#39; \sim P}k(X, X&#39;)+\mathbb{E}_{Y \sim Q, Y&#39; \sim Q}k(Y, Y&#39;)-2\mathbb{E}_{X \sim P, Y \sim Q}k(X, Y)}
\end{aligned}\]</span> where <span class="math inline">\(X\)</span> and <span class="math inline">\(X&#39;\)</span> are independent random samples from <span class="math inline">\(P\)</span>, and <span class="math inline">\(Y\)</span> and <span class="math inline">\(Y&#39;\)</span> are independent random samples from <span class="math inline">\(Q\)</span>. For characteristic kernels, the MMD is a proper metric on probability distributions, i.e. <span class="math display">\[\text{MMD}_k(P, Q)=0 \Leftrightarrow P=Q.\]</span></p>
<p>The name MMD comes from the following interpretation: it can also be written as the largest discrepancy between expectations of the unit norm RKHS functions w.r.t. two distributions: <span class="math display">\[\text{MMD}_k(P, Q)=\sup_{f \in \mathcal{H}_k: \|f\|_{\mathcal{H}_k} \leq 1} |\mathbb{E}_{X \sim P}f(X)-\mathbb{E}_{Y \sim Q}f(Y)|.\]</span> The worst case <span class="math inline">\(f\)</span> (i.e. the <span class="math inline">\(f\)</span> that forms the supremum) turns out to be proportional to the <strong>witness function</strong> <span class="math inline">\(\mu_k(P)-\mu_k(Q)\)</span>.</p>
<p>Given sets of independent samples <span class="math inline">\(\{x_i\}_{i=1}^{n_x} \overset{\text{i.i.d.}}{\sim} P\)</span> and <span class="math inline">\(\{y_i\}_{i=1}^{n_y} \overset{\text{i.i.d.}}{\sim} Q\)</span>, a simple unbiased estimator of the squared MMD is given by <span class="math display">\[\widehat{\text{MMD}_k^2}(P, Q)=\frac{1}{n_x(n_x-1)}\sum_{i \neq j} k(x_i, x_j)+\frac{1}{n_y(n_y-1)}\sum_{i \neq j} k(y_i, y_j)-\frac{2}{n_xn_y}\sum_{i=1}^{n_x} \sum_{j=1}^{n_y} k(x_i, y_j).\]</span></p>
<h2 id="hilbert-schmidt-independence-criterion">6.2. Hilbert-Schmidt Independence Criterion</h2>
<p>Another use of kernel embedding is in measuring dependence between r.v.s. taking values in some generic domains (e.g. random vectors, strings, or graphs).</p>
<div class="note info">
            <p>For any kernels <span class="math inline">\(k_\mathcal{X}\)</span> and <span class="math inline">\(k_\mathcal{Y}\)</span> on the respective domains <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span>, we can define <span class="math inline">\(k=k_\mathcal{X} \otimes k_\mathcal{Y}\)</span> given by <span class="math display">\[k((x, y), (x&#39;, y&#39;))=k_\mathcal{X}(x, x&#39;)k_\mathcal{Y}(y, y&#39;)\]</span> which is a valid kernel on the product domain <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span> from <a href="#thm5.3">theorem 5.3</a>. The canonical feature map is <span class="math inline">\(\varphi(x, y)=k_\mathcal{X}(\cdot, x) \otimes k_\mathcal{Y}(\cdot, y)\)</span>, which is a function on <span class="math inline">\(\mathcal{X} \times \mathcal{Y}\)</span>, i.e. <span class="math display">\[(\varphi(x, y))(x&#39;, y&#39;)=k_\mathcal{X}(x&#39;, x)k_\mathcal{Y}(y&#39;, y).\]</span></p>
          </div>
<p><strong>Definition 6.1</strong>    Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be r.v.s. on domains <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> (non-empty topological spaces). Let <span class="math inline">\(k_\mathcal{X}\)</span> and <span class="math inline">\(k_\mathcal{Y}\)</span> be kernels on <span class="math inline">\(\mathcal{X}\)</span> and <span class="math inline">\(\mathcal{Y}\)</span> respectively. The <strong>Hilbert-Schmidt independence criterion</strong> (<strong>HSIC</strong>) <span class="math inline">\(\Xi_{k_\mathcal{X}, k_\mathcal{Y}}(X, Y)\)</span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the squared MMD between the joint measure <span class="math inline">\(P_{XY}\)</span> and the product of marginals <span class="math inline">\(P_XP_Y\)</span>, computed with the product kernel <span class="math inline">\(k=k_\mathcal{X} \otimes k_\mathcal{Y}\)</span>, i.e. <span class="math display">\[\begin{aligned}
\Xi_{k_\mathcal{X}, k_\mathcal{Y}}(X, Y)&amp;=\text{MMD}_k^2(P_{XY}, P_XP_Y)
\\&amp;=\|\mu_k(P_{XY})-\mu_k(P_XP_Y)\|_{\mathcal{H}_k}^2
\\&amp;=\|\mathbb{E}_{XY}[k_\mathcal{X}(\cdot, X) \otimes k_\mathcal{Y}(\cdot, Y)]-\mathbb{E}_Xk_\mathcal{X}(\cdot, X) \otimes \mathbb{E}_Yk_\mathcal{Y}(\cdot, Y)\|_{\mathcal{H}_k}^2.
\end{aligned}\]</span></p>
<p><span class="math inline">\(\Xi_{k_\mathcal{X}, k_\mathcal{Y}}(X, Y)=0\)</span> if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, while for many common choices of kernel the inverse result also holds (i.e. <span class="math inline">\(\Xi_{k_\mathcal{X}, k_\mathcal{Y}}(X, Y)=0\)</span> implies independence). The larger HSIC is, the more dependent the variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are.</p>
<p>The HSIC can be estimated via the following process:</p>
<ol type="1">
<li><p>Draw <span class="math inline">\(m\)</span> independent samples <span class="math inline">\(\{(x_i, y_i)\}_{i=1}^m\)</span> from the joint distribution <span class="math inline">\(P_{XY}\)</span>.</p></li>
<li><p>Construct the kernel matrices <span class="math inline">\(K_{ij}=k_\mathcal{X}(x_i, x_j)\)</span> and <span class="math inline">\(L_{ij}=k_\mathcal{Y}(y_i, y_j)\)</span>.</p></li>
<li><p>Center the kernel matrices by computing <span class="math inline">\(\widetilde{K}=HKH\)</span> and <span class="math inline">\(\widetilde{L}=HLH\)</span> where <span class="math inline">\(\displaystyle H=I-\frac{1}{m}J\)</span>, and <span class="math inline">\(J=\mathbf{1}\mathbf{1}^\top\)</span>.</p></li>
<li><p>Construct the estimate (note that <span class="math inline">\(\widetilde{K}\)</span> and <span class="math inline">\(\widetilde{L}\)</span> are symmetric): <span class="math display">\[\widehat{\Xi_{k_\mathcal{X}, k_\mathcal{Y}}}(X, Y)=\frac{1}{m^2}\text{tr}(\widetilde{K}\widetilde{L})=\frac{1}{m^2}\sum_{i=1}^m \sum_{j=1}^m \widetilde{K}_{ij}\widetilde{L}_{ij}.\]</span></p></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Kernel/" rel="tag"><i class="fa fa-tag"></i> Kernel</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Statistics/Simulation_Method/Importance_Sampling" rel="prev" title="Importance Sampling">
      <i class="fa fa-chevron-left"></i> Importance Sampling
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#kernel-method"><span class="nav-text">1. Kernel Method</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#feature-map-and-feature-space"><span class="nav-text">2. Feature Map and Feature Space</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reproducing-kernel-hilbert-space"><span class="nav-text">3. Reproducing Kernel Hilbert Space</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rkhs-as-hypothesis-class"><span class="nav-text">4. RKHS as Hypothesis Class</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#uniqueness-of-rkhs"><span class="nav-text">4.1. Uniqueness of RKHS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#representer-theorem"><span class="nav-text">4.2. Representer Theorem</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#kernel"><span class="nav-text">5. Kernel</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#operation-with-kernel"><span class="nav-text">5.1. Operation with Kernel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mat%C3%A9rn-kernel"><span class="nav-text">5.2. Matérn Kernel</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#other-kernels"><span class="nav-text">5.3. Other Kernels</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#representation-of-probability-in-rkhs"><span class="nav-text">6. Representation of Probability in RKHS</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#maximum-mean-dsicrepancy"><span class="nav-text">6.1. Maximum Mean Dsicrepancy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hilbert-schmidt-independence-criterion"><span class="nav-text">6.2. Hilbert-Schmidt Independence Criterion</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
