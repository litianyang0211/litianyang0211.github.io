<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.loli.net/css?family=Menlo:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"litianyang0211.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Transformer in PyTorch">
<meta property="og:url" content="https://litianyang0211.github.io/Computer_Science/PyTorch/Transformer_in_PyTorch">
<meta property="og:site_name" content="Tianyang Li">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_1.png">
<meta property="og:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_2.png">
<meta property="article:published_time" content="2021-12-11T00:00:00.000Z">
<meta property="article:modified_time" content="2022-01-06T17:38:24.135Z">
<meta property="article:author" content="Tianyang Li">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Attention">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://litianyang0211.github.io/images/Transformer_in_PyTorch_1.png">

<link rel="canonical" href="https://litianyang0211.github.io/Computer_Science/PyTorch/Transformer_in_PyTorch.html">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Transformer in PyTorch | Tianyang Li</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Tianyang Li</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-posts">

    <a href="/posts/" rel="section"><i class="fa fa-blog fa-fw"></i>Posts</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-projects">

    <a href="/projects/" rel="section"><i class="fa fa-code fa-fw"></i>Projects</a>

  </li>
        <li class="menu-item menu-item-friends">

    <a href="/friends/" rel="section"><i class="fa fa-user-friends fa-fw"></i>Friends</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://litianyang0211.github.io/Computer_Science/PyTorch/Transformer_in_PyTorch">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Tianyang Li">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Tianyang Li">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Transformer in PyTorch
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-lightbulb"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-12-11 00:00:00" itemprop="dateCreated datePublished" datetime="2021-12-11T00:00:00+00:00">2021-12-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-edit"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-01-06 17:38:24" itemprop="dateModified" datetime="2022-01-06T17:38:24+00:00">2022-01-06</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/" itemprop="url" rel="index"><span itemprop="name">Computer Science</span></a>
                </span>
                  /
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a class=n href="/categories/Computer-Science/PyTorch/" itemprop="url" rel="index"><span itemprop="name">PyTorch</span></a>
                </span>
            </span>

          
            <div class="post-description"><div></div></div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="prelim">1. Prelim</h1>
<p>本文使用PyTorch实现<a href="/Computer_Science/Natural_Language_Processing/Transformer" target="_Blank">Transformer</a>, 并进一步补充细节。</p>
<p>首先导入所需模块：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>
<p>其次提前设定关键参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embed_dim = <span class="number">512</span></span><br><span class="line">num_heads = <span class="number">8</span></span><br><span class="line">dropout = <span class="number">0.01</span></span><br><span class="line">max_len = <span class="number">5000</span></span><br><span class="line">d_k = d_q = embed_dim // num_heads</span><br><span class="line">d_v = d_k</span><br><span class="line">d_ff = <span class="number">2048</span></span><br><span class="line">num_layers = <span class="number">6</span></span><br></pre></td></tr></table></figure>
<h1 id="model-architecture">2. Model Architecture</h1>
<p>大部分热门的神经序列转导模型（Neural Sequence Transduction Model）都具有编码—解码架构 <a href="#BCB14">(Bahdanau, Cho, and Bengio, 2014)</a>. Transformer也使用了这一架构：</p>
<span id="f1"><img src="/images/Transformer_in_PyTorch_1.png"></span>
<center>
Figure 1: Model architecture of Transformer <a href="#V+17">(Vaswani et al., 2017)</a>.
</center>
<h1 id="embeddings-and-positionalencoding">3. Embeddings and PositionalEncoding</h1>
<h2 id="embeddings">3.1. Embeddings</h2>
<p><code>Embeddings</code>负责将输入的序列进行词嵌入，每个单词嵌入后，映射为维度为<code>embed_dim</code>的向量。比如说，有10个单词，当<code>embed_dim=512</code>时，我们得到一个形状为<code>10 * 512</code>的词嵌入矩阵，每一行代表一个单词。在<code>forward</code>函数中乘以<span class="math inline">\(\sqrt{\texttt{embed_dim}}\)</span>是因为<code>nn.Embedding</code>在初始化时，用的是<code>xavier_uniform</code>, 乘法运算是为了让最后分布的方差为<span class="math inline">\(1\)</span>, 使网络在训练时的收敛速度更快。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_dim</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param vocab_size: 当前语言的词典大小（单词个数），为src_vocab_size或tgt_vocab_size</span></span><br><span class="line"><span class="string">        :param embed_dim:  词嵌入的维度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_dim)</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param inputs: 形状为(batch_size, seq_len)的torch.LongTensor，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">        :return:       (batch_size, seq_len, embed_dim)的张量，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># nn.Embedding在初始化时，用的是xavier_uniform，而乘法运算是为了让最后分布的方差为1，</span></span><br><span class="line">        <span class="comment"># 使网络在训练时的收敛速度更快</span></span><br><span class="line">        <span class="keyword">return</span> self.embed(inputs) * np.sqrt(self.embed_dim)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><p><code>torch.nn.Embedding</code>有多个参数，我们主要关注<code>num_embeddings</code>与<code>embedding_dim</code>. <code>num_embeddings</code>即词典的大小尺寸，<code>embedding_dim</code>即嵌入向量的维度。</p></li><li><p>在做欧洲语系和英语翻译的时候，很多词是共享词根的，因此他们的源语言和目标语言共享一个权重矩阵 <a href="#PW17">(Press and Wolf, 2017)</a>. 但对于其它语言之间（如中文和英文），则没有共享权重矩阵的必要。</p></li></ul>
          </div>
<h2 id="positionalencoding">3.2. PositionalEncoding</h2>
<p><code>PositionalEncoding</code>的作用是添加位置信息。回顾位置编码的公式：</p>
<p><span class="math display">\[\begin{aligned}
\text{PE}(\text{pos}, 2i)&amp;=\sin(\text{pos}/10000^{2i/d_\text{model}}) \\
\text{PE}(\text{pos}, 2i+1)&amp;=\cos(\text{pos}/10000^{2i/d_\text{model}})
\end{aligned}\]</span></p>
<p>其中<span class="math inline">\(\text{pos}\)</span>指的是一句话中某个单词的位置，<span class="math inline">\(i\)</span>指的是词嵌入的维度序号。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embed_dim, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param embed_dim: 词嵌入的维度</span></span><br><span class="line"><span class="string">        :param dropout:   在每次迭代训练时不参与训练的概率</span></span><br><span class="line"><span class="string">        :param max_len:   提前准备好的序列的位置编码的长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        pe = torch.zeros(max_len, embed_dim)  <span class="comment"># 得到(max_len, embed_dim)的矩阵来表示其位置编码</span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)  <span class="comment"># 得到(max_len, 1)的矩阵</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, embed_dim, <span class="number">2</span>) * -(np.log(<span class="number">10000.0</span>) / embed_dim))  <span class="comment"># 为了防止数值过大溢出</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)  <span class="comment"># 偶数下标的位置</span></span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)  <span class="comment"># 奇数下标的位置</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)  <span class="comment"># (max_len, embed_dim) -&gt; (batch_size=1, max_len, embed_dim)</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;pe&quot;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param inputs: Embeddings的词嵌入结果，为(batch_size, seq_len, embed_dim)的张量，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">        :return:       词嵌入加上位置编码的结果，为(batch_size, seq_len, embed_dim)的张量，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.dropout(inputs + self.pe[:, :inputs.size(<span class="number">1</span>), :])</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><p>带<code>nn.Dropout</code>的网络可以防止出现过拟合，<code>nn.Dropout(p=dropout)</code>的意思是指该层的神经元在每次迭代训练时会随机有<span class="math inline">\(10\%\)</span>的可能性不参与训练。</p></li><li><p><code>torch.arange</code>的结果并不包含<code>end</code>. 比如<code>torch.arange(1, 3)</code>输出<code>tensor([1, 2])</code>, 其类型为<code>torch.int64</code>. 又比如<code>torch.arange(0, 512, 2)</code>生成从<span class="math inline">\(0\)</span>到<span class="math inline">\(512\)</span>的偶数。</p></li><li><p><code>.unsqueeze()</code>主要是对数据维度进行扩充。</p></li><li><p><code>register_buffer</code>函数通常用于保存一些模型参数之外的值。</p></li></ul>
          </div>
<h1 id="encoder-and-decoder">4. Encoder and Decoder</h1>
<h2 id="multiheadedattention">4.1. MultiHeadedAttention</h2>
<p>Transformer是围绕着注意力（Attention）机制展开的，正如Vaswani等人 (2017) 提到的：</p>
<blockquote>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.</p>
</blockquote>
<p>简而言之，注意力机制将序列中的数据点联系起来。Transformer使用一种特定类型的注意力机制，称为多头注意力（Multi-Head Attention）——这是模型中最重要的部分。多头注意力如下图所示：</p>
<span id="f2"><img src="/images/Transformer_in_PyTorch_2.png"></span>
<center>
Figure 2: (Left) Scaled dot-product attention. (Right) Multi-Head attention consists of several attention layers running in parallel <a href="#V+17">(Vaswani et al., 2017)</a>.
</center>
<p>我们可以通过注意力构建多头注意力层，注意力公式输出值（Value）的加权平均，而权重来自查询（Query）和键（Key）的计算：<span class="math display">\[\text{Attention}(Q, K, V)=\text{softmax}\left(\frac{QK^\top}{\sqrt{\dim(\mathbf{k})}}\right)V.\]</span></p>
<div class="note info">
            <p>自注意力机制的核心就是通过<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>计算得到注意力权重，然后再作用于<span class="math inline">\(V\)</span>得到整个权重和输出。模型在对当前位置的信息进行编码时，会将注意力集中于自身的位置，而可能忽略其它位置，因此其中一种解决方案就是采用多头注意力机制。同时，使用多头注意力机制还能够使注意力层输出包含不同子空间中的编码表示信息，从而增强模型的表达能力。</p>
          </div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, attn_mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param query:     (batch_size, head_num, q_len, d_q)的张量</span></span><br><span class="line"><span class="string">    :param key:       (batch_size, head_num, k_len, d_k)的张量</span></span><br><span class="line"><span class="string">    :param value:     (batch_size, head_num, v_len, d_v)的张量，其中v_len=k_len</span></span><br><span class="line"><span class="string">    :param attn_mask: (batch_size, head_num, q_len, k_len)的张量</span></span><br><span class="line"><span class="string">    :param dropout:   nn.Dropout(p)</span></span><br><span class="line"><span class="string">    :return:          注意力结果，为(batch_size, head_num, q_len, d_v)的张量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    d_k = key.size(-<span class="number">1</span>)  <span class="comment"># d_k = d_q</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / np.sqrt(d_k)  <span class="comment"># -&gt; (batch_size, head_num, q_len, k_len)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对已经计算好的scores，按照mask张量填入-1e9，</span></span><br><span class="line">    <span class="comment"># 这样在下一步在计算softmax时，被设置成-1e9的数对应的值可以被忽略</span></span><br><span class="line">    <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># attn_mask是一个由True和False构成的张量</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对scores的最后一个维度执行softmax</span></span><br><span class="line">    attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用nn.Dropout防止过拟合</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        attn = dropout(attn)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(attn, value)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>.matmul</code>用于计算高维矩阵，比如维度为<code>(batch_size, head_num, seq_length, num_features)</code>的矩阵。</li></ul>
          </div>
<p>我们再定义一个<code>clones</code>函数，帮助我们复制相同的结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, n</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param module: 需要复制的结构</span></span><br><span class="line"><span class="string">    :param n:      复制的个数</span></span><br><span class="line"><span class="string">    :return:       复制的结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)])</span><br></pre></td></tr></table></figure>
<p>对于每一个头（Head），都使用三个矩阵<span class="math inline">\(W^Q\)</span>, <span class="math inline">\(W^K\)</span>和<span class="math inline">\(W^V\)</span>把输入转换为<span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>和<span class="math inline">\(V\)</span>. 然后分别用每一个头进行自注意力的计算，最后把<span class="math inline">\(h\)</span>个头的输出拼接起来，用一个矩阵<span class="math inline">\(W^O\)</span>把输出压缩，具体的计算过程为：</p>
<p><span class="math display">\[\text{MultiHead}(Q, K, V)=\text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O\]</span>其中<span class="math inline">\(\text{head}_i=\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\)</span>, <span class="math inline">\(W_i^Q \in \mathbb{R}^{d_\text{model} \times d_k}\)</span>, <span class="math inline">\(W_i^K \in \mathbb{R}^{d_\text{model} \times d_k}\)</span>, <span class="math inline">\(W_i^V \in \mathbb{R}^{d_\text{model} \times d_v}\)</span>以及<span class="math inline">\(W^O \in \mathbb{R}^{hd_v \times d_\text{model}}\)</span>. 我们假设<span class="math inline">\(h=8\)</span>及<span class="math inline">\(d_k=d_v=d_\text{model}/h=64\)</span>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 以query为例，W_Q为(embed_dim, d_q=d_k)的矩阵，一共有num_heads个头，</span></span><br><span class="line">        <span class="comment"># 对应num_heads个W_Q矩阵，拼接起来为(embed_dim, d_q*num_heads=embed_dim)的矩阵</span></span><br><span class="line">        <span class="comment"># 又例如，根据注意力公式得到的num_heads个(seq_len, d_v)矩阵Z_i，并将其拼接起来，得到一个</span></span><br><span class="line">        <span class="comment"># (seq_len, d_v*num_heads=embed_dim)的矩阵，再乘以(embed_dim, embed_dim)的矩阵W_O</span></span><br><span class="line">        <span class="comment"># 得到(seq_len, embed_dim)的矩阵Z</span></span><br><span class="line">        self.q_weight = nn.Linear(embed_dim, d_q * num_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.k_weight = nn.Linear(embed_dim, d_k * num_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.v_weight = nn.Linear(embed_dim, d_v * num_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.o_weight = nn.Linear(d_v * num_heads, embed_dim, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, q_inputs, k_inputs, v_inputs, attn_mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param q_inputs:   (batch_size, q_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :param k_inputs:   (batch_size, k_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :param v_inputs:   (batch_size, v_len, embed_dim)的张量，其中v_len=k_len</span></span><br><span class="line"><span class="string">        :param attn_mask:  (batch_size, q_len, k_len)的张量</span></span><br><span class="line"><span class="string">        :return:           (batch_size, q_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        residual = q_inputs  <span class="comment"># 用于残差连接，为(batch_size, q_len, embed_dim)的张量</span></span><br><span class="line">        batch_size = q_inputs.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成用于attention函数的mask张量</span></span><br><span class="line">        <span class="keyword">if</span> attn_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># (batch_size, q_len, k_len) -.unsqueeze-&gt; (batch_size, 1, q_len, k_len)</span></span><br><span class="line">            <span class="comment"># -.repeat-&gt; (batch_size, num_heads, q_len, k_len)</span></span><br><span class="line">            attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, num_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用线性变换计算Q, K, V矩阵，并把embed_dim平均分配给head_num个头</span></span><br><span class="line">        <span class="comment"># 以query为例，(batch_size, q_len, embed_dim) -.Linear-&gt; (batch_size, q_len, embed_dim)</span></span><br><span class="line">        <span class="comment"># -.view-&gt; (batch_size, q_len, num_heads, d_q) -.transpose-&gt; (batch_size, num_heads, q_len, d_q)</span></span><br><span class="line">        <span class="comment"># 同理，k和v分别为(batch_size, num_heads, k_len, d_k)和(batch_size, num_heads, v_len, d_v)的张量</span></span><br><span class="line">        q = self.q_weight(q_inputs).view(batch_size, -<span class="number">1</span>, num_heads, d_q).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        k = self.k_weight(k_inputs).view(batch_size, -<span class="number">1</span>, num_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v = self.v_weight(v_inputs).view(batch_size, -<span class="number">1</span>, num_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算Attention</span></span><br><span class="line">        attn = attention(q, k, v, attn_mask, self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用线性变换将head_num个头的d_v维向量拼接成head_num*d_v维的向量</span></span><br><span class="line">        <span class="comment"># (batch_size, head_num, q_len, d_v) -.transpose-&gt; (batch_size, q_len, head_num, d_v)</span></span><br><span class="line">        <span class="comment"># -.view-&gt; (batch_size, q_len, embed_dim)</span></span><br><span class="line">        z = self.o_weight(attn.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, d_v * num_heads))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_v * num_heads)(z + residual)</span><br></pre></td></tr></table></figure>
<h2 id="positionwisefeedforward">4.2. PositionwiseFeedForward</h2>
<p><code>PositionwiseFeedForward</code>是一个全连接层，由两个线性变换以及它们之间的ReLU激活组成，即：<span class="math display">\[\text{FFN}(x)=\max(0, xW_1+b_1)W_2+b_2.\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.ffn = nn.Sequential(</span><br><span class="line">            nn.Linear(embed_dim, d_ff),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(dropout),</span><br><span class="line">            nn.Linear(d_ff, embed_dim)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param inputs: (batch_size, q_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :return:       (batch_size, q_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(embed_dim)(self.ffn(inputs) + inputs)</span><br></pre></td></tr></table></figure>
<h2 id="encoderlayer">4.3. EncoderLayer</h2>
<p><code>EncoderLayer</code>是由<code>MultiHeadedAttention</code>与<code>PositionwiseFeedForward</code>构成的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pff = PositionwiseFeedForward()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs, enc_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_inputs:    (batch_size, src_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :param enc_mask:      (batch_size, src_len, src_len)的张量</span></span><br><span class="line"><span class="string">        :return:              (batch_size, src_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.pff(self.enc_attn(enc_inputs, enc_inputs, enc_inputs, enc_mask))</span><br></pre></td></tr></table></figure>
<h2 id="encoder">4.4. Encoder</h2>
<p><code>Encoder</code>是由<code>num_layers</code>个相同结构的<code>EncoderLayer</code>堆栈而成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = Embeddings(src_vocab_size, embed_dim)</span><br><span class="line">        self.pos_enc = PositionalEncoding(embed_dim)</span><br><span class="line">        self.layers = clones(EncoderLayer(), num_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_inputs: 形状为(batch_size, src_len)的torch.LongTensor</span></span><br><span class="line"><span class="string">        :return:           (batch_size, src_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        enc_outputs = self.pos_enc(self.src_emb(enc_inputs))</span><br><span class="line">        enc_mask = padding_mask(enc_inputs, enc_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            enc_outputs = layer(enc_outputs, enc_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> enc_outputs</span><br></pre></td></tr></table></figure>
<h2 id="decoderlayer">4.5. DecoderLayer</h2>
<p><code>DecoderLayer</code>中大部分的类是重复使用的，但我们要注意：底层的<code>MultiHeadAttention</code>输入目标语言序列，而上一层的<code>MultiHeadAttention</code>则要考虑源语言和目标语言。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pff = PositionwiseFeedForward()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_mask, dec_enc_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param dec_inputs:   (batch_size, tgt_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :param enc_outputs:  (batch_size, src_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :param dec_mask:     (batch_size, tgt_len, tgt_len)的张量</span></span><br><span class="line"><span class="string">        :param dec_enc_mask: (batch_size, tgt_len, src_len)的张量</span></span><br><span class="line"><span class="string">        :return:             (batch_size, tgt_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        dec_outputs = self.dec_attn(dec_inputs, dec_inputs, dec_inputs, dec_mask)</span><br><span class="line">        dec_outputs = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_mask)</span><br><span class="line">        <span class="keyword">return</span> self.pff(dec_outputs)</span><br></pre></td></tr></table></figure>
<h2 id="decoder">4.6. Decoder</h2>
<p><code>Decoder</code>与<code>Encoder</code>也是类似的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = Embeddings(tgt_vocab_size, embed_dim)</span><br><span class="line">        self.pos_enc = PositionalEncoding(embed_dim)</span><br><span class="line">        self.layers = clones(DecoderLayer(), num_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param dec_inputs:  形状为(batch_size, tgt_len)的torch.LongTensor</span></span><br><span class="line"><span class="string">        :param enc_inputs:  (batch_size, src_len)的张量</span></span><br><span class="line"><span class="string">        :param enc_outputs: (batch_size, src_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        :return:            (batch_size, tgt_len, embed_dim)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        dec_outputs = self.pos_enc(self.tgt_emb(dec_inputs))</span><br><span class="line">        dec_pad_mask = padding_mask(dec_inputs, dec_inputs)</span><br><span class="line">        dec_attn_mask = attention_mask(dec_inputs, dec_inputs)</span><br><span class="line">        dec_mask = torch.gt(dec_pad_mask + dec_attn_mask, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        dec_enc_pad_mask = padding_mask(dec_inputs, enc_inputs)</span><br><span class="line">        dec_enc_attn_mask = attention_mask(dec_inputs, enc_inputs)</span><br><span class="line">        dec_enc_mask = torch.gt(dec_enc_pad_mask + dec_enc_attn_mask, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs = layer(dec_outputs, enc_outputs, dec_mask, dec_enc_mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dec_outputs</span><br></pre></td></tr></table></figure>
<h1 id="transformer">5. Transformer</h1>
<p><code>Transformer</code>的结构相对比较简单，它包括<code>Encoder</code>和<code>Decoder</code>, 以及剩下的<code>Linear</code>层和<code>Softmax</code>层：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        self.encoder = Encoder()  <span class="comment"># .cuda()</span></span><br><span class="line">        self.decoder = Decoder()  <span class="comment"># .cuda()</span></span><br><span class="line">        self.linear = nn.Linear(embed_dim, tgt_vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param enc_inputs: (batch_size, src_len)的张量</span></span><br><span class="line"><span class="string">        :param dec_inputs: (batch_size, tgt_len)的张量</span></span><br><span class="line"><span class="string">        :return:           (batch_size*tgt_len, tgt_vocab_size)的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        enc_outputs = self.encoder(enc_inputs)  <span class="comment"># (batch_size, src_len) -&gt; (batch_size, src_len, embed_dim)</span></span><br><span class="line">        dec_outputs = self.decoder(dec_inputs, enc_inputs, enc_outputs)  <span class="comment"># -&gt; (batch_size, tgt_len, embed_dim)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># (batch_size, tgt_len, embed_dim) -.Linear-&gt; (batch_size, tgt_len, tgt_vocab_size)</span></span><br><span class="line">        dec_logits = self.linear(dec_outputs)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 利用.view将不同句子合并为长句：(batch_size, tgt_len, tgt_vocab_size) -.view-&gt; (batch_size*tgt_len, tgt_vocab_size)</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>))   </span><br></pre></td></tr></table></figure>
<h1 id="mask">6. Mask</h1>
<p>在<code>Encoder</code>和<code>Decoder</code>中都需要考虑Padding Mask，而<code>Encoder</code>和<code>Decoder</code>有一个关键的不同则是<code>Decoder</code>在解码第<span class="math inline">\(t\)</span>个时刻的时候只能使用<span class="math inline">\(1, \ldots, t\)</span>时刻的输入，而不能使用<span class="math inline">\(t+1\)</span>时刻及其之后的输入。因此我们需要两个函数来产生不同的Mask矩阵。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_mask</span>(<span class="params">seq1, seq2</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param seq1: (batch_size, seq1_len)的张量</span></span><br><span class="line"><span class="string">    :param seq2: (batch_size, seq2_len)的张量</span></span><br><span class="line"><span class="string">    :return:     (batch_size, seq1_len, seq2_len)的张量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    attn_shape = (seq1.size(<span class="number">0</span>), seq1.size(<span class="number">1</span>), seq2.size(<span class="number">1</span>))</span><br><span class="line">    attn_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(attn_mask).<span class="built_in">bool</span>()</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">padding_mask</span>(<span class="params">q, k</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param q: (batch_size, seq_len)的张量，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">    :param k: (batch_size, seq_len)的张量，seq_len为src_len或tgt_len</span></span><br><span class="line"><span class="string">    :return:  (batch_size, q_len, k_len)的张量，填充部分为True，非填充部分为False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    batch_size, q_len = q.size()</span><br><span class="line">    k_len = k.size(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># .eq(0)判断某个位置的值是否等于0</span></span><br><span class="line">    <span class="comment"># (batch_size, k_len) -.unsqueeze(1)-&gt; (batch_size, 1, k_len) -.expand-&gt; (batch_size, q_len, k_len)</span></span><br><span class="line">    <span class="keyword">return</span> k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>).expand(batch_size, q_len, k_len)</span><br></pre></td></tr></table></figure>
<div class="note info">
            <ul><li><code>np.triu</code>生成一个三角矩阵，<code>k=1</code>表示第<span class="math inline">\(k\)</span>条对角线以下都设置为<code>0</code>.</li></ul>
          </div>
<h1 id="full-model">7. Full Model</h1>
<p>我们现在生成完整的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Transformer()</span><br></pre></td></tr></table></figure>
<h1 id="reference">8. Reference</h1>
<div id="BCB14" style="line-height: 18px; font-size: 15px;">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. <em>arXiv Preprint arXiv:1409.0473</em>, 2014.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="H18" style="line-height: 18px; font-size: 15px;">
Harvard NLP. The Annotated Transformer. <em>Online: http://nlp.seas.harvard.edu/2018/04/03/attention.html</em>, 2018.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="PW17" style="line-height: 18px; font-size: 15px;">
Ofir Press and Lior Wolf. Using the Output Embedding to Improve Language Models. <em>arXiv Preprint arXiv:1608.05859</em>, 2017.
</div>
<div style="line-height: 18px; visibility: hidden;">
 
</div>
<div id="V+17" style="line-height: 18px; font-size: 15px;">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention Is All You Need. In <em>31st Conference on Neural Information Processing Systems</em>, 2017.
</div>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
              <a href="/tags/Attention/" rel="tag"><i class="fa fa-tag"></i> Attention</a>
              <a href="/tags/PyTorch/" rel="tag"><i class="fa fa-tag"></i> PyTorch</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/Computer_Science/Natural_Language_Processing/Transformer" rel="prev" title="Transformer">
      <i class="fa fa-chevron-left"></i> Transformer
    </a></div>
      <div class="post-nav-item">
    <a href="/Statistics/Graphical_Model/Causal_Inference" rel="next" title="Causal Inference">
      Causal Inference <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Contents
        </li>
        <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#prelim"><span class="nav-text">1. Prelim</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#model-architecture"><span class="nav-text">2. Model Architecture</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#embeddings-and-positionalencoding"><span class="nav-text">3. Embeddings and PositionalEncoding</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#embeddings"><span class="nav-text">3.1. Embeddings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positionalencoding"><span class="nav-text">3.2. PositionalEncoding</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#encoder-and-decoder"><span class="nav-text">4. Encoder and Decoder</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#multiheadedattention"><span class="nav-text">4.1. MultiHeadedAttention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#positionwisefeedforward"><span class="nav-text">4.2. PositionwiseFeedForward</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoderlayer"><span class="nav-text">4.3. EncoderLayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#encoder"><span class="nav-text">4.4. Encoder</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoderlayer"><span class="nav-text">4.5. DecoderLayer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#decoder"><span class="nav-text">4.6. Decoder</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#transformer"><span class="nav-text">5. Transformer</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#mask"><span class="nav-text">6. Mask</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#full-model"><span class="nav-text">7. Full Model</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#reference"><span class="nav-text">8. Reference</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Tianyang Li"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Tianyang Li</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/litianyang0211" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;litianyang0211" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:tianyang.li@linacre.ox.ac.uk" title="Email → mailto:tianyang.li@linacre.ox.ac.uk" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/derek0211" title="Facebook → https:&#x2F;&#x2F;www.facebook.com&#x2F;derek0211" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i></a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class=""></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Tianyang Li</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
